{
  "doc-1a1cfaef33c44783fb2431d333e2eec5": {
    "content": "*Corresponding author. Email: erame2019ert@gmail.com \n                      \n \n \n \nResearch Article \nA Survey of Generative Artificial Intelligence Techniques \n \nTam Sakirin1,*, , Siddartha Kusuma 2 ,   \n \n1 Computer Science Department, Faculty of science and information technology, International University, Phnom Penh, Cambodia  \n2 Balaji Institute of Technology and Science , India \n \nA r t i c l e  i n f o \n \nArticle History \nReceived 22 Dec 2022 \nAccepted 11 Feb 2023 \nPublished 10 Mar 2023 \n \n \nKeywords \nGenerative Adversarial \nNetworks (GANs) \nVariational Autoencoders \n(VAEs) \nDiffusion Models \nImage Synthesis \n \n \nA b s t r a c t  \n \nGenerative artificial intelligence (AI) refers to algorithms capable of creating novel, realistic digital \ncontent autonomously. Recently, generative models have attained groundbreaking results in domains like \nimage and audio synthesis, spurring vast interest in the field. This paper surveys the landscape of modern \ntechniques powering the rise of creative AI systems. We structurally examine predominant algorithmic \napproaches including generative adversarial networks (GANs), variational autoencoders (VAEs), and \nautoregressive models. Architectural innovations and illustrations of generated outputs are highlighted \nfor major models under each category. We give special attention to generative techniques for constructing \nrealistic images, tracing rapid progress from early GAN samples to modern diffusion models like Stable \nDiffusion. The paper further reviews generative modeling to create convincing audio, video, and 3D \nrenderings, which introduce critical challenges around fake media detection and data bias. Additionally, \nwe discuss common datasets that have enabled advances in generative modeling. Finally, open questions \naround evaluation, technique blending, controlling model behaviors, commercial deployment, and \nethical considerations are outlined as active areas for future  work. This survey presents both long -\nstanding and emerging techniques molding the state and trajectory of generative AI.  The key goals are \nto overview major algorithm families, highlight innovations through example models, synthesize \ncapabilities for multimedia generation, and discuss open problems around data, evaluation, control, and \nethics. Please let me know if you would like any clarification or modification of this proposed abstract. \n \n  \n \n \n1. INTRODUCTION \nThe ability to automatically generate novel, realistic digital content is an emerging hallmark of artificial intelligence.[1] \nGenerative algorithms create original images, audio, video, and 3D models that exhibit remarkable verisimilitude to actual \ndata distributions. The outputs emerge synthesized completely from scratch rather than recasting existing samples. Spurred \nby open-source models like DALL-E 2 [2]painting images from text and stable diffusion delivering general image editing \nabilities surpassing prior expectations, public enthusiasm surrounds creative AI. Simultaneously, rapid research progress \nexpands the horizons of what generative models can craft and constrain. \nThis survey charts the landscape of modern techniques molding generative model capabilities [3]. We focus on three \nmajor algorithmic[4] families powering generative systems today: 1) Generative adversarial networks which pit models \nagainst one another to enrich output quality, 2) Auto encoders that compress inputs into latent variables from which new \nsamples are constructed through decoding, and 3) Autoregressive models predicting sequential output tokens based on \nlearned conditional probabilities. Highlighting innovations in loss formulations, neural architectures, sampling methods, and \npre-training procedures, we track progress in genera ting increasingly realistic image, audio, video and 3D model outputs. \nDiffusion models for images and Transformers for text notably illustrate breakthroughs realized by stacking components \nfrom across algorithm families. We further discuss the role of data sets in training as well as evaluation shortcomings for \ngeneratively produced content of indeterminate ground truth quality. Beyond synthesizing existing techniques, we outline \nnewly emerging directions around control, multimodality, and responsible deploy ment that compose the future frontiers \nmolding generative AI. \n \nBabylonian Journal of Artificial Intelligence \nVol. (2023), 2023, pp. 10–14 \nDOI: https://doi.org/10.58496/BJAI/2023/003 ISSN: 3006-5437 \nhttps://mesopotamian.press/journals/index.php/BJAI \n \n \n11 Sakirin et al, Babylonian Journal of Artificial Intelligence Vol. (2023), 2023, 10 –571 \nBy reviewing fundamental approaches, paradigm -shifting models, tangible applications and open questions, this paper \nsurveys the machinery powering generative AI’ [5]s ascendance as one of the most rapidly developing and seminal fields \nwithin modern artificial intelligence research. \n \n \nFig. 1. Generative AI Tools \n2. ALGORITHMIC APPROACHES \n  \nThere are three predominant algorithm families that provide frameworks for developing generative AI systems - generative \nadversarial networks, auto encoders[6], and autoregressive models. Each approach has spawned numerous innovations \naddressing challenges of representation learning and sampling quality for synthesized content. We review foundational and \nemerging techniques within each category. \n1. Generative Adversarial Networks \nGenerative adversarial networks (GANs)[7] compose generative models as a two-player mini-max game between competing \nnetworks. The generator attempts to produce novel samples resembling the target data distribution. It is pitted against an \nadversary - the discriminator - that classifies outputs as either real or synthetically generated. The networks evolve through \nthis adversarial competition to enrich output quality. Architectural innovations enable generators to create strikingly realistic \nimages, audio, video, and 3D models. \nEarly GAN formulations suffered from problems like mode collapse with limited output diversity. Enhancements like \nWasserstein distance losses and progressive growing of g enerators address these. Conditional GANs  incorporate external \n\n \n \n12 Sakirin et al, Babylonian Journal of Artificial Intelligence Vol. (2023), 2023, 10 –571 \ninformation to guide synthesis. GAN training remains intrinsically tricky, but approaches like style-based generators (Karras \net al., 2020) sidestep typical generator architecture design challenges. \n2. Autoencoders \nAuto encoders compress input data into lower dimensional latent representations and then reconstruct inputs from these \nembedding. The encoder and decoder components are jointly trained. Variational autoencoders (VAEs)[8] impose additional \nconstraints for latent vectors to follow desired distributions. Once trained, the decoder portion generates new samples by \nsampling latent vectors from distribution priors and mapping encodings to outputs.VAEs present a more stable appro ach \nover GANs but tend to produce blurrier outputs lacking high frequency details. Very Deep VAEs begin ameliorating such \ndeficiencies. Adversarially Regularized Auto encoders augment VAE training with adversarial losses for refined realism. \n3. Autoregressive Models \nUnlike VAEs and GANs [9]which map latent vectors to outputs in parallel, autoregressive models factorize joint probabilities \nto predict portions of output sequences iteratively based on previously generated sections . Transformers shine light on \ncapturing long range dependencies. Autoregressive models produce strong results across modalities but requireexpensive \niterative calculations during sampling. Efforts like Parallel WaveNet accelerate audio synthesis. Masked autoencoders like \nBERT distill bidirectional representations useful for conditional generative tasks. Models including GPT-3 actively employ \nTransformers to attain state-of-the-art natural language generation, illustrating the scalability of autoregressive approach. \n3. GENERATING REALISTIC IMAGES \nThe ability to automatically create and edit photorealistic images presents a seminal challenge for generative algorithms. \nImpressive progress has been attained through combining adversarial objectives, encoder -decoder architectures, and \nsequential modeling under a common framework - diffusion models. Meanwhile, progress continues along numerous fronts \nto enrich detail, resolution, and fidelity of image synthesis. \n1. Diffusion Models \nDiffusion models gradually corrupt data then train networks to reverse that process towards pristine outputs. DDPM and \nDDIM establish strong performance by modeling stochastic differential equations governing corrupting Markov chains. \nLatent vector concatenation and adversarial losses in ADM boost sample quality. GLIDE  and DALL -E pretrain text-to-\nimage generators as masked autoencoders before fine-tuning on diffusion objectives. \nStable Diffusion adapts latent text encodings from GLIDE into the sampling loop of DDPM image synthesis. This \ncomposability of diverse algorithmic techniques under the diffusion framework drives Stable Diffusion's flexible \nmanipulation capabilities. Meanwhile, edge-generation refinement networks in DALL-E 2 and imaginative text-conditioning \nin Parti enrich creative expression. \n2. Ongoing Directions \nActive progress continues in numerous areas. GANs like StyleGAN -3 generate intricate high -resolution images through \nadaptive discriminator augmentations and noise conditioning. BigGANs  and StackGANs demonstrate scaling up GAN \nimage synthesis with added layers and parameters. Techniques for semantic image editing manipulate latent spaces for \nchanges aligned with human -intuitive features. Efforts also accelerate training on huge visual da tasets. These extend the \nhorizon of photorealistic image synthesis. \n3. Generating Audio, Video and 3D Renderings \nWhile image generation garners significant attention, generative models also empower realistic synthesis across modalities \nlike audio, video, and 3D shapes which introduce new intricacies around capturing temporal dynamics and high \ndimensionality. \n4. Audio Synthesis \nNeural audio synthesis models generate raw waveform samples directly. WaveNet first demonstrated this with an \nautoregressive approach. Follow ups like WaveGAN and WaveRNN improved efficiency. Recent models leverage \nadversarial training like GanSynth producing minutes long piano music coherent to human judgement. Parallel approaches \nalso help scale up synthesis, evidenced by Jukebox (Dhariwal et al., 2020) generating genre-specific music conditioning on \nmetadata. \n \n \n \n \n \n13 Sakirin et al, Babylonian Journal of Artificial Intelligence Vol. (2023), 2023, 10 –571 \n5. Video Generation \nSynthesizing realistic videos poses complex challenges of coordinating coherent visual features and dynamics across frames. \nProgress has occurred through extending GAN architectures. MoCoGAN decomposes motion and content into latent \nrepresentations for vid eo generation. StyleGAN -V adapts image based stylegan for video by predicting motion between \nstatic frames. Video prediction models like DVD-GAN-FP forecast plausible future frames by learning temporal dynamics. \n6. Model Generation \nGenerative techniques for 3D content create usable models for simulation, gaming, VR and 3D printing applications. \nVariational approaches like GRAF (Schwarz et al., 2020) couple a graph -network encoders for shape structure with mesh \ndecoders. Adversarial t raining improves outputs for domain specific 3D generation like cars, chairs, or clothing. Part \nassembly models like GAF construct 3D objects out of salient component pieces. Scalability however remains an open \nchallenge. As generative models expand across  modalities, key issues around temporal consistency and evaluation rigor \nbecome more prominent to address. Tackling these areas could enable generative AI to meaningfully impact a wider range \nof industries and applications. \n \n4. DATASETS AND ETHICS \nBehind groundbreaking generative models lie massive datasets advancing model capabilities. However, reliance on large \ncorpora of data also introduces questions around ethical sourcing, biases, and malicious use of synthetic media. \n1. Driving Datasets \nMany influential datasets underpin domains like image and language. FFT -based CLIP leverages 400 million image -text \npairs from internet sources to excel at image classification. LAION-5B's 5 billion image-text tuples (Schuhmann et al., 2022) \nfuel the DALL-E model family. For audio, LibriTTS's 585,000 hours of labeled speech (Zen et al., 2019) powers various \nvocoders. And Common Voice's 10,000 hours of donation -based recordings demonstrate more ethical curation.  These \ndatasets drove innovation but some prompted appropriation concerns, highlighting the need for responsible collection and \ndocumentation. Continual progress will require high -quality, ethically sourced datasets, necessitating standardized \nguidelines. \n2. Emerging Issues \nWhile generative models produce new data combinations, risks around data bias and misuse of synthetic content remain. \nDocumented failures of toxic text detection in language models prove that harmful biases persist in training data. Deepfakes \nmanipulating images and videos also introduce threats like political disinformation. Usage policies and technical protections \nare active discussion points. There also exists potential for generative models to help, such as using synthetic data to augment \nminority groups lacking representation. Overall there is a crucial need for ethical foresight to guide responsible advancement \nof generative AI alongside societal readiness.  Open questions cover a wide range swath from the nature of data curation \npowering future progress to governance of applications. Principled stances steering access, monitoring for harms, and \ninclusive development carry models beyond technical capabilities alone towards enacting positive real-world change.  \n5. CURRENT FRONTIERS AND FUTURE OUTLOOK \nWhile modern techniques already enable remarkable generative applications, many compelling research questions remain \nunresolved on the path towards more advanced creative AI systems.  Several open challenges currently confront the field \nacross dimensions of quality, capability, and control. Improved evaluation metrics measuring output realism and diversity \nlag behind synthesis advances. Training procedures for multimodal inputs and outp uts remain less explored, though \npromising for more representational embo diments of creativity. And dynamics governing emerging behavior and aligned \nvalue systems pose complex questions around controlling model development and deployment directions. \nIn the near term, techniques blending algorithmic innovations across families could compound strengths while ameliorating \nlimitations of individual approaches. For instance, autoregressive guidance of diffusion models to sharpen text-conditional \nimage gene ration based on progressive feedback. Dynamic composability also promotes accessibility to guide rapid \ninnovation. In the longer term, lifecycle considerations around data curation, carbon footprint, commercialization and \ncollaboration must complement technical progress. Real-world integration further necessitates international standards, safety \nguidelines and monitoring against potential harms.Ultimately, generative AI's profound creative potential compels informed, \ninclusive advancement of capabilities driven by humanistic values. Technique is not an endpoint - it must enable positive \nreal-world impact. This survey highlights achievements so far while framing directions needed to responsibly realize AI \ncreativity for social good. With ethical foundations and inspired vision, generative models hold immense promise to enhance \nhow we communicate, create and build mutual understanding.  \n \n \n14 Sakirin et al, Babylonian Journal of Artificial Intelligence Vol. (2023), 2023, 10 –571 \n6. CONCLUSION \nThis survey has traced the current landscape and frontiers of generative artificial intelligence - where algorithms \nautonomously synthesize novel realistic content without simply recasting existing samples. We structured an analysis across \nthree predominant algorithm families powering advances in this machine creativity. Generative adversarial networks were \nshown to produce striking creations across modalities like images, audio, and video by pitting generator models against \nadversarial discriminators. Vari ational autoencoders enable sampling from learned latent spaces using encoder -decoder \nneural network pipelines. Autoregressive models iteratively predict sequence outputs while capturing long -range \ndependencies. Technique consolidations under the diffusion  model framework meanwhile drive recent versatile image \nmanipulation breakthroughs.  We highlighted innovations in loss formulations, neural architectures, and pre -training \nstrategies that enriched generative modeling capabilities over time. Performance imp rovements were charted through \nexample models like StyleGAN, WaveNet, and GPT-3 which represent paradigm shifts for image realism, audio generation, \nand language modeling respectively. Beyond common datasets powering progress, crucial open questions around evaluation \npractices, multimodal synthesis, responsible model development, and real-world integration remain active frontiers requiring \ninterdisciplinary perspectives.  Ultimately, accelerating generative AI's capabilities syncs tightly with deliberating i ts \nethically guided integration towards positive change. While technical machinery molded the era of machine creativity we \nsurveyed here, it is proactive translation of these tools to advance societal benefit that defines the vital path ahead. Generative \nAI thereby warrants informed, inclusive progress in line with human values - for technological potential is only optimally \nrealized through respecting the many dimensions of lives it stands to transform. \nFunding \nNone. \nConflicts of Interest \nThe authors declare that there is no conﬂict of interests regarding the publication of this paper. \nAcknowledgment \nThe authors would like to express their gratitude to the Balaji Institute of Technology and Science. Please accept my \nsincere gratitude for the useful recommendations and constructive remarks provided by the anonymous reviewers. \n \nReferences \n[1] D. Baidoo-Anu and L. O. Ansah, \"Education in the era of generative artificial intelligence (AI): Understanding \nthe potential benefits of ChatGPT in promoting teaching and learning,\" Journal of AI, vol. 7, no. 1, pp. 52 -62, \n2023. \n[2] G. Harshvardhan, M. K. Gourisaria, M. Pandey, and S. S. Rautaray, \"A comprehensive survey and analysis of \ngenerative models in machine learning,\" Computer Science Review, vol. 38, p. 100285, 2020. \n[3] S. Noy and W. Zhang, \"Experimental evidence on the productivity effects of generative artificial intelligence,\" \nAvailable at SSRN 4375283, 2023. \n[4] K. Wach et al., \"The dark side of generative artificial intelligence: A critical analysis of controversies and risks of \nChatGPT,\" Entrepreneurial Business and Economics Review, vol. 11, no. 2, pp. 7-30, 2023. \n[5] M. Castelli and L. Manzoni, \"Generative models in artificial intelligence and their applications,\"  vol. 12, ed: \nMDPI, 2022, p. 4127. \n[6] Z. Bahroun, C. Anane, V. Ahmed, and A. Zacca, \"Transforming education: A comprehensive review of generative \nartificial intelligence in educational settings through bibliometric and content analysis,\" Sustainability, vol. 15, \nno. 17, p. 12983, 2023. \n[7] D. K. Kanbach, L. Heiduk, G. Blueher, M. Schreiter, and A. Lahmann, \"The GenAI is out of the bottle: generative \nartificial intelligence from a business model innovation perspective,\" Review of Managerial Science, pp. 1-32, \n2023. \n[8] V. Ratten and P. Jones, \"Generative artificial intelligence (ChatGPT): Implications for management educators,\" \nThe International Journal of Management Education, vol. 21, no. 3, p. 100857, 2023. \n[9] R. T. Hughes, L. Zhu, and T. Bednarz, \"Generative adversarial networks –enabled human–artificial intelligence \ncollaborative applications for creative and design industries: A systematic review of current approaches and \ntrends,\" Frontiers in artificial intelligence, vol. 4, p. 604234, 2021. \n \nVol.:(0123456789)1 3\nElectronic Markets (2023) 33:63 \nhttps://doi.org/10.1007/s12525-023-00680-1\nFUNDAMENTALS\nGenerative artificial intelligence\nLeonardo Banh1  · Gero Strobel1\nReceived: 26 June 2023 / Accepted: 7 November 2023 / Published online: 6 December 2023 \n© The Author(s) 2023\nAbstract\nRecent developments in the field of artificial intelligence (AI) have enabled new paradigms of machine processing, shift-\ning from data-driven, discriminative AI tasks toward sophisticated, creative tasks through generative AI. Leveraging deep \ngenerative models, generative AI is capable of producing novel and realistic content across a broad spectrum (e.g., texts, \nimages, or programming code) for various domains based on basic user prompts. In this article, we offer a comprehensive \noverview of the fundamentals of generative AI with its underpinning concepts and prospects. We provide a conceptual \nintroduction to relevant terms and techniques, outline the inherent properties that constitute generative AI, and elaborate \non the potentials and challenges. We underline the necessity for researchers and practitioners to comprehend the distinctive \ncharacteristics of generative artificial intelligence in order to harness its potential while mitigating its risks and to contribute \nto a principal understanding.\nKeywords Generative AI · Artificial intelligence · Deep learning · Deep generative models · Large language models\nJEL Classification C8 · M21\nIntroduction\n“A groundbreaking fusion of data-driven creativity and \nartificial intelligence, poised to redefine the bounda-\nries of innovation and transform the future of digital \nlandscapes.”\n-ChatGPT on GPT-4, Mar 23 Version, on Generative AI\nIn an era where applications like ChatGPT set records \nfor the fastest-growing user base by demonstrating unprec-\nedented domain-independent expertise (Hu, 2023), the con-\ncept of “Generative Artificial Intelligence” (GAI) emerges \nas a disruptor in the digital landscape (Dwivedi et al., 2023; \nTeubner et al., 2023; Wessel et al., 2023). With capabili-\nties to generate high-quality, contextually relevant content \nalmost indistinguishable from human-created work, discus-\nsions arise on whether this new technology even holds early \nsigns of artificial general intelligence (Bubeck et al., 2023; \nThe Washington Post, 2022). Regardless of discussions on \nAI’s sentiency, the latest advancements in machine learn -\ning (ML) and deep learning (DL) have extended traditional, \ndata-driven AI tasks such as predictions, classifications, or \nrecommendations toward the generation of unique, realis-\ntic, and creative content. Prevalent collaborations between \nhumans and intelligent systems in businesses and private \nlife have been shaped by the adoption of AI in various ways, \nespecially in the field of process optimization and decision-\nmaking (Brynjolfsson & McAfee, 2016; Burström et al., \n2021; Moussawi et al., 2021). GAI addresses this develop-\nment by providing novel augmentation and automation pros-\npects in form of innovative services and business models \n(Huang & Grady, 2022; Mondal et al., 2023), e.g., by assist-\ning customer support employees with suggestions of appro-\npriate conversation responses (Brynjolfsson et al., 2023). \nThe fast-changing and continuously evolving landscape of \nGAI calls for an extensive conceptualization of the proper -\nties and capabilities to fathom the phenomenon (Dwivedi \net al., 2023; Strobel et al., 2024).\nResponsible Editor: Christian Matt\n * Leonardo Banh \n leonardo.banh@uni-due.de\n Gero Strobel \n gero.strobel@uni-due.de\n1 University of Duisburg-Essen, Universitätsstraße 2, \nEssen 45141, Germany\n Electronic Markets (2023) 33:63\n1 363 Page 2 of 17\nWhile GAI research and development is continuing to \ninvest toward better, faster, and more capable models (e.g., \nMicrosoft, 2023), studies on the fundamental principles, \napplications, and socio-economic impact remain largely \nunexplored in the academic discourse (Strobel et al., 2024; \nSusarla et al., 2023; Wessel et al., 2023). GAI provides inno-\nvation opportunities for various domains (e.g., networked \nbusinesses and digital platforms) but also comes with chal-\nlenges (e.g., transparency, biases, and misuse) that need to \nbe addressed for successful implementations (Houde et al., \n2020; Schramowski et al., 2022; van Slyke et al., 2023). \nHowever, an examination of the key concepts is yet to be \nconducted, leaving a clear image and understanding of gen-\nerative AI undefined. To overcome that shortcoming, this \narticle provides an introduction to the fundamentals of gen-\nerative AI, with its concepts, applications, and challenges. \nTo do so, we exploratively synthesize recent literature on the \ntechnical foundation leading toward generative AI in combi-\nnation with contemporary empirical examples of generative \nAI models and applications. Our aim is to conceptualize the \nkey properties of GAI and differentiate them from ML and \nDL methods, to foster the understanding of the theoretical \nfoundations of generative AI, and to guide further endeav -\nors in examining as well as designing generative AI-based \nsystems.\nThe remainder of the article is structured as follows: In \nthe next section, we conceptualize generative AI and provide \na distinction between related AI methods as well as outline \nthe technological foundations. Afterward, we elaborate on \nthe prospects and applications of the generative AI value \nchain and examine the impact of different generation modal-\nities. Then, we address the potential challenges of adopt-\ning generative AI before concluding with a brief research \noutlook.\nConceptualization\nThe field of artificial intelligence (AI) has taken a promi-\nnent place in research and practice across various disciplines \nfor the past decades. Especially in information systems (IS) \nresearch, the socio-technical impact of AI as a phenom-\nenon is at the core of investigation (Ågerfalk et al., 2022; \nBerente et al., 2021). These technologies have transformed \nthe way we interact with data and make decisions, leading \nto uncharted ways in society as well as economy (Fügener \net al., 2021; J. Li et al., 2021; van den Broek et al., 2021). \nHowever, as we venture into the new era of generative AI, \nit becomes increasingly crucial to understand the core con-\ncepts and distinctions within GAI as a rapidly evolving tech-\nnology. To differentiate GAI from other AI concepts and \nprovide a fundamental conceptualization, we will present \na brief overview of AI and its subfields, machine learning \n(ML) and deep learning (DL), in the next section (Fig.  1). \nAfterward, we continue to elaborate on how DL has driven \nthe development of deep generative models to enable distinct \nGAI characteristics and capabilities, ultimately leading to \nan even broader array of AI applications and opportunities \nfor various fields.\nFrom artificial intelligence to deep learning\nArtificial intelligence is considered an umbrella term, span-\nning over different computational algorithms capable of \nperforming tasks that typically require human intelligence, \nsuch as understanding natural language, recognizing pat-\nterns, making decisions, and learning from experience \n(Castelvecchi, 2016; Winston, 1993). Early AI systems, such \nas expert systems and knowledge bases, were rule based and \naimed at supporting users and businesses in decision-making \nFig. 1  Generative AI and \nother AI concepts (inspired by \nGoodfellow et al., 2016, p. 9; \nJaniesch et al., 2021, p. 687)\nArtificial Intelligence\nMachine Learning\nDeep Learning\nGenerative AI\ne.g., expert systems, knowledge bases, …\ne.g., support vector machines, decision trees, k-nearest neighbors, … \ne.g., neural networks, convolutional neural networks, …\ne.g., large language models, generative\nadversarial networks, variational autoencoders,\nlatent diffusion models, …\nElectronic Markets (2023) 33:63 \n1 3 Page 3 of 17 63\n(Harmon, 1985; Patterson, 1990). Machine learning as a \nsubfield of AI deals with the development of algorithms \ncapable to autonomously solve tasks through exposure to \ndata without being explicitly programmed—i.e., learning \n(Brynjolfsson & Mitchell, 2017). In the realm of ML, there \nare several types of learning approaches based on the nature \nof the data and the desired outcome. Supervised learning is a \ncommon approach, e.g., for applications in commercial con-\ntexts, as algorithms are trained on labeled datasets to clas-\nsify or forecast (business) data (Janiesch et al., 2021). The \nalgorithm learns to map inputs to outputs and, thus, is capa-\nble of making predictions on new, unseen data. Moreover, \nunsupervised learning (i.e., discovering hidden structures or \npatterns within unlabeled data) and reinforcement learning  \n(i.e., learning optimal decision-making by interacting with \nan environment and maximizing cumulative rewards over \ntime through trial and error) are further learning strategies in \nML (Kühl et al., 2022). What ML algorithms share in com-\nmon are their discriminative properties, i.e., the goal of pro-\ncessing data to conduct classification, regression, or cluster \nand determine decision boundaries. Exemplary algorithms \ninclude decision trees, k-nearest neighbors, or support vector \nmachines (Ray, 2019). Deep learning is a more advanced \nsubset of ML and leverages artificial neural networks to \nmodel complex data representations and automatically detect \ncorrelations and patterns in large datasets (Janiesch et al., \n2021; Samtani et al., 2023). Neural networks are computa-\ntional models inspired by the structure and function of the \nhuman brain, consisting of interconnected layers of artificial \nneurons (Goodfellow et al., 2016). In DL, neural networks \ncomprise of multiple hidden layers in a nested architecture \nto learn hierarchical feature representations from the data, \nleading to improved performance on various tasks. Thus, DL \nis capable of processing high-dimensional data in various \ndomains, ranging from one-dimensional data like signals \nand texts to multidimensional data such as images, video, or \naudio (LeCun et al., 2015). These advances have enabled a \nplethora of use cases across different domains, from societal \ngood, such as improving healthcare and environmental sus-\ntainability (Piccialli et al., 2021; Schoormann et al., 2023; \nStrobel et al., 2023), to electronic markets, where DL can \noptimize pricing, serve as recommendation systems, forecast \ndemands, and detect fake consumer reviews (Ferreira et al., \n2016; M. Li et al., 2022; Zhang et al., 2023b).\nToward generative AI\nFueled by advancements in DL techniques, deep genera-\ntive models (DGMs) have emerged as a class of DL models \nto generate new content based on existing data, creating a \nvariety of new possibilities for AI applications (Lehmann & \nBuschek, 2020; Tomczak, 2022). These models are trained \nto understand complex data distributions, which allows \nthem to produce outputs that closely resemble real-world \ndata. By leveraging statistics, the goal of DGM training is \nto learn high-dimensional probability distributions from a \nfinite training dataset and create new, similar samples that \nresemble an approximation to the underlying class of training \ndata (Ruthotto & Haber, 2021). While discriminative models \nfocus on modeling the relationship between input features \nand output labels, generative models learn the inherent data \nstructure and generation processes (Jebara, 2004). Generative \nmodels have been around for decades, with, for example, hid-\nden Markov models or Bayesian networks aiming to model \nstatistical problems involving time series or sequences (Gm \net al., 2020). Nonetheless, DGMs relying on neural networks \nhave paved the way for significantly higher-quality generated \ncontent in recent advancements in the field of so-called gen-\nerative AI. Thus, the goals of DGMs differ from traditional \ndiscriminative AI models (e.g., in ML) because the focus \nlies on the probabilistic generation of new data instead of \ndetermining extant data’s decision boundaries (e.g., classi-\nfication, regression, or clustering) (Tomczak, 2022; Weisz \net al., 2023). In the following, we will focus on DGMs as \nthe underpinning of GAI and give an overview of four core \nDGMs that have shaped the evolution of GAI in Table 1.\nTo leverage DGMs in GAI applications, they can be \ntrained to generate new data and enable a variety of use \ncases (we refer to DGMs implemented in GAI applica-\ntions as GAI models). Training a GAI model can be differ -\nent than a discriminative AI model due to semi-supervised \nlearning, a combination of learning techniques leveraging a \nsmall amount of labeled data (i.e., supervised) followed by \nextensive unlabeled data (i.e., unsupervised) (Kingma et al., \n2014). For instance, recent GAI models apply techniques \nlike supervised fine-tuning (SFT), reward models, and rein-\nforcement learning via proximal policy optimization (PPO) \nto achieve an alignment of the model with the developers’ \nintentions and values (OpenAI, 2023; Ouyang et al., 2022). \nThis unique approach allows the training of very large data-\nsets required for GAI models without the need for difficult \ncomplete labeling.\nThe application system functions as an interface for the \nuser to interact with a GAI model. Prompting is an interac-\ntion technique and unique GAI property that enables end \nusers using natural language to engage with and instruct GAI \napplication (e.g., LLMs) to create desired output such as \ntext, images, or other types (Dang et al., 2022; Liu & Chil-\nton, 2022). Depending on the application, prompts vary in \ntheir modality and directly influence the mode of operation. \nFor instance, text-to-image applications use textual prompts \ndescribing the visuals of the desired image, while image-\nto-image applications rely on an input image to steer the \ngeneration process.\n Electronic Markets (2023) 33:63\n1 363 Page 4 of 17\nBy design, the outputs of generative AI models are prob-\nabilistic and not replicable compared to the deterministic \noutcomes of discriminative AI—i.e., variance (Weisz et al., \n2023). For one exact input prompt, a GAI application will \ngenerate varying outputs each time it is prompted, but the \nresults remain valid and prompt fulfilling. On the other hand, \ndifferent input prompts can lead to the same goal. Hence, \nformulating a meaningful prompt that leads to the desired \noutcome is based on trial-and-error process, e.g., by rephras-\ning textual prompts with the same keywords. The field of \nprompt engineering deals with systematically constructing \nprompts to improve the generated outputs (Liu & Chilton, \n2022).\nBased on the heuristic approach of prompt engineering \nand the inherent variance in the generated content, GAI \nusers continuously and iteratively specify their desired \ntasks as input prompts to generate outputs until their task is \nsolved. The primary goal of generating new, probabilistically \nproduced data (i.e., content) with varying outputs based on \nthe same input distinguishes generative AI from discrimina-\ntive AI, which pursues boundary determination by analyzing \ndata and making a decision (see Fig.  2). Hence, a primary \ndifference lies in the role of data, as GAI leverages very large \ndatasets in its generative model to produce diverse content, \nwhile discriminative AI processes user data based on a (pre-\ntrained) algorithm.\nProspects and applications of generative AI\nComplementing discriminative AI, GAI has recently \nemerged as a novel tool with a wide range of new possibili-\nties impacting multiple sectors, from education and health-\ncare (Brand et al., 2023; Burger et al., 2023; Cooper, 2023) \nTable 1  Overview of core deep generative models\nDeep generative model Description\nGenerative adversarial network (GAN) Generative adversarial networks consist of two competing neural networks: a generator and a discrimi-\nnator (Goodfellow et al., 2020). The generator creates realistic data samples, while the discrimina-\ntor distinguishes between real and generated samples (Pan et al., 2019). Both neural networks are \ntrained together until the discriminator is not able to differentiate both samples (Janiesch et al., 2021). \nThis adversarial competition results in the generator improving its data generation capabilities over \ntime, eventually producing high-quality, realistic outputs. Hence, GANs find various applications, \nfor instance, in image generation and manipulation, object detection and segmentation, and natural \nlanguage processing (Aggarwal et al., 2021; Gui et al., 2023)\nVariational autoencoder (VAE) Variational autoencoders employ a neural network to learn encoding compressed input data into a \nlower-dimensional latent space and then decode the data by reconstructing the original data from \nthe latent space representation (Kingma et al., 2014). By optimizing a variational lower bound on \nthe data likelihood in a probabilistic approach, VAEs can generate new samples that resemble the \noriginal data distribution. Typical use cases for VAEs can be seen in the synthetic generation and \nreconstruction of data such as images, in anomaly detection, and recommendation systems (Wei & \nMahmood, 2021)\nTransformer Transformer models have become the basis for many state-of-the-art natural language processing tasks \nand succeeding models. They are a specific type of neural network architecture that employ self-\nattention mechanisms to capture long-range dependencies in the data, making them well-suited for \nlarge-scale language modeling tasks (Vaswani et al., 2017)\nGenerative pre-trained transformers (GPT) build on the transformer architecture and were trained with \nlarge datasets of unlabeled data (Brown et al., 2020). Due to their large size (i.e., a very large number \nof trainable parameters), GPT trained on text data are often referred to as large language models \n(LLMs) (Schramowski et al., 2022). The goal of LLMs is to generate novel, coherent, contextually \nrelevant human-like text by predicting which token is most likely to occur after the prior tokens in a \nsentence (Brown et al., 2020; H. Li, 2022). Hence, LLMs can serve as the foundation for conversa-\ntional AI tools like ChatGPT (Teubner et al., 2023). Besides conversing, the large amount of informa-\ntion stored in LLMs can be used for text generation, writing, or even programming, e.g., to support \nscholars (Cooper, 2023; Lund et al., 2023)\nLatent diffusion model (LDM) Latent diffusion models are transformer based and build on the concepts of denoising score matching \nand contrastive divergence to learn a stochastic data generation process (Rombach et al., 2022). In \nLDMs, the generation process starts with a simple initial distribution, such as Gaussian noise. Then, \nthe data gets gradually refined through a series of noise-reduction steps following a predefined dif-\nfusion process through a latent space (Ho et al., 2020). The key advantage of LDMs is their ability \nto learn complex data distributions without requiring adversarial training (as in GANs) or optimiz-\ning variational lower bounds (as in VAEs). They also feature improved stability over other DGMs \nduring training to be less prone to issues like mode collapse (Kodali et al., 2017; Rombach et al., \n2022), making them well-suited for high-quality and detailed outputs, such as high-resolution image \nsynthesis (Ho et al., 2020)\nElectronic Markets (2023) 33:63 \n1 3 Page 5 of 17 63\nto networked businesses (Dwivedi et al., 2023; Wessel et al., \n2023). These emerging applications inherit the generativity \nand variance properties of GAI and, therefore, are capable of \nproducing unique and creative content, going beyond mere \nassistance. Hence, GAI becomes increasingly multidiscipli-\nnary, enabling disruptive innovations and automating even \ntraditionally creative tasks, e.g., by generating customized \ncontextual texts or images, facilitating new opportunities for \nbusinesses to innovate and differentiate themselves in the \ncompetitive economic landscape (Dwivedi et al., 2021; Lund \net al., 2023; Pavlik, 2023).\nGenerative AI finds its utility across various modalities, \nincluding the generation of text, image, video, code, sound, \nand other produced content, such as molecules or 3D ren-\nderings (see Table 2). For example, GAI applications aim to \ncreate tailored marketing content, generate realistic (prod-\nuct) images or videos, and even assist in software devel-\nopment by generating code (Bakpayev et al., 2022; Elasri \net al., 2022; Kowalczyk et al., 2023). Several modalities can \nserve as the input for GAI models. Distinguishing the dif-\nferent modality types, unimodal models generate the same \noutput type as their input type, e.g., text-to-text or image-\nto-image generation, whereas multi-modal models combine \ndifferent input and output types, for instance, in a text-to-\nimage or code-to-text scenario. Different multi-modal mod-\nels can subsume as x-to-modality models (e.g., x-to-text or \nx-to-image).\nExamining the architecture of GAI-based systems, three \nmajor component layers can be identified: model layer, con-\nnection layer, and application layer (see Fig. 3). These parts \nembed generative AI in its information systems context and \ndraw a boundary from external entities that can be inter -\nacted with its environment (e.g., users, organizations) and \ndata (i.e., public and enterprise data) (Samtani et al., 2023). \nInside the boundaries of GAI-based systems, the preva -\nlent characteristics of generativity and variance persist and \naffect all layers and processes. The model layer  comprises \nthe pre-trained, deployable GAI artifact (i.e., a DGM) for \napplication systems. Depending on the training dataset, gen-\neral purpose models aim at solving a wide range of tasks in \nmultiple domains (e.g., GPT-4), whereas customized models \nare designed for domain-specific tasks and were, therefore, \ntrained on highly specific data (e.g., CodeBERT).\nIntegrating these models into a system environment that \naffects people and organizations leads to the application \nlayer of generative AI. By providing a proper context for \nthe artifact, users are able to leverage the capabilities of GAI \nmodels for a specific application use case. Observing the \ntrend of various recently emerging GAI applications build-\ning on top of existing models, it becomes apparent to further \ndistinguish between end-to-end applications that are based \non undisclosed, proprietary models (e.g., Midjourney) and \nopen applications that are built around open-source models \nor leverage publicly accessible pre-trained models (e.g., Jas-\nper and Elicit using OpenAI’s GPT-3 (Elicit, 2022; Jasper, \n2022)). Huang and Grady (2022) describe GAI applications \nas a “UI layer and ‘little brain’ [i.e., application layer] that \nsits on top of the ‘big brain’ that is the large general-purpose \nmodels [i.e., model layer].” This perspective emphasizes that \nnew business models and applications can be developed \nwithout the need to train large GAI models from scratch by \nleveraging publicly available application programming inter-\nfaces (API) or AI-as-a-service platforms (Burström et al., \n2021; Janiesch et al., 2021; Lins et al., 2021). Indeed, the \naccessibility and availability of pre-trained GAI models fos-\nter value co-creation and can be leveraged via a connection \nlayer (e.g., Hugging Face). Fully integrated GAI systems, \non the other hand, employ their own, custom-trained propri-\netary models. In many cases, end-to-end GAI applications \nrepresent fully integrated systems (e.g., GitHub Copilot), \nwhile open GAI applications leverage external models via \nAPIs (e.g., Stable Diffusion). Overall, GAI models may aim \nfor general purposes  or customized  tasks regardless their \nconnection or application characteristics (see Fig.  3). To \nenrich GAI-based systems with additional data beyond their \ntraining state and the GAI boundary, external data sources \nGenerate\nDiscriminative Model\nPrompt\nGenerative Model\nData Decision\nCreation\nVariance\nGenerativity\nContent\nBoundary \nDetermination\nSpecifyGenerative \nAI\nDiscriminative\nAI\nFig. 2  Procedural differences of generative AI and discriminative AI\n Electronic Markets (2023) 33:63\n1 363 Page 6 of 17\ncan be connected. Enterprise data (e.g., internal documents, \nenterprise resource planning (ERP) systems, knowledge \nbases) and public data (e.g., the Internet, libraries, social \nmedia) may serve as complementary, contextual data that \nGAI applications can further draw upon for more relevant \nand personalized results.\nEmploying GAI in enterprises can extend the level of \nassistance for workers and open up opportunities for aug -\nmentation and automation of the job, leading to new forms \nof collaborations between humans and machines (Einola & \nKhoreva, 2023). Furthermore, GAI transforms the way busi-\nnesses operate in their daily tasks, innovate, and interact \nTable 2  Overview of different output modalities for generative AI applications\nModality Description\nText X-to-text applications are centered around text generation and natural language processing. The goal is to generate human-like writ-\nten text that fits the user’s input prompt by providing a meaningful answer within the context. For instance, chatbots like OpenAI’s \nChatGPT imitate textual conversations with the user and can be guided to output text artifacts as desired (OpenAI, 2023). Fur-\nthermore, text-producing applications can be leveraged for content creation (e.g., copywriting or specific writing in e-commerce \ncontexts) (Bakpayev et al., 2022; Brand et al., 2023). Moreover, text generation can support processes in sales or support by provid-\ning the ability to produce customized texts tailored toward the requests (Mondal et al., 2023). Systems integrating GAI models \nwith further knowledge bases (e.g., enterprise data and Internet access) extend the available information beyond the model’s initial \ntraining dataset\nImage X-to-image applications generate images based on the user’s prompting. Relying on GANs or diffusion models as DGMs, synthetic \nimages are created that find use cases in marketing, design and fashion, or creative fields in the form of new visual art (Haase \net al., 2023; Mayahi & Vidrih, 2022; Zhang et al., 2023a). For instance, Stable Diffusion is an open-sourced x-to-image model that \nenables the generation of images in multiple GAI applications (Rombach et al., 2022). Moreover, generated synthetic images can \nact as training data for further ML models to train classifiers (e.g., medical images to detect diseases (Ali et al., 2023)). Besides a \ntext-to-image creation process, image editing capabilities are possible, e.g., via image-to-image systems that manipulate and extend \nimages according to the user’s prompting (Oppenlaender, 2022)\nVideo X-to-video applications deal with the creation of synthetic videos, i.e., dynamic motion images. New video clips are generated by \ndescribing the content of the desired video footage (text-to-video) or applying the style and composition via text or image prompt \nto a source video (video-to-video) (Esser et al., 2023). These prospects allow the fast and convenient creation and editing of videos \nvia natural language and other modalities (Zhan et al., 2021). Thus, not only videographers benefit from x-to-video applications \nbut also people without filming and editing skills are enabled to creatively express themselves due to an accessible creation process \n(Anantrasirichai & Bull, 2022). Besides recreational and entertainment purposes, x-to-video GAI models find application in sales \nand marketing (e.g., product marketing videos), onboarding and education (e.g., virtual avatars in training videos), or in customer \nsupport (e.g., how-to videos) (Leiker et al., 2023; Mayahi & Vidrih, 2022). As an exemplary application, Synthesia is a video crea-\ntion platform specialized in generating professional videos with virtual avatars and synthetic voiceovers (Synthesia, 2023)\nCode In the realm of software development, x-to-code GAI applications offer transformative potential in how developers work and code \nby providing x-to-text capabilities specific to programming languages. Models like CodeBERT (Feng et al., 2020) or GraphCode-\nBERT (Guo et al., 2021) were trained on programming code to generate source code from natural language or modeling languages \nfor new software programs. Several x-to-text models also offer coding capabilities because general-purpose LLMs are trained with \nincreasingly large datasets that contain code (e.g., Stability.ai, 2023). Programmers using applications such as GitHub Copilot are \nsupported by automatically written chunks of code, ideas converted into actionable scripts, auto-completion functions, generated \nunit tests, duplicate code detection, and bug fixing (Sun et al., 2022). These automation potentials allow developers to focus on \nhigher-level tasks and problem solving, enhancing their productivity and the final product’s overall quality, reducing time-to-market, \nsupporting rapid prototyping, and promoting continuous innovation for the product and business\nAudio X-to-audio applications focus on audio content generation and comprise, for instance, the generation of speech with synthetically \ngenerated human-like voices (Borsos et al., 2022; Wang et al., 2023). Especially text-to-speech and speech-to-speech models are \nbeing heavily researched and can be used to power various applications, ranging from digital assistants and customer services to \naudiobook and training narration and accessibility tools (Moussawi et al., 2021; Qiu & Benbasat, 2005). GAI models like Micro-\nsoft’s VALL-E (Wang et al., 2023) offer a more personalized and engaging user experience by enabling realistic voice modeling. \nMoreover, x-to-sound models find application in music creation. By specifying genres or melodies via prompts, unique pieces of \nmusic can be generated that respect the original intent (Agostinelli et al., 2023). GAI models such as MusicLM (Agostinelli et al., \n2023) help musicians in their creative process, offering inspiration and aiding the composition of complex pieces. Businesses in the \nmusic industry can leverage high-fidelity music generation to create customized soundtracks for marketing, movies, or video games, \nsignificantly reducing the cost and time associated with traditional music production (Anantrasirichai & Bull, 2022; Weng & Chen, \n2020)\nOther The applications of GAI extend beyond the stated modality types and domains, impacting multiple other, specific areas. For instance, \nx-to-molecules models like AlphaFold (Jumper et al., 2021) and OpenBioML (Murphy & Thomas, 2023) generate viable protein \nstructures and design new molecules by generating valid, novel molecular structures, supporting drug discovery and bioengineer-\ning researchers (Walters & Murcko, 2020). 3D modeling is also impacted by GAI applications such as DreamFusion (Poole et al., \n2023), Nvidia GET3D (Gao et al., 2022), and Point-E (Nichol et al., 2022), which generate realistic and complex 3D models that \nfacilitate a range of applications from product design and architecture to virtual reality and game development\nElectronic Markets (2023) 33:63 \n1 3 Page 7 of 17 63\nwith their customers (Brynjolfsson et al., 2023 ; Mondal \net al., 2023). Thus, the prospects of value co-creation come \nin hand with potential changes in human work roles, requir-\ning workforces in various domains to adapt their tasks as \na diverse set of tasks could be impacted by generative AI \n(Brynjolfsson & McAfee, 2016; Eloundou et al., 2023). The \nongoing diffusion of AI into businesses gets accelerated by \nGAI applications, resulting in a possible replacement of \nhuman jobs on the one hand but also the creation of new jobs \n(e.g., for prompt engineers or with new business models) on \nthe other hand (Einola & Khoreva, 2023). Hence, the effect \non the labor market by the disruption needs to be discussed, \nand businesses should seek to understand and embrace the \npotential of generative AI (Eloundou et al., 2023; Willcocks, \n2020).\nChallenges for generative AI‑based systems\nWhile generative AI holds transformative potential for \nindividuals, organizations, and society due to its vast pos-\nsible application space, the technology also inherits vari-\nous challenges that parallel those of traditional ML and \nDL systems. The domain of electronic markets is a prime \nexample that moved into the center of transformation \ndue to its latest focus on data-driven efforts (Selz, 2020). \nOutlining and emphasizing these challenges relevant for \nresearch and practice helps to raise awareness of the con-\nstraints as well as supports future efforts in developing, \nimplementing, and improving GAI-based systems.\nBias\nBecause of GAI’s data-driven nature, data quality plays an \nessential role in how GAI-based systems perform and, thus, \nhow feasible their adoption for real-world scenarios in busi-\nness contexts is. Similar to their traditional discriminative AI \nrelatives, GAI models are prone to bias causing biased deci-\nsions, disadvantages, and discriminations (Ferrara, 2023; \nSchramowski et al., 2022). Biases manifest in different ways \nand evolve primarily during two development phases of an \nAI-based system: training and inference.\nData bias gets injected during the model’s training phase \nand leads to biased results because of faulty datasets. Fac-\ntors such as non-representative, imbalanced sampling, incor-\nrect labeling, and mismeasured features during the selection \nand processing of datasets hinder an unbiased training of \nthe GAI model, ultimately leading to biased algorithmic \noutcomes (Mehrabi et al., 2022; Ntoutsi et al., 2020). The \ndevelopment of large-scale training datasets is especially \nFig. 3  Conceptual framework of \ngenerative AI\nApplication\nLayer\nModel\nLayer\nConnection\nLayer\nEnvironment\nGenerative AI\nData\nKnowledge \nBaseInternetLibraries\nPublic Data Enterprise Data\nAPI\nOpen\nGeneral Purpose\nFully-Integrated\nEnd-To-End\nCustomized\nERPSocial\nMedia Documents\n Electronic Markets (2023) 33:63\n1 363 Page 8 of 17\nimportant for GAI models and often involves strategies of \nscraping public-available data on the Internet (Schuhmann \net al., 2022). This approach is usually performed unsuper -\nvised and autonomously, which complicates the dataset’s \nquality assurance because of its large quantity of unstruc-\ntured data. Since GAI models are often trained to be gen-\neral-purpose and multi-modal, they require and rely even \nmore on such training datasets. Hence, moderating potential \ndata bias is crucial for applications in business contexts like \nelectronic markets due to the closeness to customers (e.g., \npoints of contact via advertisements, social media, or cus -\ntomer support). Furthermore, social bias as a form of data \nbias can cause distorted views in generated texts or images \nand should be considered as well as mitigated (Baeza-Yates, \n2018).\nAlgorithmic bias is introduced during the inference phase, \nindependent from the model’s training dataset (Mehrabi \net al., 2022). In this case, the models have been trained on \ndiverse, unbiased input data, and either the model’s algo-\nrithm or the application around it introduces biases affecting \nusers. Overfitting is a typical phenomenon that originates \nfrom the chosen learning strategies or optimization functions \nand causes biased algorithmic outcomes (Danks & London, \n2017; Hooker, 2021). In this case, GAI models might intro-\nduce biases not reflected in the data because they fail to learn \nthe data distribution correctly. Likewise, the presentation of \nand the user interaction with GAI-based systems can cause \nbiases, such as when only selected generated content (e.g., \none image out of multiple variants) is shown to the user \n(Baeza-Yates, 2018).\nThus, generative AI applications exerting biased results \ninfluence users’ opinions and judgement and require control \nmechanisms (Jakesch et al., 2023a). Strategies should be \ndeveloped to prevent, detect, and mitigate biases in order to \nsafeguard users and ensure the service quality and reputa-\ntions of a company. One approach to steer the quality of \noutputs from GAI models is via reinforcement learning from \nhuman feedback (RLHF) (Christiano et al., 2017; Griffith \net al., 2013). The technique involves feedback from human \nevaluators to guide the model’s training process, with evalu-\nators assessing and comparing the quality of generated out-\nputs. This approach enables generative models to refine their \noutput generation process, aiming for better alignment with \nhuman expectations and objectives. Nevertheless, determin-\ning what content is “good” or “right” remains a difficult and \nbias-prone task (Teubner et al., 2023).\nTransparency\nThe need for explainability arises with the unpredictability of \nthe inherent generative nature of GAI models and the over-\nall functionality of ML models as “black boxes” (Janiesch \net al., 2021; Meske et al., 2022). While the impact of GAI \nmay not reach as far as discriminative AI use cases (e.g., \ndecision-making or dynamic pricing), research on explain-\nable generative AI is still in its infancy, and the justification \nfor more transparency is without a doubt (Brasse et al., 2023; \nSun et al., 2022). Governments are already discussing the \nenforcement of AI regulations that include explainable AI to \nprotect the general society and mitigate risks tied to the tech-\nnology (Hamon et al., 2020). Interpretability (i.e., the human \ncapability to understand the AI system’s processes and deci-\nsions) is key, especially for GAI-based systems employed in \nlarge-scale information systems that affect large user groups, \nsuch as in networked businesses and digital platforms. In \nthese cases, generated content has the potential to impact \nindividuals and society, for instance, when generative AI \nserves as an advisor based on user questions and provides \nunsophisticated answers that are difficult to verify. Inaccu-\nracy in generated product recommendations can have vary-\ning consequences depending on the situation, ranging from \nselecting the wrong product to taking the wrong medication. \nEarly studies have shown how the chatbot ChatGPT per -\nformed surprisingly well in medical exams, suggesting inher-\nent knowledge similar to medical students (Bhayana et al., \n2023; Gilson et al., 2023). However, the seemingly omnisci-\nent capabilities may be restricted because, in the case of the \nmedical exams, the GAI model might have been trained on \nthe exam data and can reproduce its answers but is not able \nto comprehend the contextual state of an individual relevant \nfor medical assessment. Therefore, understanding how the \nsystem performs sensemaking and generates its data helps \nusers and businesses to achieve their goals responsibly and \neffectively, satisfying stakeholders’ needs and expectations \n(Miller, 2019; Sun et al., 2022). Particularly for autonomous \nsystems in critical business applications that interact with \nhuman beings, supervision and explainability of the GAI-\ngenerated content remain vital to ensure reliable, safe, and \ntrustworthy outputs (Brasse et al., 2023; Hamm et al., 2023).\nAnother angle of transparency concerns the debate \nbetween open-source and closed-source models. Legal \nissues revolving around copyright, licenses, and intellectual \nproperty make it difficult for individuals and enterprises to \ndeploy GAI-based systems, especially when the large train-\ning data of closed-sourced GAI models is procured through \nInternet scraping (Jin et al., 2023; Smits & Borghuis, 2022). \nResearch initiatives revolving around open-source datasets \n(e.g., Schuhmann et al., 2022) and open-source models (e.g., \nStability.ai, 2023) aim at increasing the transparency on data \nprovenance and highlight, for instance, the data sources as \nwell as the presence of watermarks on images (Schuhmann \net al., 2022). Although most system engineers will rely on \npre-trained models and perform fine-tuning for their specific \nuse case, open-source efforts are facilitating a legally safer \nroute for businesses to deploy GAI models, ensuring legal \ncompliance, and mitigating associated risks.\nElectronic Markets (2023) 33:63 \n1 3 Page 9 of 17 63\nHallucinations\nDue to the probabilistic variance property of GAI, genera-\ntive models are not immune to output errors, or so-called \nhallucinations, which manifest themselves in confidently \ngenerated results that seem plausible but are unreasonable \nwith respect to the source of information (Ji et al., 2023; \nSusarla et al., 2023). The underlying causes of hallucinations \nare still being researched, with early findings suggesting that \nthe training data that might contain contradictory or fictional \ncontent besides factual information (Dziri et al., 2022). This \ncombination of varied inputs can lead to the generation of \noutputs that deviate from reality and introduce false infor -\nmation. As a result, the uncertainty of generation quality is \nfurther fueled by closed-source models that do not disclose \nany information on their training, making it crucial to care-\nfully select appropriate datasets and models to mitigate the \nrisk of hallucinations.\nTo illustrate the occurrence of hallucinations, studies \nhave identified GAI-based image generators that facili-\ntate anatomical inaccuracies in the generated images of \nhumans (Choi et al., 2022; Hughes, 2023). These inaccu-\nracies suggest that GAI models require further refinement \nand improvement before they can be reliably used for unsu-\npervised production tasks (e.g., advertisement production, \nautomated social media posts). Additionally, errors in sim-\nple arithmetic operations have also been observed (Bubeck \net al., 2023), highlighting the limitations and potential \nshortcomings of current generative models in performing \neven basic computations accurately. Due to the seemingly \nrealistic data produced by GAI models, the detection and \nevaluation of hallucinations are a challenging task. Current \nautomatic evaluation includes statistical methods to measure \ndiscrepancies between ground-truth references and gener -\nated data and model-based metrics that leverage additional \nDL models to detect content inconsistencies (Ji et al., 2023). \nHowever, both approaches can be subjected to errors and \nare still inferior to cumbersome human evaluation. These \ninstances emphasize the importance of appropriate halluci-\nnation mitigation methods, such as human supervision, to \nensure the quality and accuracy of generated content.\nMoving forward, addressing the issue of hallucinations \nin generative AI requires ongoing research and development \nefforts. Enhancing the transparency of training data and \ncomputation processes as well as promoting the adoption \nof open-source models can help mitigate the risk of gen-\nerating misleading or flawed results. Furthermore, refining \nthe underlying algorithms and incorporating robust error-\nchecking mechanisms can contribute to the overall reliability \nand trustworthiness of GAI models (Zhou et al., 2023).\nMisuse\nThe access to GAI-based content creation tools with realistic \noutputs does not only enable new creative opportunities for \ngood (e.g., novel automation and innovation prospects across \nmultiple domains for businesses and users), but can also be \nleveraged for malicious purposes to intentionally cause risk \nand harm to society (Weidinger et al., 2022). Deepfakes have \nbecome increasingly sophisticated over the past decade as a \nresult of the low cost and ease of creating such media using \nx-to-image, x-to-video, and x-to-sound GAI models (Mirsky \n& Lee, 2022; Vasist & Krishnan, 2022). They are authentic \nmedia content designed to impersonate individuals, such \nas celebrities or politicians, and are created to entertain or \nmanipulate the viewers. For example, deepfakes depicting \nTrump’s arrest have circulated on social media and in the \nnews, causing misinformation due to their hyperrealistic, \nalmost unrecognizable appearance (BBC, 2023). This is just \none example of how generative AI carries potential for abuse \nand can be extended to other areas of daily life, such as \nfraudulent service offers, identity theft, or fake shops (Houde \net al., 2020; Weidinger et al., 2022). The availability of GAI \nmodels provides starting points for new applications and \nbusiness models for misuse and criminals, ultimately being \nleveraged to spread misinformation and influence the media \nand politics, or to defraud individuals and businesses (Hart-\nmann et al., 2023; Kreps et al., 2022; Mirsky & Lee, 2022). \nSuch issues, combined with low data quality and bias, pro-\nvide a preview of potential social and ethical harms that may \nresult in discrimination, exclusion, toxicity, and information \nhazards (Weidinger et al., 2022). This increases the urgency \nfor disclosure and transparency of GAI models, along with \nthe aforementioned calls for explainability (Brasse et al., \n2023; Horneber & Laumer, 2023; Raj et al., 2023).\nGenerative AI researchers seek to develop measures for \nsafer and more responsible use (van Slyke et al., 2023). \nRLHF and carefully crafted open-source datasets are first \nattempts at improvement, besides input filters restricting user \nprompts to harmless content. However, applications can still \nbe tricked into bypassing filters and safeguards of GAI mod-\nels, for instance, through prompt injections that insert mali-\ncious prompts to achieve misaligned outputs of generative \nAI applications (Perez & Ribeiro, 2022). The collaborative \nefforts between researchers, organizations, and regulators \n(e.g., by initiatives such as the European Union AI Act or \nUS National AI Initiative) serve as promising initial steps \ntoward opening pathways for future research to effectively \naddress these issues, ensuring that AI-generated content is \nmorally, ethically, and legally appropriate and cannot be mis-\nused (Hacker et al., 2023).\n Electronic Markets (2023) 33:63\n1 363 Page 10 of 17\nTable 3  Future research questions regarding the challenges of generative AI-based systems\nGenerative AI perspective Environment perspective Data perspective\nBias • How do potential biases from GAI-generated content \naffect service offerings?\n• What are implications of addressing bias in GAI mod-\nels and data within electronic markets, and how can \nthey be measured and managed?\n• How can we prevent GAI-based systems from perpetu-\nating biases or discrimination against certain groups \nof people?\n• What measures can be taken to ensure that GAI-\ngenerated content respects cultural values and societal \nnorms?\n• How can GAI help identify and address bias in user-\ngenerated data?\n• How can GAI be used to synthesize diverse and \nrepresentative datasets that minimize bias in decision-\nmaking?\nTransparency • How can we ensure that GAI-based systems are trans-\nparent and accountable?\n• How and to what extent should GAI-based services \ndeclare the use of GAI technology?\n• What measures can be taken to ensure that users are \naware of when they are interacting with GAI-generated \ncontent, thus enhancing transparency?\n• How can GAI-based systems be designed to respect \nindividual privacy while still delivering personalized \ncontent?\n• How does transparency in GAI-generated content \nimpact user trust and engagement within digital plat-\nforms and ecosystems?\n• How can platforms involving GAI transparently disclose \nthe origins of data and preserve copyright?\n• How can data transparency promote trust in GAI-based \nservices, particularly when dealing with sensitive \ninformation?\nHallucinations • How can GAI-related hallucinations be mitigated on \nbusiness strategy level?\n• When and how should humans-in-the-loop be inte-\ngrated in a GAI-based system to tackle hallucinations?\n• What is the impact of hallucinations toward consumer \nbehavior?\n• How do hallucinations in GAI-generated content affect \nuser trust, engagement, and decision-making on digital \nplatforms?\n• What role can explainable AI play in identifying and \naddressing hallucinations in GAI-generated content?\n• How can data preprocessing and validation methods be \nimproved to detect and mitigate hallucinations?\nMisuse • Which impact does the misuse of GAI has on current \ndigital platforms and electronic markets?\n• How can we detect and prevent the misuse of \n(e-commerce) platforms by fraudulent providers and \ncustomers who leverage GAI?\n• How can businesses develop effective strategies and \nregulations to prevent the misuse of GAI and protect \nthe privacy and security of market participants?\n• What are the ethical implications of using GAI in \nelectronic markets?\n• How do instances of GAI misuse impact competition \nand market dynamics within platform ecosystems, \nand what regulatory frameworks can be established to \naddress this?\n• How can GAI models be designed to flag and reject \ncontent that exhibits signs of potential misuse during the \ngeneration process?\n• How can data sources be checked and authenticated to \nensure that training data for GAI is not compromised or \nmanipulated to encourage misuse?\nSocietal impact • Where should the boundary of liability be drawn when \nGAI generates false content?\n• What are the potential implications of GAI-based \nsystems on the future of work?\n• How can GAI empower users to understand and con-\ntrol the content they interact with, promoting a more \ninformed and empowered societal experience?\n• How can we ensure that GAI-based systems do not \nlead to worker displacement or other negative social \nimpacts?\n• How can GAI enable individuals to perform tasks and \noffer novel services they have not been trained for?\n• How can businesses and platform ecosystems proac-\ntively engage with their user communities to address \nsocietal concerns regarding AI-generated content and \nadapt their practices accordingly?\n• How can GAI models continuously adapt to evolving \nsocietal norms, ethics, and cultural sensitivities, particu-\nlarly in content generation and data handling?\n• How can the generation of data be leveraged with GAI \nmodels to achieve greater goods for society?\nElectronic Markets (2023) 33:63 \n1 3 Page 11 of 17 63\nSocietal impact\nWith its ability to produce novel and diverse content, gen-\nerative AI has significant implications for society in sev -\neral areas. Besides the aforementioned risks of misuse and \nmisinformation, the environmental footprint of developing \nand operating any AI-based system is an emerging con-\ncern and gets amplified for GAI-based systems due to the \ndemanding technical requirements for training and running \nlarge DGMs and their high implicit energy consumption \n(Schneider et al., 2023; Schoormann et al., 2023). Conse-\nquently, the public acceptance of AI on a societal but also \nindividual level inevitably guides the future development \nand use of GAI. Individually, researchers have investigated \nhow AI can be perceived as a threat by some users (John-\nson & Verdicchio, 2017; Lysyakov & Viswanathan, 2022). \nFor example, a fear of employees is losing their jobs due \nto AI’s automation potential (Mirbabaie et al., 2022). With \nGAI’s capabilities of producing high-quality, customized \nrealistic content, the frontier toward machines taking over \nhuman tasks moves even closer. Hence, trust plays a pivotal \nrole in the adoption of emerging technologies and comes \nwith direct impact between users and application providers \n(Lukyanenko et al., 2022; Riedl, 2022; Wanner et al., 2022; \nYang & Wibowo, 2022). The promotion of explainable \nand responsible AI becomes paramount once more (Meske \net al., 2022), especially in a technological state where dis-\ntinguishing between generated and real content by GAI \nmodels is becoming increasingly more challenging (Jakesch \net al., 2023b; Lehmann & Buschek, 2020). Early studies on \ngenerative AI suggest that generated content is perceived \nas less trustworthy, resulting in aversion toward generated \nproducts (Longoni et al., 2022; Rix & Hess, 2023). Nonethe-\nless, GAI applications offer a variety of value propositions \nthat revolve around building a trusting relationship toward \nsuch systems (Tomitza et al., 2023). For instance, social \nchatbots like Replika aim to realistically mimic or replace \nhumans, leading to feelings of attachment toward machines \namong users (Pentina et al., 2023), whereas assistive GAI \napplications intent to support their users in the best way pos-\nsible (Burger et al., 2023; van Dun et al., 2023).\nWhile generative AI promises significant advances and \nhas the potential to revolutionize domains, such as market-\ning, arts and culture, or even electronic markets as a whole, \nby supporting, augmenting, and automating a wide range of \noperations and offering novel services, it also underlines the \nimportance of fully understanding its inherent challenges \nand mitigating possible risks. The far-reaching implications \nof GAI, ranging from poor data quality and hallucinations \nto cases of societal misuse, require a proactive and sustained \neffort by researchers and practitioners to develop appropriate \nsolutions ensuring the responsible and beneficial integra-\ntion of generative AI technology into our digital society. In \nTable 3, we summarize the aforementioned challenges of \nGAI-based systems and suggest future research questions \nrevolving around the GAI system, environment, and data \nperspective.\nConclusion\nWith this fundamentals article, we provide an introduction \nto generative artificial intelligence. Drawing on ML and DL \nas the underlying technologies, we conceptualized genera-\ntive AI and differentiated it from traditional AI methods. We \noutlined the most common DGMs as the theoretical founda-\ntion and described the core principles of generativity and \nvariance. Moreover, we discussed the potential applications, \nacknowledging the distinct generation modalities and layers \nalong the value chain. The paradigm shift of AI applica-\ntions from discriminative to generative is leading to unique \nuse cases and promising opportunities in various domains, \nincluding those traditionally resistant to automation. There-\nfore, researchers and practitioners need to understand the \ninherent properties of generative AI to effectively leverage \nits potential while also mitigating associated risks. We also \nconsidered five core challenges by elaborating on bias, trans-\nparency, hallucinations, misuse, and societal impact.\nGenerative AI carries the potential to significantly impact \nvarious industries. Taking the different layers of the value \nchain into account, this progression could further lead to \nthe creation of new platforms and services centered around \nwhole GAI ecosystems. It remains to be seen how these \nforms of unprecedented artificial creativity and generativity \nwill find a place in the industry and everyday life. Future \nresearch will need to address the challenges for safety and \nresponsibility measures, especially when employed in highly \nregulated or autonomous scenarios. For example, ensuring \ntechnological transparency, increasing public trust in GAI, \nand developing process models for employing GAI-based \nsystems can contribute to the body of knowledge on genera-\ntive AI. We believe this article provides an entry point to this \nnovel type of technology and guides other researcher in their \nefforts in examining generative AI. Closing with the initial \nquote by ChatGPT, generative AI indeed holds transforma-\ntive the potential to redefine innovation boundaries of the \ndigital landscapes.\n Electronic Markets (2023) 33:63\n1 363 Page 12 of 17\nFunding Open Access funding enabled and organized by Projekt \nDEAL.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\nÅgerfalk, P. J., Conboy, K., Crowston, K., Eriksson Lundström, J. \nS. Z., Jarvenpaa, S., Ram, S., & Mikalef, P. (2022). Artificial \nintelligence in information systems: State of the art and research \nroadmap. Communications of the Association for Information \nSystems, 50(1), 420–438. https:// doi. org/ 10. 17705/ 1CAIS. 05017\nAggarwal, A., Mittal, M., & Battineni, G. (2021). Generative adver -\nsarial network: An overview of theory and applications. Interna-\ntional Journal of Information Management Data Insights, 1(1), \n100004. https:// doi. org/ 10. 1016/j. jjimei. 2020. 100004\nAgostinelli, A., Denk, T. I., Borsos, Z., Engel, J., Verzetti, M., Cail-\nlon, A., Huang, Q., Jansen, A., Roberts, A., Tagliasacchi, M., \nSharifi, M., Zeghidour, N., & Frank, C. (2023). MusicLM: Gen-\nerating Music From Text. https:// doi. org/ 10. 48550/ arXiv. 2301. \n11325\nAli, H., Murad, S., & Shah, Z. (2023). Spot the fake lungs: Generat-\ning synthetic medical images using neural diffusion models. In \nL. Longo & R. O’Reilly (Eds.), Communications in Computer \nand Information Science. Artificial Intelligence and Cognitive \nScience (Vol. 1662, pp. 32–39). Springer Nature Switzerland. \nhttps:// doi. org/ 10. 1007/ 978-3- 031- 26438-2_3\nAnantrasirichai, N., & Bull, D. (2022). Artificial intelligence in the cre-\native industries: A review. Artificial Intelligence Review, 55(1), \n589–656. https:// doi. org/ 10. 1007/ s10462- 021- 10039-7\nBaeza-Yates, R. (2018). Bias on the web. Communications of the ACM, \n61(6), 54–61. https:// doi. org/ 10. 1145/ 32095 81\nBakpayev, M., Baek, T. H., van Esch, P., & Yoon, S. (2022). Pro-\ngrammatic creative: AI can think but it cannot feel. Australa -\nsian Marketing Journal, 30(1), 90–95. https:// doi. org/ 10. 1016/j. \nausmj. 2020. 04. 002\nBBC. (2023). Fake Trump arrest photos: How to spot an AI-generated \nimage. https:// www. bbc. com/ news/ world- us- canada- 65069 316\nBerente, N., Gu, B., Recker, J., & Santhanam, R. (2021). Special issue \neditor’s comments: Managing artificial intelligence. MIS Quar -\nterly, 45(3), 1433–1450. https:// doi. org/ 10. 25300/ MISQ/ 2021/ \n16274\nBhayana, R., Krishna, S., & Bleakney, R. R. (2023). Performance of \nChatGPT on a radiology board-style examination: Insights into \ncurrent strengths and limitations. Radiology, 307(5), e230582. \nhttps:// doi. org/ 10. 1148/ radiol. 230582\nBorsos, Z., Marinier, R., Vincent, D., Kharitonov, E., Pietquin, O., \nSharifi, M., Teboul, O., Grangier, D., Tagliasacchi, M., & Zeghi-\ndour, N. (2022). AudioLM: a Language Modeling Approach to \nAudio Generation. https:// doi. org/ 10. 48550/ arXiv. 2209. 03143\nBrand, J., Israeli, A., & Ngwe, D. (2023). Using GPT for market \nresearch. Harvard Business School Marketing Unit Working \nPaper. Advance online publication. https:// doi. org/ 10. 2139/  \nssrn. 43957 51\nBrasse, J., Broder, H. R., Förster, M., Klier, M., & Sigler, I. (2023). \nExplainable artificial intelligence in information systems: A \nreview of the status quo and future research directions. Electronic \nMarkets, 33, 26. https:// doi. org/ 10. 1007/ s12525- 023- 00644-5\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, \nP., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agar -\nwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, \nR., Ramesh, A., Ziegler, D., Wu, J., Winter, C., & Amodei, D. \n(2020). Language models are few-shot learners. In H. Larochelle, \nM. Ranzato, R. Hadsell, M. F. Balcan, & H. Lin (Eds.), Advances \nin neural information processing systems 33 (pp. 1877–1901). \nCurran Associates Inc.\nBrynjolfsson, E., & McAfee, A. (2016). The second machine age: \nWork, progress, and prosperity in a time of brilliant technolo-\ngies. W.W. Norton & Company.\nBrynjolfsson, E., Li, D., & Raymond, L. (2023). Generative AI at \nWork. Cambridge MA. https:// doi. org/ 10. 3386/ w31161\nBrynjolfsson, E., & Mitchell, T. (2017). What can machine learning \ndo? Workforce implications. Science, 358(6370), 1530–1534. \nhttps:// doi. org/ 10. 1126/ scien ce. aap80 62\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., \nKamar, E., Lee, P., Lee, Y. T., Li, Y., Lundberg, S., Nori, H., \nPalangi, H., Ribeiro, M. T., & Zhang, Y. (2023). Sparks of artifi-\ncial general intelligence: Early experiments with GPT-4. https:// \ndoi. org/ 10. 48550/ arXiv. 2303. 12712\nBurger, B., Kanbach, D. K., Kraus, S., Breier, M., & Corvello, V. \n(2023). On the use of AI-based tools like ChatGPT to sup-\nport management research. European Journal of Innova-\ntion Management, 26(7), 233–241. https:// doi. org/ 10. 1108/  \nEJIM- 02- 2023- 0156\nBurström, T., Parida, V., Lahti, T., & Wincent, J. (2021). AI-enabled \nbusiness-model innovation and transformation in industrial eco-\nsystems: A framework, model and outline for further research. \nJournal of Business Research, 127, 85–95. https:// doi. org/ 10. \n1016/j. jbusr es. 2021. 01. 016\nCastelvecchi, D. (2016). Can we open the black box of AI? Nature,  \n538(7623), 20–23. https:// doi. org/ 10. 1038/ 53802 0a\nChoi, H., Chang, W., & Choi, J. (2022). Can we find neurons that cause \nunrealistic images in deep generative networks? In R. Dechter & \nL. de Raedt (Eds.), Proceedings of the thirty-first international \njoint conference on artificial intelligence (pp. 2888–2894). Inter-\nnational Joint Conferences on Artificial Intelligence Organiza-\ntion. https:// doi. org/ 10. 24963/ ijcai. 2022/ 400\nChristiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., & Amo-\ndei, D. (2017). Deep reinforcement learning from human pref-\nerences. In I. Guyon, U. von Luxburg, S. Bengio, H. Wallach, \nR. Fergus, S. Vishwanathan, & R. Garnett (Eds.), Advances in \nneural information processing systems 30  (Vol. 30). Curran \nAssociates, Inc.\nCooper, G. (2023). Examining science education in ChatGPT: An \nexploratory study of generative artificial intelligence. Journal \nof Science Education and Technology, 32(3), 444–452. https://  \ndoi. org/ 10. 1007/ s10956- 023- 10039-y\nDang, H., Mecke, L., Lehmann, F., Goller, S., & Buschek, D. (2022). \nHow to prompt? Opportunities and challenges of zero- and few-\nshot learning for human-ai interaction in creative applications of \ngenerative models. In Generative AI and HCI Workshop: CHI \n2022, New Orleans, LA. https:// doi. org/ 10. 48550/ arXiv. 2209. \n01390\nDanks, D., & London, A. J. (2017). Algorithmic bias in autono-\nmous systems. In F. Bacchus & C. Sierra (Eds.), Proceedings \nof the twenty-sixth international joint conference on artificial \nElectronic Markets (2023) 33:63 \n1 3 Page 13 of 17 63\nintelligence  (pp. 4691–4697). International Joint Conferences \non Artificial Intelligence Organization. https:// doi. org/ 10. 24963/ \nijcai. 2017/ 654\nDwivedi, Y. K., Hughes, L., Ismagilova, E., Aarts, G., Coombs, C., \nCrick, T., Duan, Y., Dwivedi, R., Edwards, J., Eirug, A., Gala -\nnos, V., Ilavarasan, P. V., Janssen, M., Jones, P., Kar, A. K., \nKizgin, H., Kronemann, B., Lal, B., Lucini, B., & Williams, M. \nD. (2021). Artificial intelligence (AI): Multidisciplinary per -\nspectives on emerging challenges, opportunities, and agenda for \nresearch, practice and policy. International Journal of Informa-\ntion Management, 57, 101994. https:// doi. org/ 10. 1016/j. ijinf \nomgt. 2019. 08. 002\nDwivedi, Y. K., Kshetri, N., Hughes, L., Slade, E. L., Jeyaraj, A., Kar, \nA. K., Baabdullah, A. M., Koohang, A., Raghavan, V., Ahuja, \nM., Albanna, H., Albashrawi, M. A., Al-Busaidi, A. S., Bal-\nakrishnan, J., Barlette, Y., Basu, S., Bose, I., Brooks, L., Buhalis, \nD., & Wright, R. (2023). “So what if ChatGPT wrote it?” Multi-\ndisciplinary perspectives on opportunities, challenges and impli-\ncations of generative conversational AI for research, practice and \npolicy. International Journal of Information Management, 71, \n102642. https:// doi. org/ 10. 1016/j. ijinf omgt. 2023. 102642\nDziri, N., Milton, S., Yu, M., Zaiane, O., & Reddy, S. (2022). On \nthe origin of hallucinations in conversational models: Is it the \ndatasets or the models? In M. Carpuat, M.-C. de Marneffe, & I. \nV. Meza Ruiz (Eds.), Proceedings of the 2022 Conference of the \nNorth American Chapter of the Association for Computational \nLinguistics: Human Language Technologies (pp. 5271–5285). \nAssociation for Computational Linguistics. https:// doi. org/ 10. \n18653/ v1/ 2022. naacl- main. 387\nEinola, K., & Khoreva, V. (2023). Best friend or broken tool? Explor-\ning the co-existence of humans and artificial intelligence in the \nworkplace ecosystem. Human Resource Management, 62(1), \n117–135. https:// doi. org/ 10. 1002/ hrm. 22147\nElasri, M., Elharrouss, O., Al-Maadeed, S., & Tairi, H. (2022). Image \ngeneration: A review. Neural Processing Letters, 54(5), 4609–\n4646. https:// doi. org/ 10. 1007/ s11063- 022- 10777-x\nElicit. (2022). Frequently asked questions: What is elicit? https:// elicit. \norg/ faq# what- is- elicit\nEloundou, T., Manning, S., Mishkin, P., & Rock, D. (2023). GPTs \nare GPTs: An early look at the labor market impact potential \nof large language models. arXiv. https:// doi. org/ 10. 48550/ arXiv. \n2303. 10130\nEsser, P., Chiu, J., Atighehchian, P., Granskog, J., & Germanidis, A. \n(2023). Structure and content-guided video synthesis with diffu-\nsion models. https:// doi. org/ 10. 48550/ arXiv. 2302. 03011\nFeng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., \nQin, B., Liu, T., Jiang, D., & Zhou, M. (2020). CodeBERT: A \npre-trained model for programming and natural languages. In \nT. Cohn, Y. He, & Y. Liu (Eds.), Findings of the association \nfor computational linguistics: EMNLP 2020 (pp. 1536–1547). \nAssociation for Computational Linguistics. https:// doi. org/ 10. \n18653/ v1% 2F2020. findi ngs- emnlp. 139\nFerrara, E. (2023). Should ChatGPT be biased? Challenges and risks \nof bias in large language models. arXiv. https:// doi. org/ 10. 48550/ \narXiv. 2304. 03738\nFerreira, K. J., Lee, B. H. A., & Simchi-Levi, D. (2016). Analytics \nfor an online retailer: Demand forecasting and price optimiza -\ntion. Manufacturing & Service Operations Management, 18(1), \n69–88. https:// doi. org/ 10. 1287/ msom. 2015. 0561\nFügener, A., Grahl, J., Gupta, A., & Ketter, W. (2021). Will humans-\nin-the-loop become borgs? Merits and pitfalls of working with \nAI. MIS Quarterly, 45(3), 1527–1556. https:// doi. org/ 10. 25300/ \nMISQ/ 2021/ 16553\nGao, J., Shen, T., Wang, Z, Chen, W., Yin, K., Li, D, Litany, O., \nGojcic, Z., & Fidler, S. (2022). GET3D: A generative model \nof high quality 3D textured shapes learned from images. In S. \nKoyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, & A. \nOh (Eds.), Advances in Neural Information Processing Systems \n35. Curran Associates, Inc.\nGilson, A., Safranek, C. W., Huang, T., Socrates, V., Chi, L., Taylor, R. \nA., & Chartash, D. (2023). How does ChatGPT perform on the \nUnited States Medical Licensing Examination? The implications \nof large language models for medical education and knowledge \nassessment. JMIR Medical Education, 9, e45312. https:// doi. org/ \n10. 2196/ 45312\nGm, H., Gourisaria, M. K., Pandey, M., & Rautaray, S. (2020). A com-\nprehensive survey and analysis of generative models in machine \nlearning. Computer Science Review, 38, 100285. https:// doi. org/ \n10. 1016/j. cosrev. 2020. 100285\nGoodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. \nThe MIT Press.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., \nOzair, S., Courville, A., & Bengio, Y. (2020). Generative adver-\nsarial networks. Communications of the ACM, 63(11), 139–144. \nhttps:// doi. org/ 10. 1145/ 34226 22\nGriffith, S., Subramanian, K., Scholz, J., Isbell, C. L., & Thomaz, A. L. \n(2013). Policy shaping: Integrating Human feedback with rein-\nforcement learning. In C. J. C. Burges, L. Bottou, Z. Ghahramani, \n& K. Q. Weinberger (Eds.), Advances in Neural Information Pro-\ncessing Systems 26 (Vol. 26). Curran Associates, Inc.\nGui, J., Sun, Z., Wen, Y., Tao, D., & Ye, J. (2023). A review on genera-\ntive adversarial networks: Algorithms, theory, and applications. \nIEEE Transactions on Knowledge and Data Engineering, 35(4), \n3313–3332. https:// doi. org/ 10. 1109/ TKDE. 2021. 31301 91\nGuo, D., Ren, S., Lu, S., Feng, Z., Tang, D., Liu, S., Zhou, L., Duan, \nN., Svyatkovskiy, A., Fu, S., Tufano, M., Deng, S. K., Clement, \nC., Drain, D., Sundaresan, N., Yin, J., Jiang, D., & Zhou, M. \n(2021). GraphCodeBERT: Pre-training code representations \nwith data flow. 9th International Conference on Learning Rep-\nresentations 2021 (ICLR), Virtual.\nHaase, J., Djurica, D., & Mendling, J. (2023). The art of inspir -\ning creativity: Exploring the unique impact of AI-generated \nimages. AMCIS 2023 Proceedings.\nHacker, P., Engel, A., & Mauer, M. (2023). Regulating ChatGPT and \nother large generative AI models. 2023 ACM Conference on \nFairness, Accountability, and Transparency (pp. 1112–1123). \nACM. https:// doi. org/ 10. 1145/ 35930 13. 35940 67\nHamm, P., Klesel, M., Coberger, P., & Wittmann, H. F. (2023). \nExplanation matters: An experimental study on explain-\nable AI. Electronic Markets, 33, 17. https:// doi. org/ 10. 1007/ \ns12525- 023- 00640-9\nHamon, R., Junklewitz, H., & Sanchez, I. (2020). Robustness and \nexplainability of artificial intelligence: From technical to \npolicy solutions. EUR: Vol. 30040. Publications Office of the \nEuropean Union.\nHarmon, P. (1985). Expert systems: Artificial intelligence in busi-\nness. Wiley & Sons.\nHartmann, J., Schwenzow, J., & Witte, M. (2023). The political \nideology of conversational AI: Converging evidence on Chat-\nGPT's pro-environmental, left-libertarian orientation. https:// \ndoi. org/ 10. 48550/ arXiv. 2301. 01768\nHo, J., Jain, A., & Abbeel, P. (2020). Denoising diffusion probabil-\nistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. \nBalcan, & H. Lin (Eds.), Advances in Neural Information Pro-\ncessing Systems 33 (pp. 6840–6851). Curran Associates Inc.\nHooker, S. (2021). Moving beyond “algorithmic bias is a data prob-\nlem”. Patterns (New York, N.Y.), 2(4), 100241. https:// doi. org/ \n10. 1016/j. patter. 2021. 100241\nHorneber, D., & Laumer, S. (2023). Algorithmic accountability. \nBusiness & Information Systems Engineering. Advance online \npublication. https:// doi. org/ 10. 1007/ s12599- 023- 00817-8\n Electronic Markets (2023) 33:63\n1 363 Page 14 of 17\nHoude, S., Liao, V., Martino, J., Muller, M., Piorkowski, D., Rich-\nards, J., Weisz, J., & Zhang, Y. (2020). Business (mis)Use \nCases of Generative AI. In W. Geyer, Y. Khazaeni, & M. \nShmueli-Scheuer (Eds.), Joint Proceedings of the Workshops \non Human-AI Co-Creation with Generative Models and User-\nAware Conversational Agents co-located with 25th Interna -\ntional Conference on Intelligent User Interfaces (IUI 2020).  \nCEUR. https:// doi. org/ 10. 48550/ arXiv. 2003. 07679\nHu, K. (2023, February 2). ChatGPT sets record for fastest-growing \nuser base - Analyst note . Reuters. https://  www. reute  rs. com/ \ntechn ology/ chatg pt- sets- record- faste st- growi ng- user- base- \nanaly st- note- 2023- 02- 01/\nHuang, S., & Grady, P. (2022). Generative AI: A Creative New \nWorld.  Sequoia. https://  www. sequo iacap. com/ artic le/ gener \native- ai-a- creat ive- new- world/\nHughes, A. (2023). Why AI-generated hands are the stuff of night-\nmares, explained by a scientist. BBC Science Focus. https://  \nwww. scien cefoc us. com/ future- techn ology/ why- ai- gener ated- \nhands- are- the- stuff- of- night mares- expla ined- by-a- scien tist/\nJakesch, M., Bhat, A., Buschek, D., Zalmanson, L., & Naaman, M. \n(2023a). Co-writing with opinionated language models affects \nusers’ views. In A. Schmidt, K. Väänänen, T. Goyal, P. O. \nKristensson, A. Peters, S. Mueller, J. R. Williamson, & M. \nL. Wilson (Eds.), Proceedings of the 2023 CHI Conference \non Human Factors in Computing Systems (pp. 1–15). ACM. \nhttps:// doi. org/ 10. 1145/ 35445 48. 35811 96.\nJakesch, M., Hancock, J. T., & Naaman, M. (2023b). Human heuris-\ntics for AI-generated language are flawed. Proceedings of the \nNational Academy of Sciences of the United States of America,  \n120(11), e2208839120. https://  doi.  org/ 10. 1073/  pnas.  22088  \n39120\nJaniesch, C., Zschech, P., & Heinrich, K. (2021). Machine learning and \ndeep learning. Electronic Markets, 31(3), 685–695. https:// doi. \norg/ 10. 1007/ s12525- 021- 00475-2\nJasper. (2022). ChatGPT vs. Jasper: How it’s different from Jasper \nchat. https:// www. jasper. ai/ blog/ what- is- chatg pt\nJebara, T. (2004). Generative versus discriminative learning. In T. \nJebara (Ed.), Machine Learning (pp. 17–60). Springer US. \nhttps:// doi. org/ 10. 1007/ 978-1- 4419- 9011-2_2\nJi, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y. \nJ., Madotto, A., & Fung, P. (2023). Survey of hallucination in \nnatural language generation. ACM Computing Surveys, 55(12), \n1–38. https:// doi. org/ 10. 1145/ 35717 30\nJin, Y., Jang, E., Cui, J., Chung, J.- W., Lee, Y., & Shin, S. (2023). \nDarkBERT: A language model for the dark side of the Internet. \nIn 61st Annual Meeting of the Association for Computational \nLinguistics (ACL’23), Toronto, Canada.\nJohnson, D. G., & Verdicchio, M. (2017). AI Anxiety. Journal of the \nAssociation for Information Science and Technology, 68(9), \n2267–2270. https:// doi. org/ 10. 1002/ asi. 23867\nJumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, \nO., Tunyasuvunakool, K., Bates, R., Žídek, A., Potapenko, A., \nBridgland, A., Meyer, C., Kohl, S. A. A., Ballard, A. J., Cowie, \nA., Romera-Paredes, B., Nikolov, S., Jain, R., Adler, J., & Has-\nsabis, D. (2021). Highly accurate protein structure prediction \nwith AlphaFold. Nature, 596(7873), 583–589. https:// doi. org/ \n10. 1038/ s41586- 021- 03819-2\nKingma, D. P., & Welling, M (2014). Auto-encoding variational Bayes. \nInternational Conference on Learning Representations 2021 \n(ICLR), Banff, Canada.\nKingma, D. P., Mohamed, S., Jimenez Rezende, D., & Welling, M. \n(2014).Semi-supervised learning with deep generative models. \nIn Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, & K. Q. \nWeinberger (Eds.), Advances in Neural Information Processing \nSystems 27 (Vol. 27). Curran Associates, Inc.\nKodali, N., Abernethy, J., Hays, J., & Kira, Z. (2017).On convergence \nand stability of GANs. arXiv. https:// doi. org/ 10. 48550/ arXiv. \n1705. 07215\nKowalczyk, P., Röder, M., & Thiesse, F. (2023). Nudging creativity in \ndigital marketing with generative artificial intelligence: Opportu-\nnities and limitations. ECIS 2023 Research-in-Progress Papers, \nArticle 22.\nKreps, S., McCain, R. M., & Brundage, M. (2022). All the news that’s \nfit to fabricate: AI-generated text as a tool of media misinforma-\ntion. Journal of Experimental Political Science, 9(1), 104–117. \nhttps:// doi. org/ 10. 1017/ XPS. 2020. 37\nKühl, N., Schemmer, M., Goutier, M., & Satzger, G. (2022). Artificial \nintelligence and machine learning. Electronic Markets, 32(4), \n2235–2244. https:// doi. org/ 10. 1007/ s12525- 022- 00598-0\nLeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, \n521(7553), 436–444. https:// doi. org/ 10. 1038/ natur e14539\nLehmann, F., & Buschek, D. (2020). Examining autocompletion as a \nbasic concept for interaction with generative AI. I-Com, 19(3), \n251–264. https:// doi. org/ 10. 1515/ icom- 2020- 0025\nLeiker, D., Gyllen, A. R., Eldesouky, I., & Cukurova, M. (2023). Gen-\nerative AI for learning: Investigating the potential of synthetic \nlearning videos. In  24th International Conference of Artificial \nIntelligence in Education (AIED 2023), Tokyo, Japan.\nLi, H. (2022). Language models. Communications of the ACM, 65(7), \n56–63. https:// doi. org/ 10. 1145/ 34904 43\nLi, J., Li, M., Wang, X., & Thatcher, J. B. (2021). Strategic directions \nfor AI: The role of CIOs and boards of directors. MIS Quarterly, \n45(3), 1603–1644. https:// doi. org/ 10. 25300/ MISQ/ 2021/ 16523\nLi, M., Bao, X., Chang, L., & Gu, T. (2022). Modeling personalized \nrepresentation for within-basket recommendation based on deep \nlearning. Expert Systems with Applications, 192, 116383. https:// \ndoi. org/ 10. 1016/j. eswa. 2021. 116383\nLins, S., Pandl, K. D., Teigeler, H., Thiebes, S., Bayer, C., & Suny -\naev, A. (2021). Artificial intelligence as a service. Business & \nInformation Systems Engineering, 63(4), 441–456. https:// doi. \norg/ 10. 1007/ s12599- 021- 00708-w\nLiu, V., & Chilton, L. B. (2022). Design guidelines for prompt engi-\nneering text-to-image generative models. In S. Barbosa, C. \nLampe, C. Appert, D. A. Shamma, S. Drucker, J. Williamson, \n& K. Yatani (Eds.), CHI Conference on Human Factors in \nComputing Systems (pp. 1–23). ACM. https:// doi. org/ 10. 1145/ \n34911 02. 35018 25\nLongoni, C., Fradkin, A., Cian, L., & Pennycook, G. (2022). News \nfrom generative artificial intelligence is believed less. In  2022 \nACM Conference on Fairness, Accountability, and Transpar -\nency (pp. 97–106). ACM. https:// doi. org/ 10. 1145/ 35311 46.  \n35330 77\nLukyanenko, R., Maass, W., & Storey, V. C. (2022). Trust in arti -\nficial intelligence: From a Foundational Trust Framework to \nemerging research opportunities. Electronic Markets, 32(4), \n1993–2020. https:// doi. org/ 10. 1007/ s12525- 022- 00605-4\nLund, B. D., Wang, T., Mannuru, N. R., Nie, B., Shimray, S., & \nWang, Z. (2023). ChatGPT and a new academic reality: Arti-\nficial intelligence-written research papers and the ethics of the \nlarge language models in scholarly publishing. Journal of the \nAssociation for Information Science and Technology, 74(5), \n570–581. https:// doi. org/ 10. 1002/ asi. 24750\nLysyakov, M., & Viswanathan, S. (2022). Threatened by AI: Analyz-\ning users’ responses to the introduction of AI in a crowd-sourc -\ning platform. Information Systems Research,  34(3). Advance \nonline publication. https:// doi. org/ 10. 1287/ isre. 2022. 1184\nMayahi, S., & Vidrih, M. (2022). The impact of generative AI on the \nfuture of visual content marketing.  https://  doi. org/ 10. 48550/  \narXiv. 2211. 12660\nElectronic Markets (2023) 33:63 \n1 3 Page 15 of 17 63\nMehrabi, N., Morstatter, F., Saxena, N., Lerman, K., & Galstyan, \nA. (2022). A survey on bias and fairness in machine learn -\ning. ACM Computing Surveys, 54(6), 1–35. https:// doi. org/ 10. \n1145/ 34576 07\nMeske, C., Abedin, B., Klier, M., & Rabhi, F. (2022). Explain-\nable and responsible artificial intelligence. Electronic \nMarkets, 32(4), 2103–2106. https:// doi. org/ 10. 1007/  \ns12525- 022- 00607-2\nMicrosoft. (2023). Microsoft and OpenAI extend partnership. https:// \nblogs. micro soft. com/ blog/ 2023/ 01/ 23/ micro softa ndope naiex \ntendp artne rship/\nMiller, T. (2019). Explanation in artificial intelligence: Insights from \nthe social sciences. Artificial Intelligence, 267, 1–38. https:// doi. \norg/ 10. 1016/j. artint. 2018. 07. 007\nMirbabaie, M., Brünker, F., Möllmann Frick, N. R. J., & Stieglitz, S. \n(2022). The rise of artificial intelligence – Understanding the \nAI identity threat at the workplace. Electronic Markets, 32(1), \n73–99. https:// doi. org/ 10. 1007/ s12525- 021- 00496-x\nMirsky, Y., & Lee, W. (2022). The creation and detection of deepfakes. \nACM Computing Surveys, 54(1), 1–41. https:// doi. org/ 10. 1145/ \n34257 80\nMondal, S., Das, S., & Vrana, V. G. (2023). How to bell the cat? A \ntheoretical review of generative artificial intelligence towards \ndigital disruption in all walks of life. Technologies, 11(2), 44. \nhttps:// doi. org/ 10. 3390/ techn ologi es110 20044\nMoussawi, S., Koufaris, M., & Benbunan-Fich, R. (2021). How percep-\ntions of intelligence and anthropomorphism affect adoption of \npersonal intelligent agents. Electronic Markets, 31(2), 343–364. \nhttps:// doi. org/ 10. 1007/ s12525- 020- 00411-w\nMurphy, C., & Thomas, F. P. (2023). Generative AI in spinal cord \ninjury research and care: Opportunities and challenges ahead. \nThe Journal of Spinal Cord Medicine, 46(3), 341–342. https://  \ndoi. org/ 10. 1080/ 10790 268. 2023. 21989 26\nNichol, A., Jun, H., Dhariwal, P., Mishkin, P., & Chen, M. (2022). \nPoint-E: A system for generating 3D point clouds from complex \nprompts. arXiv. https:// doi. org/ 10. 48550/ arXiv. 2212. 08751\nNtoutsi, E., Fafalios, P., Gadiraju, U., Iosifidis, V., Nejdl, W., Vidal, \nM.-E., Ruggieri, S., Turini, F., Papadopoulos, S., Krasanakis, \nE., Kompatsiaris, I., Kinder-Kurlanda, K., Wagner, C., Karimi, \nF., Fernandez, M., Alani, H., Berendt, B., Kruegel, T., Heinze, \nC., & Staab, S. (2020). Bias in data-driven artificial intelligence \nsystems—An introductory survey. WIREs Data Mining and \nKnowledge Discovery, 10(3), e1356. https:// doi. org/ 10. 1002/ \nwidm. 1356\nOpenAI. (2023). GPT-4 technical report. arXiv. https:// doi. org/ 10. \n48550/ arXiv. 2303. 08774\nOppenlaender, J. (2022). The creativity of text-to-image generation. \nProceedings of the 25th International Academic Mindtrek Con-\nference (pp. 192–202). ACM. https:// doi. org/ 10. 1145/ 35692 19. \n35693 52\nOuyang,  L., Wu,  J, Jiang,  X., Almeida,  D., Wainwright,  C.  L., \nMishkin, P., Zhang, C, Agarwal, S., Slama, K., Ray, A., Schul-\nman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., \nWelinder, P., Christiano, P., Leike, J., & Lowe, R. (2022). Train-\ning language models to follow instructions with human feedback. \nhttps:// doi. org/ 10. 48550/ arXiv. 2203. 02155\nPan, Z., Yu, W., Yi, X., Khan, A., Yuan, F., & Zheng, Y. (2019). Recent \nprogress on generative adversarial networks (GANs): A survey. \nIEEE Access, 7, 36322–36333. https:// doi. org/ 10. 1109/ ACCESS. \n2019. 29050 15\nPatterson, D. W. (1990). Introduction to artificial intelligence and \nexpert systems. Prentice Hall.\nPavlik, J. V. (2023). Collaborating with ChatGPT: Considering the \nimplications of generative artificial intelligence for journal-\nism and media education. Journalism & Mass Communication \nEducator, 78(1), 84–93. https:// doi. org/ 10. 1177/ 10776 95822 \n11495 77\nPentina, I., Hancock, T., & Xie, T. (2023). Exploring relationship \ndevelopment with social chatbots: A mixed-method study of \nreplika. Computers in Human Behavior, 140, 107600. https://  \ndoi. org/ 10. 1016/j. chb. 2022. 107600\nPerez, F., & Ribeiro, I. (2022). Ignore previous prompt: Attack tech-\nniques for language models. In D. Hendrycks, V. Krakovna, D. \nSong, J. Steinhardt, & N. Carlini (Chairs), Thirty-sixth Confer -\nence on Neural Information Processing Systems (NeurIPS),  \nVirtual.\nPiccialli, F., Di Cola, V. S., Giampaolo, F., & Cuomo, S. (2021). The \nrole of artificial intelligence in fighting the COVID-19 pan-\ndemic. Information Systems Frontiers : A Journal of Research \nand Innovation, 23(6), 1467–1497. https:// doi. org/ 10. 1007/  \ns10796- 021- 10131-x\nPoole, B., Jain, A., Barron, J. T., & Mildenhall, B. (2023). DreamFu-\nsion: Text-to-3D using 2D diffusion. In  Eleventh International \nConference on Learning Representations (ICLR 2023), Kigali, \nRwanda.\nQiu, L., & Benbasat, I. (2005). An investigation into the effects of text-\nto-speech voice and 3D avatars on the perception of presence and \nflow of live help in electronic commerce. ACM Transactions on \nComputer-Human Interaction, 12(4), 329–355. https:// doi. org/ \n10. 1145/ 11211 12. 11211 13\nRaj, M., Berg, J., & Seamans, R. (2023). Art-ificial intelligence: The \neffect of AI disclosure on evaluations of creative content. arXiv. \nhttps:// doi. org/ 10. 48550/ arXiv. 2303. 06217\nRay, S. (2019). A quick review of machine learning algorithms. In 2019 \nInternational Conference on Machine Learning, Big Data, Cloud \nand Parallel Computing (COMITCon) (pp. 35–39). IEEE. https:// \ndoi. org/ 10. 1109/ COMIT Con. 2019. 88624 51\nRiedl, R. (2022). Is trust in artificial intelligence systems related to user \npersonality? Review of empirical evidence and future research \ndirections. Electronic Markets, 32(4), 2021–2051. https:// doi. org/ \n10. 1007/ s12525- 022- 00594-4\nRix, J., & Hess, T. (2023). From “handmade” to “AI-made”: Mitigat-\ning consumers’ aversion towards AI-generated textual products. \nAMCIS 2023 Proceedings.\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. \n(2022). High-resolution image synthesis with latent diffusion \nmodels. In 2022 IEEE/CVF Conference on Computer Vision and \nPattern Recognition (CVPR) (pp. 10674–10685). IEEE. https://  \ndoi. org/ 10. 1109/ CVPR5 2688. 2022. 01042\nRuthotto, L., & Haber, E. (2021). An introduction to deep generative \nmodeling. GAMM-Mitteilungen, 44(2), e202100008. https:// doi. \norg/ 10. 1002/ gamm. 20210 0008\nSamtani, S., Zhu, H., Padmanabhan, B., Chai, Y., Chen, H., & \nNunamaker, J. F. (2023). Deep learning for information systems \nresearch. Journal of Management Information Systems, 40(1), \n271–301. https:// doi. org/ 10. 1080/ 07421 222. 2023. 21727 72\nSchneider, J., Seidel, S., Basalla, M., & vom Brocke, J. (2023). Reuse, \nreduce, support: Design Principles for green data mining. Busi-\nness & Information Systems Engineering, 65(1), 65–83. https:// \ndoi. org/ 10. 1007/ s12599- 022- 00780-w\nSchoormann, T., Strobel, G., Möller, F., Petrik, D., & Zschech, P. \n(2023). Artificial intelligence for sustainability - A systematic \nreview of information systems literature. Communications of the \nAssociation for Information Systems, 52(1), 199–237. https:// doi. \norg/ 10. 17705/ 1CAIS. 05209\nSchramowski, P., Turan, C., Andersen, N., Rothkopf, C. A., & Ker -\nsting, K. (2022). Large pre-trained language models contain \nhuman-like biases of what is right and wrong to do. Nature \nMachine Intelligence, 4(3), 258–268. https:// doi. org/ 10. 1038/ \ns42256- 022- 00458-8\n Electronic Markets (2023) 33:63\n1 363 Page 16 of 17\nSchuhmann, C., Beaumont, R., Vencu, R., Gordon, C. W., Wight-\nman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Worts-\nman, M., Schramowski, P., Kundurthy, S. R., Crowson, K., \nSchmidt, L., Kaczmarczyk, R., & Jitsev, J. (2022). LAION-5B: \nAn open large-scale dataset for training next generation image-\ntext models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Bel-\ngrave, K. Cho, & A. Oh (Eds.), Advances in neural information \nprocessing systems 35. Curran Associates, Inc.\nSelz, D. (2020). From electronic markets to data driven insights. \nElectronic Markets, 30(1), 57–59. https:// doi. org/ 10. 1007/  \ns12525- 019- 00393-4\nSmits, J., & Borghuis, T. (2022). Generative AI and intellectual prop-\nerty rights. In B. Custers & E. Fosch-Villaronga (Eds.), Informa-\ntion Technology and Law Series. Law and Artificial Intelligence \n(Vol. 35, pp. 323–344). T.M.C. Asser Press. https:// doi. org/ 10. \n1007/ 978- 94- 6265- 523-2_ 17\nStability.ai. (2023). Stability AI launches the first of its StableLM suite \nof language models. https:// stabi lity. ai/ blog/ stabi lity- ai- launc hes- \nthe- first- of- its- stabl elm- suite- of- langu age- models\nStrobel, G., Banh, L., Möller, F., & Schoormann, T. (2024). Explor -\ning generative artificial intelligence: A taxonomy and types. \nIn Hawaii International Conference on System Sciences 2024 \n(HICSS 2024), Hawaii, USA.\nStrobel, G., Schoormann, T., Banh, L., & Möller, F. (2023). Artificial \nintelligence for sign language translation – A design science \nresearch study. Communications of the Association for Informa-\ntion Systems, 53(1), 42–64. https:// doi. org/ 10. 17705/ 1CAIS. 05303\nSun, J., Liao, Q. V., Muller, M., Agarwal, M., Houde, S., Talamadu-\npula, K., & Weisz, J. D. (2022). Investigating explainability of \ngenerative AI for code through scenario-based design. In  27th \nInternational Conference on Intelligent User Interfaces (pp. \n212–228). ACM. https:// doi. org/ 10. 1145/ 34900 99. 35111 19\nSusarla, A., Gopal, R., Thatcher, J. B., & Sarker, S. (2023). The Janus \neffect of generative AI: Charting the path for responsible con-\nduct of scholarly activities in information systems. Information \nSystems Research, 34(2), 399–408. https:// doi. org/ 10. 1287/ isre. \n2023. ed. v34. n2\nSynthesia. (2023). Synthesia | #1 AI Video Generation Platform. \nhttps:// www. synth esia. io/\nTeubner, T., Flath, C. M., Weinhardt, C., van der Aalst, W., & Hinz, \nO. (2023). Welcome to the era of ChatGPT et al.: The pros-\npects of large language models. Business & Information \nSystems Engineering, 65, 95–101. https:// doi. org/ 10. 1007/  \ns12599- 023- 00795-x\nThe Washington Post. (2022). The Google engineer who thinks the \ncompany’s AI has come to life. https:// www. washi ngton post. com/ \ntechn ology/ 2022/ 06/ 11/ google- ai- lamda- blake- lemoi ne/\nTomczak, J. M. (2022). Deep generative modeling. Springer Interna-\ntional Publishing. https:// doi. org/ 10. 1007/ 978-3- 030- 93158-2\nTomitza, C., Schaschek, M., Straub, L., & Winkelmann, A. (2023). \nWhat is the minimum to trust AI?—A requirement analysis \nfor (generative) AI-based texts. Wirtschaftsinformatik 2023 \nProceedings.\nvan den Broek, E., Sergeeva, A., & Huysman Vrije, M. (2021). When \nthe machine meets the expert: An ethnography of developing AI \nfor hiring. MIS Quarterly, 45(3), 1557–1580. https:// doi. org/ 10. \n25300/ MISQ/ 2021/ 16559\nvan Dun, C., Moder, L., Kratsch, W., & Röglinger, M. (2023). Pro -\ncessGAN: Supporting the creation of business process improve-\nment ideas through generative machine learning. Decision Sup-\nport Systems, 165, 113880. https:// doi. org/ 10. 1016/j. dss. 2022. \n113880\nvan Slyke, C., Johnson, R., & Sarabadani, J. (2023). Generative artifi-\ncial intelligence in information systems education: Challenges, \nconsequences, and responses. Communications of the Associa-\ntion for Information Systems,  53(1), 1–21. https://  doi. org/ 10. \n17705/ 1CAIS. 05301\nVasist, P. N., & Krishnan, S. (2022). Deepfakes An integrative review \nof the literature and an agenda for future research. Communica-\ntions of the Association for Information Systems, 51, 590–636. \nhttps:// doi. org/ 10. 17705/ 1CAIS. 05126\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, \nA. N., Kaiser, U., & Polosukhin, I. (2017). Attention is all you \nneed. In I. Guyon, U. von Luxburg, S. Bengio, H. Wallach, R. \nFergus, S. Vishwanathan, & R. Garnett (Eds.), Advances in neu-\nral information processing systems 30 (pp. 5999–6009). Curran \nAssociates Inc.\nWalters, W. P., & Murcko, M. (2020). Assessing the impact of genera-\ntive AI on medicinal chemistry. Nature Biotechnology, 38(2), \n143–145. https:// doi. org/ 10. 1038/ s41587- 020- 0418-2\nWang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S., Chen, Z., Liu, \nY., Wang, H., Li, J., He, L., Zhao, S., & Wei, F. (2023). Neural \ncodec language models are zero-shot text to speech synthesizers. \narXiv. https:// doi. org/ 10. 48550/ arXiv. 2301. 02111\nWanner, J., Herm, L.-V., Heinrich, K., & Janiesch, C. (2022). The \neffect of transparency and trust on intelligent system acceptance: \nEvidence from a user-based study. Electronic Markets, 32(4), \n2079–2102. https:// doi. org/ 10. 1007/ s12525- 022- 00593-5\nWei, R., & Mahmood, A. (2021). Recent advances in variational \nautoencoders with representation learning for biomedical infor -\nmatics: A survey. IEEE Access, 9, 4939–4956. https:// doi. org/ 10. \n1109/ ACCESS. 2020. 30483 09\nWeidinger, L., Uesato, J., Rauh, M., Griffin, C., Huang, P.-S., Mel-\nlor, J., Glaese, A., Cheng, M., Balle, B., Kasirzadeh, A., Biles, \nC., Brown, S., Kenton, Z., Hawkins, W., Stepleton, T., Birhane, \nA., Hendricks, L. A., Rimell, L., Isaac, W., Gabriel, I. (2022). \nTaxonomy of risks posed by language models. In  2022 ACM \nConference on Fairness, Accountability, and Transparency (pp. \n214–229). ACM. https:// doi. org/ 10. 1145/ 35311 46. 35330 88\nWeisz, J., Muller, M., He, J., & Houde, S. (2023). Toward general \ndesign principles for generative AI applications. In  4th Work-\nshop on Human-AI Co-Creation with Generative Models, Syd-\nney, Australia.\nWeng, S.-S., & Chen, H.-C. (2020). Exploring the role of deep learning \ntechnology in the sustainable development of the music produc-\ntion industry. Sustainability, 12(2), 625. https:// doi. org/ 10. 3390/ \nsu120 20625\nWessel, M., Adam, M., Benlian, A., Majchrzak, A., & Thies, F. (2023). \nCall for papers to the special issue: Generative AI and its tran -\nformative value for digital platforms. Journal of Management \nInformation Systems. https:// www. jmis- web. org/ cfps/ JMIS_ SI_ \nCfP_ Gener ative_ AI. pdf\nWillcocks, L. (2020). Robo-Apocalypse cancelled? Reframing the \nautomation and future of work debate. Journal of Information \nTechnology, 35(4), 286–302. https:// doi. org/ 10. 1177/ 02683 96220 \n925830\nWinston, P. H. (1993). Artificial intelligence  (3. ed., reprinted with \ncorr). Addison-Wesley.\nYang, R., & Wibowo, S. (2022). User trust in artificial intelligence: \nA comprehensive conceptual framework. Electronic Markets,  \n32(4), 2053–2077. https:// doi. org/ 10. 1007/ s12525- 022- 00592-6\nZhan, F., Yu, Y., Wu, R., Zhang, J., Lu, S., Liu, L., Kortylewski, A., \nTheobalt, C., & Xing, E. (2021). Multimodal Image Synthesis \nand Editing: A Survey . arXiv. https:// doi. org/ 10. 48550/ arXiv. \n2112. 13592\nElectronic Markets (2023) 33:63 \n1 3 Page 17 of 17 63\nZhang, C., Zhang, C., Zhang, M., & Kweon, I. S. (2023a). Text-to-\nimage diffusion models in generative AI: A survey. arXiv. https:// \ndoi. org/ 10. 48550/ arXiv. 2303. 07909\nZhang, D., Li, W., Niu, B., & Wu, C. (2023b). A deep learning \napproach for detecting fake reviewers: Exploiting reviewing \nbehavior and textual information. Decision Support Systems, 166, \n113911. https:// doi. org/ 10. 1016/j. dss. 2022. 113911\nZhou, J., Zhang, Y., Luo, Q., Parker, A. G., & Choudhury, M. de (2023). \nSynthetic lies: Understanding AI-generated misinformation and \nevaluating algorithmic and human solutions. In A. Schmidt, K. \nVäänänen, T. Goyal, P. O. Kristensson, A. Peters, S. Mueller, \nJ. R. Williamson, & M. L. Wilson (Eds.), Proceedings of the \n2023 CHI Conference on Human Factors in Computing Systems \n(pp. 1–20). ACM. https:// doi. org/ 10. 1145/ 35445 48. 35813 18\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations. \nTitle:Generative AI for Physical Layer Communications: A Survey. Abstract:The recent evolution of generative artificial intelligence (GAI) leads to the emergence of groundbreaking applications such as ChatGPT, which not only enhances the efficiency of digital content production, such as text, audio, video, or even network traffic data, but also enriches its diversity. Beyond digital content creation, GAI’s capability in analyzing complex data distributions offers great potential for wireless communications, particularly amidst a rapid expansion of new physical layer communication technologies. For example, the diffusion model can learn input signal distributions and use them to improve the channel estimation accuracy, while the variational autoencoder can model channel distribution and infer latent variables for blind channel equalization. Therefore, this paper presents a comprehensive investigation of GAI’s applications for communications at the physical layer, ranging from traditional issues, including signal classification, channel estimation, and equalization, to emerging topics, such as intelligent reflecting surfaces and joint source channel coding. We also compare GAI-enabled physical layer communications with those supported by traditional AI, highlighting GAI’s inherent capabilities and unique contributions in these areas. Finally, the paper discusses open issues and proposes several future research directions, laying a foundation for further exploration and advancement of GAI in physical layer communications.\nTitle:Generative artificial intelligence: a systematic review and applications. Abstract:In recent years, the study of artificial intelligence (AI) has undergone a paradigm shift. This has been propelled by the groundbreaking capabilities of generative models both in supervised and unsupervised learning scenarios. Generative AI has shown state-of-the-art performance in solving perplexing real-world conundrums in fields such as image translation, medical diagnostics, textual imagery fusion, natural language processing, and beyond. This paper documents the systematic review and analysis of recent advancements and techniques in Generative AI with a detailed discussion of their applications including application-specific models. Indeed, the major impact that generative AI has made to date, has been in language generation with the development of large language models, in the field of image translation and several other interdisciplinary applications of generative AI. Moreover, the primary contribution of this paper lies in its coherent synthesis of the latest advancements in these areas, seamlessly weaving together contemporary breakthroughs in the field. Particularly, how it shares an exploration of the future trajectory for generative AI. In conclusion, the paper ends with a discussion of Responsible AI principles, and the necessary ethical considerations for the sustainability and growth of these generative models.\nTitle:Advancements in Generative AI: A Comprehensive Review of GANs, GPT, Autoencoders, Diffusion Model, and Transformers. Abstract:The launch of ChatGPT in 2022 garnered global attention, marking a significant milestone in the Generative Artificial Intelligence (GAI) field. While GAI has been in effect for the past decade, the introduction of ChatGPT sparked a new wave of research and innovation in the Artificial Intelligence (AI) domain. This surge has led to the development and release of numerous cutting-edge tools, such as Bard, Stable Diffusion, DALL-E, Make-A-Video, Runway ML, and Jukebox, among others. These tools exhibit remarkable capabilities, encompassing tasks ranging from text generation and music composition, image creation, video production, code generation, and even scientific work. They are built upon various state-of-the-art models, including Stable Diffusion, transformer models like GPT-3 (recent GPT-4), variational autoencoders, and generative adversarial networks. This advancement in GAI presents a wealth of exciting opportunities across various sectors, such as business, healthcare, education, entertainment, and media. However, concurrently, it poses unprecedented challenges such as impersonation, job displacement, privacy breaches, security vulnerabilities, and misinformation. To addressing these challenges requires a new direction for research to develop solutions and refine existing products. In our endeavor to contribute profound insights to society and advance research on GAI, we present a comprehensive journal which explores the theoretical and mathematical foundations of GAI state-of-the-art models, exploring the diverse spectrum of tasks they can perform, examining the challenges they entail, and discussing the promising prospects for the future of GAI.\nTitle:A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT. Abstract:Recently, ChatGPT, along with DALL-E-2 and Codex,has been gaining significant attention from society. As a result, many individuals have become interested in related resources and are seeking to uncover the background and secrets behind its impressive performance. In fact, ChatGPT and other Generative AI (GAI) techniques belong to the category of Artificial Intelligence Generated Content (AIGC), which involves the creation of digital content, such as images, music, and natural language, through AI models. The goal of AIGC is to make the content creation process more efficient and accessible, allowing for the production of high-quality content at a faster pace. AIGC is achieved by extracting and understanding intent information from instructions provided by human, and generating the content according to its knowledge and the intent information. In recent years, large-scale models have become increasingly important in AIGC as they provide better intent extraction and thus, improved generation results. With the growth of data and the size of the models, the distribution that the model can learn becomes more comprehensive and closer to reality, leading to more realistic and high-quality content generation. This survey provides a comprehensive review on the history of generative models, and basic components, recent advances in AIGC from unimodal interaction and multimodal interaction. From the perspective of unimodality, we introduce the generation tasks and relative models of text and image. From the perspective of multimodality, we introduce the cross-application between the modalities mentioned above. Finally, we discuss the existing open problems and future challenges in AIGC.\nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1109/ACCESS.2017.DOI\nGenerative AI for Transformative\nHealthcare: A Comprehensive Study of\nEmerging Models, Applications, Case\nStudies and Limitations\nSIVA SAI1, AANCHAL GAUR2, REVANT SAI3, VINAY CHAMOLA1 (Senior Member, IEEE),\nMOHSEN GUIZANI4 (FELLOW, IEEE) and JOEL J. P. C. RODRIGUES5, (FELLOW, IEEE)\n1Department of Electrical and Electronics Engineering, Birla Institute of Technology & Science (BITS), Pilani 333031, Rajasthan, India (e-mails:\n{p20220063,vinay.chamola}@pilani.bits-pilani.ac.in)\n2Department of Electrical and Communication Engineering, Maharaja Agrasen Institute of Technology, New Delhi, India (e-mail: anchal.avm@gmail.com)\n3Department of Computer Science and Engineering, Birla Institute of Technology & Science (BITS), Pilani 333031, Rajasthan, India (e-mail:\nf20212536@pilani.bits-pilani.ac.in )\n4Department of Machine Learning , Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi 999041, UAE (e-mail: mguizani@ieee.org))\n5 COPELABS, Lusófona University, Lisbon, Portugal\nThis work was supported by CHANAKY A Fellowship Program of TIH Foundation for IoT & IoE (TIH-IoT) received by Dr. Vinay\nChamola under Project Grant File CFP/2022/027.\nABSTRACT Generative artificial intelligence (GAI) can be broadly described as an artificial intelligence\nsystem capable of generating images, text, and other media types with human prompts. GAI models like\nChatGPT, DALL-E, and Bard have recently caught the attention of industry and academia equally. GAI\napplications span various industries like art, gaming, fashion, and healthcare. In healthcare, GAI shows\npromise in medical research, diagnosis, treatment, and patient care and is already making strides in real-\nworld deployments. There has yet to be any detailed study concerning the applications and scope of\nGAI in healthcare. Addressing this research gap, we explore several applications, real-world scenarios,\nand limitations of GAI in healthcare. We examine how GAI models like ChatGPT and DALL-E can be\nleveraged to aid in the applications of medical imaging, drug discovery, personalized patient treatment,\nmedical simulation and training, clinical trial optimization, mental health support, healthcare operations\nand research, medical chatbots, human movement simulation, and a few more applications. Along with\napplications, we cover four real-world healthcare scenarios that employ GAI: visual snow syndrome\ndiagnosis, molecular drug optimization, medical education, and dentistry. We also provide an elaborate\ndiscussion on seven healthcare-customized LLMs like Med-PaLM, BioGPT, DeepHealth, etc.,Since GAI is\nstill evolving, it poses challenges like the lack of professional expertise in decision making, risk of patient\ndata privacy, issues in integrating with existing healthcare systems, and the problem of data bias which are\nelaborated on in this work along with several other challenges. We also put forward multiple directions for\nfuture research in GAI for healthcare.\nINDEX TERMS Generative AI, ChatGPT, Healthcare, LLMs, Applications.\nI. INTRODUCTION\nTools based on artificial intelligence have gradually increased\nin recent decades, and generative AI has emerged as a pow-\nerful tool within this landscape. Generative AI combines ma-\nchine learning techniques, deep neural networks, and natural\nlanguage processing (NLP) to learn patterns and character-\nistics from vast datasets and generate outputs that resemble\nhuman-generated content. The output can be generated in\nvarious forms, such as audio, video, and text, depending upon\nthe demand. ChatGPT, developed by Open AI, is a language\nmodel capable of generating human-like responses to text\ninputs. It is built upon a transformer model and is one of the\nmost popular GAI models [1]. GAI models such as DALL-E\n[2], Midjourney [3], and Stable diffusion [4] are capable of\ngenerating high-quality images from textual prompts. These\nGAI models also showcase the ability to bridge the gap\nVOLUME X, 2020 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nbetween modalities [5] and aid education [6].\nThe GAI technology has shown promising potential in\nhealthcare. It can revolutionize how we approach medical\ndiagnosis, treatment, and patient care. The GAI models can\nassist healthcare professionals in making clinical decisions\nin various fields such as urology, radiology, and cardiology\n[7]. A very recent survey by Market.us [8] predicts that GAI\nin the healthcare industry is set to reach around $17 billion\nby 2032, primarily driven by the automation in healthcare\noperations of medical imaging and diagnostics and drug\ndiscovery and development.\nTo generate reliable results in healthcare, GAI models need\nto train on a large volume of medical data, including patient\nrecords, medical images, and genomic sequences. These\ntrained models can provide innovative solutions to traditional\nproblem-solving and augment healthcare professionals’ ca-\npabilities to enhance patient outcomes. GAI models can also\nsimulate and predict disease progression, thus further helping\nin better understanding and monitoring. A study by Messiah\net al. indicates the reliability of such models. The GAI model\nwas able to answer all queries regarding typical clinical tox-\nicology cases of acute organophosphate poisoning [9]. The\nGAI technology can also be used in disease management and\nrisk assessment, as well as to increase research education and\ndrug development. It has opened the windows to innovative\nhealthcare using technology [10].\nOne of the major areas where GAI is making a significant\ncontribution to healthcare is medical imaging. GAI models\nlike DALL-E can assist in detecting and diagnosing diseases\nby analyzing patient medical images, such as X-rays, MRIs,\nand CT scans. GAI algorithms can be trained to learn and\nidentify subtle patterns and anomalies in scans that often slide\npast the naked eye. By using generative models, timely inter-\nventions and improved patient care can be provided. With\nhigher accuracy and speed, this technology helps in early\ndisease detection, such as cancer or neurodegenerative dis-\norders. Furthermore, GAI can augment healthcare research\nand education. The models help in experimentation and hy-\npothesis by generating synthesized data. They can produce\nvirtual patient scenarios, enabling more practical education\nfor healthcare professionals. GAI models can help Biomarker\nidentification by analyzing large-scale genomic, proteomic,\nor imaging data. They facilitate research and study by gen-\nerating synthetic data on which medical researchers can\nperform experimentation. GAI models can generate synthetic\ndata samples that exhibit specific biomarker characteristics\nto help study pathology, which helps researchers visualize\ncomplex medical data and facilitate exploratory analysis for\nbetter understanding.\nIn addition to medical imaging and research, GAI has vari-\nous other applications, including drug development, chatbots,\npersonalized patient treatment, medical simulation and train-\ning, clinical trial optimization, and mental health support.\nHealthcare professionals can use various GAI models like\nChatGPT for assistance in diagnosis and treatment. It is seen\nthat GAI models like ChatGPT can learn and identify their\nown mistakes just by prompting it to check if any output is\nwrong. These models can also generate patients’ discharge\nsummaries by leveraging a large amount of data they are\ntrained on and by analyzing patient data and their medical\nrecords. They can do so without any detailed description or\nmeaning of medical terms provided beforehand. This paper\ndiscusses how these and other generative models can enhance\nthe healthcare system.\nThe integration of GAI and healthcare also presents mul-\ntiple challenges. Data privacy, ethical considerations, and\ndata bias are critical aspects that need attention in this GAI-\nhealthcare confluence to utilize the technology’s full poten-\ntial while ensuring patient safety and security. Therefore,\nin this work, the contributions are summarized as follows:\n1. Discuss how GAI can support healthcare systems, high-\nlighting their limitations and how to overcome them. 2.\nAnalyze some real-world GAI models in healthcare systems.\n3. Provide a variety of applications of GAI in healthcare. 4.\nDescribe four real-world scenarios of using generative AI\nin healthcare. 5. Discuss seven healthcare-customized GAI\nmodels. 6. Present several limitations and future directions\non the applications of GAI in healthcare.\nA. ORGANIZATION\nThe rest of the paper is organized as follows. Section II pro-\nvides a brief overview of generative AI. Section III presents\nan analysis of the real-world performance of GAI models\nin healthcare. In Section IV, we provide and elaborate on\na variety of applications of GAI in healthcare.Section VI\ndescribes four real-world scenarios of using generative AI in\nhealthcare - visual snow syndrome, molecular optimization,\nmedical education, and dentistry. Section V discusses seven\nhealthcare-customized GAI models. Sections VII and VIII\npresent limitations and future research directions on GAI\napplications in healthcare, respectively. Finally, the review is\nconcluded in Section IX.\nII. AN OVERVIEW OF GAI\nGenerative AI is an advanced artificial intelligence tech-\nnology that has recently gaining significant attention and\ncorporate funding. Its popularity has led to different startups\nbeing formed solely on the development of GAI technology\n[11]. In response to user prompts, GAI models can generate\nvarious forms of media, including text, images, audio, video,\nand 3D models. This cutting-edge technology harnesses the\npower of pattern recognition and learns from existing data to\ngenerate novel and distinctive results that closely resemble\nthe characteristics of the input training data. GAI has rapidly\ngained popularity and is now widely regarded as one of the\nmost coveted technologies in the world.\nWhat sets GAI apart is its ability to produce realistic\nand coherent outputs. Unlike traditional AI systems designed\nfor specific tasks, GAI surpasses rule-based and determin-\nistic approaches. It extensively utilizes advanced machine\nlearning techniques such as Deep Learning (DL), Natural\nLanguage Processing (NLP), and Neural Networks. These\n2 VOLUME X, 2020\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\ntechniques enable systems to discern patterns and traits from\nvast training datasets, empowering them to generate new\ndata that closely resembles the original information. GAI\nmodels are unique, showing enhanced creativity and novelty\nin generating data. The data produced is not just a copy of\ntraining data but something different with its original traits.\nThey can train on unlabeled data and map underlying patterns\nand structures independently. This ability of unsupervised\nlearning makes GAI models valuable when labelled data is\nscarce.\nGAI models have seen a sharp rise in usage and pro-\nduction. Over time, GAI models have become more so-\nphisticated, employing complex architectures with improved\nstability and quality in generating realistic data in different\nmodalities. Techniques like conditional generation in GANs\nand fine-tuning language models enable more precise and\ncontrollable content generation.\nNoteworthy examples of GAI systems include ChatGPT,\nDall-E, Midjourney and Bard. ChatGPT, developed by Ope-\nnAI, is one of the most popular GAI models known for its\nnatural language processing capabilities. It engages users in\ncoherent and contextually relevant conversations.\nLarge Language Models (LLMs) represent a transforma-\ntive breakthrough in natural language processing (NLP),\nmarked by their immense scale, complex architecture, and\nremarkable language generation capabilities. ChatGPT is\na generative pre-trained transformer and belongs to the\nfamily of large language models. GPT utilizes a decoder-\nonly transformer architecture. This architecture enables it\nto probabilistically generate sequences of words or tokens,\ngiven an input prompt or context. The model predicts the\nmost likely sequence of words following the input based\non the patterns learned during training. It relies heavily\non self-attention mechanisms. The transformer architecture\nfacilitates the model’s ability to process sequential data ef-\nficiently by simultaneously attending to different parts of\nthe input sequence. This mechanism allows the model to\ncapture dependencies and relationships between words in\nlong-range contexts, which is crucial for understanding and\ngenerating coherent text. Notably, the technical prowess and\ngenerative prowess of GPT have set new benchmarks in the\nfield, showcasing its adaptability and performance without\nextensive fine-tuning for specific tasks. GPT models, such as\nGPT-3 [12], have enormous parameters, often in the billions,\nallowing them to capture intricate linguistic nuances and\ncontext. This large parameter count contributes to their abil-\nity to generate diverse and contextually relevant text across\nvarious domains. However, LLMs’ scale and computational\ndemands, like GPT, pose challenges regarding resource re-\nquirements and potential biases inherited from the training\ndata. Further research is ongoing to optimize these models\nfor efficiency and mitigate biases. Nevertheless, the advent\nof Large Language Models, particularly exemplified by GPT,\nstands as a monumental advancement in machine learning,\nrevolutionizing the capabilities of NLP systems and paving\nthe way for increasingly sophisticated language understand-\ning and generation technologies.\nFigure 1 displays existing GAI models. As the figure\nshows, GAI models can generate various data types such as\naudio, video, text, images, 3D visual and code. These GAI\nmodels can generate intricate content that mirrors human\ncreativity. This characteristic makes GAI an invaluable tool\nwhich can be used in various industries, including gaming,\nentertainment, and product design. Over the past decade,\nthis has led many multinational corporations like Google,\nMicrosoft and numerous smaller firms to invest in actively\ndeveloping and refining this technology. Dall-E is another\nGAI model developed by OpenAI. It produces images based\non textual prompts. Its potential can be extended into health-\ncare in many ways. It can be used in medical imaging to assist\nradiologists and clinical workers, as this model can be trained\non different medical image data and their textual descriptions\nand generate relevant synthetic data. It is important to note\nthat GAI is a rapidly evolving field with ongoing research\nand experimentation to develop this technology further.\nIII. REAL WORLD GAI PERFORMANCE IN HEALTHCARE\nGAI has shown great development in recent years, demon-\nstrating remarkable capabilities in various applications, from\ntext to images. However, while GAI has shown great promise\nin controlled environments, assessing the GAI models in\nreal-world scenarios is essential to test the model’s relia-\nbility and effectiveness. While evaluating the performance\nof GAI models outside a controlled model, several factors\nneed to be considered, including the reliability of outputs,\nbias and fairness, generalization across different populations,\ninterpretability, and the potential impact on human decision-\nmaking. Testing requires rigorous evaluation methodologies\nand comprehensive datasets.\nAn evaluation was done on ChatGPT, a large language\nmodel (LLM) by Kung et al. [13].They tested ChatGPT to\nanswer the United States Medical Licensing Exam (USMLE)\nquestions. The test was conducted in three stages of stan-\ndardized tests, and ChatGPT could pass all the stages without\nspecific training or reinforcement. It is important to note that\nthese tests were of expert-level knowledge, and ChatGPT was\nable to pass with 60% accuracy. Furthermore, the answers\nprovided by ChatGPT did not require any specialized input\nand were detailed while including clinical insights and com-\nprehensive reasoning.\nAnother evaluation was done on ChatGPT 4 [14], an\nupgraded version of ChatGPT by Teebagy et al. [15]. They\ntested the model on the Ophthalmology Knowledge As-\nsessment Program (OKAP) examination conducted by the\nAmerican Academy of Ophthalmology (AAO) to assess res-\nidents’ knowledge of ophthalmology training programs. The\nexamination consisted of 180 questions and content of ques-\ntions ranging from anatomy and physiology to ophthalmic\nsubspecialties such as cornea and neuro-ophthalmology [16].\nResults indicated that ChatGPT 4 could pass the examination\nand outperform its predecessor, ChatGPT 3.5, thus establish-\ning the growing potential for using GAI models in healthcare\nVOLUME X, 2020 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nTEXT AUDIO VIDEO IMAGE 3D VISUAL CODE\nGENERATIVE AI MODELS\nMurf  AI Voice\nGenerator\nChatGPT Google\nImagen Video\nOpenAI-Dall-\nE2\nOpenAI-Point-\nE\nDeepMind\nAlphaCode\nGoogle's\nBERT\nStability Stable\nDiffusion\nOpenAI\nMuseNet\nKrikey AI\nplatform\nOpenAI ChatGPT\n& ChatGPT 4 \nGoogle Dream\nFusion\nCTRL Soundful AI\nMusic Generator\nMeta Make a\nVideo\n Midjourney Microsoft\nRodin Diffusion\nFIGURE 1: Popular GAI models\nconsultation and treatment. ChatGPT also passed the radiol-\nogy board–style examination nearly [17]. The examination\nof 150 questions without images of multiple choice answers\nwas conducted with the questions of the difficulty level of\nCanadian Royal College and American Board of Radiology\nexaminations. ChatGPT gained an overall score of 69% by\ncorrectly answering 104 questions out of 150. It showed\nbetter performance in clinical management questions and\nlow-order thinking questions. Table 1 shows that GPT 4 can\nhandle multi-level prompts, perform complex analysis and\nprovide more detailed results compared to its predecessor\nChatGPT-3 [18].\nThe performance evaluation of ChatGPT showed the po-\ntential reliable use of GAI in healthcare. With the GAI\nmodels currently evolving and under rigorous research and\ndevelopment, it opens a wide window for integrating various\nGAI models in the daily dealings of healthcare professionals.\nThe GAI technology will soon be positioned to be a part\nof regular clinical practice, contributing in various lengths,\naccounting for its wide applications in various healthcare\nfields, and enhancing patient care.\nIV. APPLICATIONS\nThis section discusses the applications of using GAI in\ndifferent healthcare spheres. Figure 2 presents an overview\nof the discussed applications.\nA. MEDICAL IMAGING\nMedical imaging is a rich and non-invasive technique that\nprovides healthcare professionals with a detailed visualiza-\ntion of the patient’s anatomical structures for treating medical\nconditions. This technique enables early detection of dis-\neases, improves screening procedures, and guides treatment\nplanning strategies. In the current scenario, medical imag-\ning faces many limitations, such as insufficient annotated\ndata and limited imaging modalities and contrast. GAI has\nemerged as a promising solution to address these challenges\nand further enhance medical imaging capabilities.\n1) Data augmentation\nIt is often challenging to train deep learning models as the\ndatasets available in medical imaging are limited. GAI tech-\nniques, such as Generative Adversarial Networks (GANs)\nand Variational Autoencoders (V AEs), provide a reliable\nsolution as they can generate synthetic images that resemble\npatient data, increasing the models’ robustness. GAI models\nare also capable of generating data on rare conditions such as\n4 VOLUME X, 2020\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nApplications of Generative AI for\nhealthcare\nPersonalised\n Patient Treatment\nMedical\nImaging\nText\n Generation and\nSummarization\nDrug\nDiscovery and\nDevelopment\nClinical Trial\nOptimization\nMental\nHealth Support\nChatbots Medical\nSimulation and\nTraining\nHuman\nMovement\n Simulation and\nAnalysis\nHealthcare\n Operations and\nResource\nManagement\nFIGURE 2: Applications of Generative AI in Healthcare\nAquagenic Urticaria [19], methemoglobinemia [20] and gen-\nerate missing data, thus providing an augmented and diverse\ndataset for training and evaluation, enhancing the ability to\ndetect and diagnose diseases in real-world scenarios. It is\nobserved that while GAI methods like GANs and V AEs play\na pivotal role in addressing limited annotated data in medical\nimaging, the extent of their utilization might vary due to the\nprevalence of scarce datasets for rare conditions.\n2) Image enhancement and reconstruction\nOne of the essential elements in disease diagnosis through\nmedical imaging is the image quality of scans such as an X-\nray or CT scan. When this image suffers distortion due to\nnoise, missing data, and low resolution, it can lead to misin-\nterpretations and delays in the diagnosis. GAI models can be\ntrained to remove noise from these images using frameworks\nsuch as generative adversarial networks and autoencoders;\nthis improves the accuracy of quantitative analysis, allow-\ning reliable measurements and quantitative parameters that\neventually aid in assessing disease progression, longitudinal\nmonitoring and treatment response. Diffusion-based model\nDiffMIC [21] is tailored for medical image classification. It\nfocuses on eliminating noise and perturbations while robustly\ncapturing semantic representations. DiffMIC uses a dual con-\nditional guidance strategy that enhances step-wise regional\nattention by conditioning each diffusion step with multiple\ngranularities. Additionally, this study [21] proposes a method\nto learn mutual information within each granularity by en-\nforcing Maximum-Mean Discrepancy regularization during\nthe diffusion forward process. GAI models can also produce\nhigh-resolution images from low-resolution data by applying\na super-resolution approach. The super-resolved images are\nused for accurate disease diagnosis and detailed study of\nthe anatomical structures. Bing et al. [22] used an enhanced\ngenerative adversarial network for the super-resolution re-\nconstruction of images. The authors improved the squeeze\nand excitation blocks in GANs generator and discriminator\nby strengthening important features and weakening the non-\nimportant ones. Furthermore, they used low function loss to\ntrain the model by combining L1 loss, mean square error loss,\nperceptual loss, and relativistic adversarial loss.\nFurthermore, challenges like varying sequence lengths,\nmissing data or frames, and high dimensionality pose sig-\nnificant hurdles for conventional models. A novel approach\ncalled Sequence-Aware Diffusion Model (SADM) [23] is\nintroduced for generating longitudinal medical images to\naddress this challenge of modeling the dynamic anatomy\nof the human body, which can be influenced by both long-\nterm (e.g. chronic diseases) and short-term (e.g. heartbeat).\nIt introduces a sequence-aware transformer as the condi-\ntional module within the diffusion model. This innovative de-\nsign enables learning longitudinal dependencies, even amidst\nmissing data during training. Furthermore, it enables the\nautoregressive generation of image sequences during infer-\nence, offering a more comprehensive insight into anatomical\nchanges over time.\n3) Cross-Modality Image Translation\nThere are many ways to perform medical imaging. These\nmethods include techniques such as X-ray, positron emis-\nsion tomography (PET), magnetic resonance imaging (MRI),\ncomputed tomography (CT), ultrasound, and Single-Photon\nVOLUME X, 2020 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nEmission Computed Tomography (SPECT), to name a\nfew. These different techniques are called modalities. Each\nmodality uses different physical properties and imaging tech-\nniques to generate images; each has different advantages\nand limitations regarding image quality, spatial resolution,\nsensitivity, and contrast.\nCross-modality image translation converts medical images\nfrom one modality to another while preserving relevant fea-\ntures. This enables the fusion of modality-specific advantages\nand complementary information; it addresses data limitations\nand offers clinical flexibility. This flexibility allows several\ndiagnostic choices, but it is a challenge to translate infor-\nmation from one modality to another. GAI techniques, such\nas generative adversarial networks (GANs) and variational\nautoencoders (V AEs), have shown remarkable capabilities in\ncross-modality image translation. They excel at learning the\ncomplex mapping between different image modalities. They\npreserve underlying structural and pathological information,\nfacilitating accurate cross-modality translation. Generative\nAI also helps overcome modality-specific limitations, such\nas if a modality has spatial resolution or contrast issues.\nGAI can transform images from higher resolution or con-\ntrast modality to enhance visualization. It helps healthcare\nprofessionals make more clinically informed decisions; they\ncan leverage the benefits of multiple modalities to produce\nadvanced imaging to facilitate diagnosis and treatment.\nGenerative AI-based cross-modality translation opens up\nnew avenues for research and innovation in medical imag-\ning. It has proved useful in generating synthetic images,\npreserving important information, promoting research and\ninnovation, and fusing inter-modality data. The GAI has\nunlocked the full potential of diverse image modalities by\nwhich healthcare professionals can make better diagnoses\nand devise personalized patient treatment strategies.\n4) Interpretability and Explainability\nGAI models help achieve interpretability and explainabil-\nity in medical imaging [24]. Deep learning models often\nlack this, making understanding the reasoning behind their\npredictions challenging. Interpretability refers to the ability\nto understand and explain the decision-making process of\nthe AI model. In medical imaging, interpretability is crucial\nbecause healthcare providers and patients must trust and un-\nderstand the generated results to proceed with treatment and\nmake informed decisions [25]. Various methods can achieve\nthis [26], the most important being visualization. Learning\nfeatures are visualized to understand which image regions\nor characteristics influence the generated output. For this,\nattention mechanisms are also used, explicitly highlighting\nthe important regions or features that contributed to the\noutput; this provides transparency and a clear understanding\nto healthcare professionals as to why certain areas are em-\nphasized. Explainability tells the user why a specific decision\nwas made by providing a clear and coherent explanation.\nIt makes the decision-making process more transparent and\neasy to understand for the users, allowing them to put better\ntrust in the model. Some explainability techniques help the\nmodel achieve this. Local explanation focuses on explain-\ning the decision on a specific image instance; this helps\nunderstand the model’s decision at every level. Keeping the\nmodel architecture and decision-making process simple and\ntransparent can enhance explainability. GAI aids in achiev-\ning interpretability and explainability, which are crucial for\ntrust and understanding in medical AI. Techniques such as\nattention mechanisms and local explanations contribute to\ntransparency; however, ensuring a balance between model\ncomplexity and explainability remains challenging.\nB. DRUG DISCOVERY AND DEVELOPMENT\nDrug development is associated with bringing new therapies\nto the market. It is a complex and time-consuming pro-\ncess involving high costs and low success rates. GAI offers\na promising solution to the de novo design of molecular\nstructures. It can generate novel compounds, optimize drug\ncandidates and predict the properties of drugs. GAI is capable\nof 1D, 2D and now 3D models of molecules [27]. Inception\nScore (IS) was proposed by Salimans et al. for generative\nmodels [28], which investigates if the generated molecules\ncan be classified correctly to cover the chemical space de-\nfined by the training set.\nAmong various challenges in drug development is the\nprediction of protein function, an area that has seen sig-\nnificant advancement through various machine-learning ap-\nproaches in recent years. However, prevalent methods often\nframe this task as a multi-classification problem, assigning\npredetermined labels to proteins. Prot2Text [29] is a novel\napproach that departs from the traditional binary or cate-\ngorical classifications by predicting protein functions in a\nfree-text format. This innovative methodology employs an\nencoder-decoder framework that integrates Graph Neural\nNetworks (GNNs) and Large Language Models (LLMs).\nThrough this amalgamation, diverse data types, including\nprotein sequences, structures, and textual annotations, are\neffectively assimilated, ensuring a comprehensive represen-\ntation of proteins’ functions and generating detailed and\naccurate descriptions. To evaluate the efficacy of Prot2Text,\na multimodal protein dataset was curated from SwissProt,\ndemonstrating its effectiveness through empirical analysis.\nThe results underscore the transformative potential of mul-\ntimodal models, particularly the fusion of GNNs and LLMs,\nwhich equips researchers with potent tools for precise predic-\ntions of proteins’ functions.\nThe GAI also assists in understanding the structure-\nactivity relationship (SAR) of the molecules. It generates a\ndiverse set of molecules and studies how the activity changes\ncorresponding to their structure which helps researchers get\ninsights into chemical features essential for drug-target inter-\nactions.\nThe GAI also facilitates generating new molecules with\ndifferent core structures while retaining key pharmacophoric\nfeatures known as scaffold hopping. It enables the re-\nsearchers to explore the chemical space beyond existing\n6 VOLUME X, 2020\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nPrompts ChatGPT GPT 4\nPrompt 1: Diagnosing a patient\nshowing ambiguous symptoms\nA Patient presents with occasional dizziness,\nweight loss, fatigue and low blood pressure.\nWhat are some possible causes of these\nsymptoms?\nA 30 year old female with a 2 month\nhistory of unintentional weight loss\nof about 10 pounds, progressive fatigue\nand episodes of dizziness. Please provide\ndifferential diagnosis and suggest relevant\ndiagnostic tests.\nPrompt 2: Patient education Explain Tuberculosis in simple terms.\nCreate a patient friendly handout on\ntuberculosis, including an overview of\nthe condition, symptoms, risk factors,\npotential complications, and management\nstrategies.\nPrompt 3 – Reviewing medical\nresearch\nTell me about the benefits of exercise in\nimproving mental health.\nDefine the relationship between physical\nexercises and mental health by summarizing\nrecent research findings. Include the\ninfluence of different types of exercise and\nrecommendations for various populations.\nTABLE 1: Performance of GPT 4 over ChatGPT based on user prompts\nscaffolds, potentially leading to improved drug candidates\nwith different properties and mechanisms of action.\nC. PERSONALIZED PATIENT TREATMENT\nGAI has advanced significantly in personalized patient treat-\nment, evolving from initial predictive modeling to integrat-\ning specific conditions for tailored treatment plans. Condi-\ntional Variational Autoencoders (CV AEs) are GAI models\nthat combine conditional variables and variational autoen-\ncoders to learn latent patient information while incorporating\nspecific conditions relevant to personalized treatment. By\naltering values of different conditions, the model generates\nvarious treatment plans tailored to individual patients [30].\nFine-tuned NLP models such as Bidirectional Encoder Rep-\nresentation from Transformers (BERT) have improved in\ngenerating personalized treatment summaries and adaptive\nplans based on patient data.\nD. MEDICAL SIMULATION AND TRAINING\nMedical trainees and professionals must refine their clinical\nskills in a controlled environment; this is done via medical\nsimulation. Realistic and immersive experiences of real-life\ncritical conditions can be simulated using GAI. GAI tech-\nniques such as virtual patient simulation, procedural simula-\ntions, scenario generation, and haptic feedback can transform\nmedical education.\nGAI models can be employed to generate virtual avatars\nof patients. These avatars resemble closely to the patients\nand can be customized by adjusting parameters such as age,\ngender, and medical history, allowing for a more realistic\nexperience. StyleGAN2 (Style-based Generative Adversarial\nNetwork) model can generate high-resolution images with\nrealistic details; these models can be adapted to generate\nvirtual avatars of the patient [31]. By incorporating data from\nphysiological models and clinical knowledge, the GAI can\ncreate fundamental changes in organ function, vital signs\nand disease progression over time by capturing the tem-\nporal dynamics and complications associated with specific\nconditions. It not only helps medical professionals simulate\ncomplicated surgery but also helps train medical students.\nThe generative models can expose healthcare professionals\nto a wide range of patient cases, sudden deterioration, and\nadverse reactions. The different scenarios are designed to\ntrain them to handle complex and unpredictable situations,\nenhancing their decision-making skills and clinical compe-\ntence. The generative models can generate rare and uncom-\nmon conditions; this allows students to Gain exposure and\ndevelop proficiency in dealing with such cases.\nGAI models can generate realistic haptic feedback by\nanalyzing visual and contextual information. The haptic sig-\nnals mimic the tactile sensations experienced during medical\nprocedures. The generative models can also generate 3D\nrepresentations of organs, bones, or blood vessels, which\ncan be integrated into haptic feedback systems allowing\nprofessionals to interact with and manipulate them as if they\nwere real.\nE. CLINICAL TRIAL OPTIMIZATION\nIntroducing and evaluating new interventions and translating\nresearch findings into clinical practice is essential. They al-\nlow head-to-head comparisons of different treatment options,\nproviding researchers with valuable evidence for choosing\nthe most appropriate treatment for specific patient popula-\ntions. However, the complexity and challenges of recruiting\na diverse patient pool while ensuring safety make clinical\ntrials tedious. GAI presents a transformative approach to\naddress these complexities and optimize clinical trials. GAI\nhelps advance protocol design and validation for clinical\ntrials [32]. It does so by simulating virtual trials with differ-\nent designs, randomizing strategies or building an inclusion\nVOLUME X, 2020 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\ncriterion. GAI helps researchers reduce biases, predict treat-\nment responses based on patient characteristics, and simulate\ndifferent scenarios. Generative models like DeepSurv and\nDeepHit can predict patient responses based on different\npatient characteristics and genetic information; these models\ncan evaluate the potential outcomes of different interventions,\nwhich aids professionals in understanding how different\nsubgroups react to specific conditions. GAI helps estimate\nsample size for clinical trials by running multiple iterations\nof simulated virtual clinical trials using synthetic patient\npopulations. Dreesbach et al. [33] proposed a new approach\nfor clinical trials using longitudinal clinical study data by\nemploying the Variational Autoencoder Modular Bayesian\nNetwork (V AMBN) model. Virtual patient data was gener-\nated while making theoretical guarantees on data privacy. It\ncould help in trial design and facilitate data sharing. The GAI\ncan successfully select and optimize endpoints for clinical\ntrials. It can identify clinical outcomes and endpoints by an-\nalyzing historical data and meaningful patterns for patients,\nresearchers and regulatory agencies. The GAI in clinical trial\noptimization can significantly enhance trial efficiency, im-\nprove patient stratification, reduce costs and generate reliable\nand generalizable evidence. Researchers can optimize trial\nprotocols using the GAI to personalize treatment and improve\npatient care.\nF. MENTAL HEALTH SUPPORT\nMental health is of utmost importance in leading a healthy\nand peaceful life. Any disruption in mental health directly\nimpacts the overall well-being of an individual. Today, prob-\nlems like anxiety, frustration, and depression are common\nworldwide. The GAI is a tool that can help the stressed\npopulation to improve their quality of life. The GAI gives\npersonal treatment plans and therapy based on individual\nneeds. They can perform sentimental analysis by analyzing\ntext and speech patterns and detecting sentimental and emo-\ntional cues. The GAI contributes to the early detection of\nmental health conditions by analyzing large volumes of data\nand identifying patterns and indicators suggestive of specific\nmental health conditions and can flag individuals who may\nbe at a higher risk or may require immediate assistance.\nYang et al. [34] use generative adversarial networks and\nhierarchical attention mechanisms to diagnose depression us-\ning multi-modal data, including text and audio physiological\nsignals. Generative AI can generate an immersive virtual\nreality environment for mental health patients, providing a\nsafe time off from the real world [35]. A correct dosage of\nthis can significantly improve their conditions; the virtual\nreality environment can simulate exposure therapy, relaxation\ntechniques and stress management scenarios. By leveraging\nGAI capabilities, mental health support can be more acces-\nsible, effective and personalized. With human oversight, the\nGAI can augment existing mental health services. Therefore,\nLeveraging GAI capabilities augments existing mental health\nservices, making them more accessible, effective, and per-\nsonalized. With human oversight, GAI tools can supplement\nand enhance mental health support by providing tailored in-\nterventions, ultimately aiding in improving the overall well-\nbeing of individuals. [36]\nG. HEALTHCARE OPERATIONS AND RESOURCE\nMANAGEMENT\nHealthcare operations and resource management are complex\nand multifaceted areas which involve various aspects such\nas resource allocation, demand prediction, workflow opti-\nmization, and much more. Largely, these tasks are driven by\nhuman decision-making, which can be time-consuming and\nprone to errors; given the medical aspect, it can also be fatal\nin some instances. However, with the advent of GAI models,\nhealthcare organizations can harness GAI’s power to make\ndata-driven decisions and enhance operational efficiency. A\ngenerative model, DALL-E, can be used to generate visual\nrepresentations of healthcare facility layouts and floor plans\nto help identify areas of improvement, streamline workflows\nand ensure optimum utilization of space and equipment. It\ncan improve communication and understanding by generat-\ning visual aids for patients with specific needs or language\nbarriers. It also generates illustrations for standard operating\nprocedures (SOPs) or guidelines on different healthcare con-\nditions to train staff and ensure consistent practices. Another\ngenerative AI model, ChatGPT, can assist in operations man-\nagement by scheduling appointments with patients, integrat-\ning with hospital information systems, and providing real-\ntime updates on wait times. GAI models can automate routine\ndecision-making processes such as approving and denying\nand even recommending [37] different procedures based\non predefined criteria or conditions, reducing administrative\nburden and enabling a faster response time.\nH. MEDICAL CHATBOTS\nOne of the significant applications of GAI has been the\ndevelopment of medical chatbots. Chatbots are versatile com-\nputer programs designed to simulate human conversation\nand automated responses. They can serve as virtual assis-\ntants for patient support and engagement. They can answer\ncommon questions, offer guidance and provide medication\nreminders. ChatGPT, based on the GPT (Generative Pre-\ntrained Transformer) architecture, can be trained on a wide\nrange of healthcare data, enabling it to provide accurate and\nconsistent responses to patient queries. Dave et al.. [38] dis-\ncuss various applications of ChatGPT in medicine, including\nits aid to patients as a virtual assistant and record keeping\nof patient files. Patients usually seek information about any\nsymptom they are showing or a specific medical condition;\nChatGPT can offer preliminary guidance by asking relevant\nquestions. Based on the information provided by the patient,\nChatGPT can assess the urgency and severity of the symptom\nand provide general information about potential causes and\ncommon conditions associated with those symptoms, self-\ncare measures and when to seek medical attention; in this\nway, they assist in triage and symptom assessment.\n8 VOLUME X, 2020\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nChatbots can be a reliable source of health information and\neducation; they can explain medical terms Moreover, it offers\nguidance on various health topics. Information provided by\nChatGPT is reliable as it passed the United States USMLE,\n[13] which is a medical licensing exam; the exam was con-\nducted in three levels, in which ChatGPT could perform near\nthe passing threshold and gave answers with concordance and\ninsights in its explanations. Lee [39] explores the potential of\nChatGPT in medical education as a virtual assistant to teach\nmedical students and increase their engagement and learning.\nRecently, DiagnaMed Holdings Corp. released Dr GAI, a\nGAI medical chatbot based on ChatGPT, which helps people\nin a general health advisory. Dr GAI is the third commercial\nproduct from the company’s Health GAI division, which\nfocuses on building generative AI applications for healthcare.\nAnother chatbot, InstructGPT [40], a ChatGPT variant,\ncan provide step-by-step instructions to patients. It is advan-\ntageous in providing medication administration instructions\nto patients. It can generate step-by-step guides on admin-\nistering different medications, including information from\nopening packages to measuring dosages and using specific\nmedical devices such as syringes or droppers. InstructGPT\ncan also generate medical schedules for patients depend-\ning on their prescriptions. However, it is essential to note\nthat chatbots like ChatGPT and InstructGPT should only\nwork as a tool for assistance under clinical supervision.\nSeeking proper medical guidance from certified healthcare\nprofessionals is essential in all medical conditions. Hence,\nGAI-driven medical chatbots emphasize their versatility as\nvirtual assistants, aiding in triage, providing reliable health\ninformation, supporting medical education, guiding patients\nin medication administration, and highlighting the necessity\nof clinical supervision in all medical conditions. At the\nsame time, while GAI-powered chatbots are beneficial as\nassistance tools, they should always operate under clinical\nsupervision.\nI. HUMAN MOVEMENT SIMULATION AND ANALYSIS\nUnderstanding human movements is vital to get a detailed\npicture of the dynamic anatomy of the human body. Human\nmovement simulation and analysis can be performed using\nGAI technologies and get valuable insights into diagnostic,\ntherapeutic, and performance-based optimization processes.\nHealthcare professionals can significantly improve treatment\nplanning and patient care by carefully analyzing human body\nmovements. Generative models utilize advanced machine-\nlearning techniques to simulate human movement with re-\nmarkable accuracy and detail.\nMidjourney can simulate movements tailored to individual\npatients’ needs, helping in virtual rehabilitation and phys-\nical therapy sessions. It can provide interactive guidance\nand visual demonstrations of correct movement techniques.\nThis allows better patient engagement and rehabilitation out-\ncomes. GAI models also help assess gait abnormalities, an-\nalyze movement patterns and guide rehabilitation strategies\nfor individuals with mobility impairments, enabling tailored\ninterventions and objective progress monitoring.\nBy leveraging Midjourney and similar GAI models in\nhuman movement simulation and analysis, healthcare profes-\nsionals can access a powerful tool that accurately simulates\nrealistic movements, enhancing their understanding of move-\nment patterns and contributing to better patient care.\nJ. INSURANCE PRE-AUTHORIZATION/PRIOR\nAUTHORIZATION\nPre-authorization, also known as prior authorization or pre-\napproval, is a mechanism that healthcare payers (such as\ninsurance companies) use to decide whether to cover and\nreimburse a certain medical operation, prescription, or ser-\nvice. It entails getting the payer’s consent before a health-\ncare service is rendered or a course of treatment is begun.\nPre-authorization ensures that planned healthcare services\nor treatments satisfy the payer’s requirements for coverage,\nwhich may include medical necessity, appropriateness, cost-\neffectiveness, and adherence to set standards or rules. It gives\npayers a mechanism to keep expenses under control, limit\nuse, and guarantee that the services being rendered comply\nwith the conditions of the insurance or healthcare plan.\nPrior authorization is one healthcare procedure that GAI\ncould potentially enhance. Even though the healthcare sector\nhas made progress towards automating and standardizing PA,\nthe procedure still presents administrative challenges. Re-\nviewing PA requests requires a significant amount of clinical\nstaff time from payers. According to reports, doctors and staff\nspend up to 13 hours each week on the PA process [41]; many\nclinicians think this compromises their clinical judgement\nand can delay providing timely care.\nAn in-depth evaluation by McKinsey and Company in-\ndicates that GAI-enabled Pre-Authorization can automate\n50 to 75 per cent [41] of manual activities, increasing ef-\nficiency, lowering costs, and enabling clinicians at payers\nand providers to concentrate on challenging cases and actual\ncare delivery and coordination. As a result, both clinicians\nand health insurance subscribers may have a better overall\nhealthcare experience.\nThe current pre-authorization workflow involves much\nmanual labour. Some payers have started the PA automation\npath to increase productivity, decrease provider distrust and\nunhappiness, and enhance doctor and client experiences. For\ninstance, electronic prior authorization quickens the data\nflow between payers and providers. Electronic PA digitalizes\nworkflows, which speeds up turnaround times. With elec-\ntronic PA, close to 60 per cent of requests were fulfilled\nwithin two hours as opposed to zero requests sent by phone\nor fax.\nThe GAI may help organize data from electronic health\nrecords, emails, policies, medical procedures, and other\nsources by utilizing advanced Machine Learning algorithms\nlike Natural Language Processing and digital and workflow\nmanagement technologies. This could significantly reduce\nlow-value, time-consuming tasks involving searching, com-\npiling, and validating details that people previously per-\nVOLUME X, 2020 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nformed manually. While GAI will undoubtedly change the\nprior authorization process, several obstacles must first be\nsolved. Payers will first want unrestricted access to EHRs,\nwhich requires rigorous adherence to data privacy laws and\na significant amount of design work to assure interoperabil-\nity among diverse EHR application software and platforms.\nAnother crucial requirement for AI-driven PA automation\nis the definition of common criteria for attachments, data\nblueprints, and information-sharing protocols by industry\nparticipants at the operational level.\nAlthough highly skilled physicians will always be the\nones to make the final pre-authorization decisions, GAI can\nhelp payers and providers make critical decisions while also\nincreasing efficiency and improving provider and patient ex-\nperience. Insurance companies can delegate the most difficult\nand delicate decision-making to highly skilled doctors by\nautomating most pre-authorization decisions. For optimal use\nof these advantages, stakeholders will need to collaborate to\ndesign a new set of standards for data sharing and additional\nprotocols for system integration and interoperability.\nK. MEDICAL TRIAGE\nIn medicine, triage is defined as prioritizing the care of\npatients (or catastrophe victims) based on their condition,\nseverity, prognosis, and resource availability. Triage is used\nto identify patients requiring rapid resuscitation, allocate\nthem to a designated patient care area to prioritize their care,\nand start necessary diagnostic and therapeutic procedures\n[42].\nBy offering helpful assistance and enhancing decision-\nmaking, generative AI has the potential to impact the field\nof medical triage significantly [43].Creating triage support\nsystems is an important example of generative AI’s use in\nmedical triage. These systems analyze patient data, includ-\ning symptoms, medical history, and test results, using deep\nlearning and pattern recognition to provide predictions and\nsuggestions on the criticality of therapy [44]. Generative AI\ncan help healthcare personnel make better-educated triage\ndecisions by quickly analyzing massive quantities of data.\nRapid triage can be made possible using the GAI in\nemergencies where time is of the essence. Disaster events and\nemergency departments sometimes include difficult circum-\nstances and scarce resources. Healthcare practitioners can\ninput patient information into the system using GAI models,\nwhich can quickly analyze the data and produce assessments\nthat help prioritize patients based on the extent of their condi-\ntions. Assuring prompt care for individuals most in need can\ngreatly improve productivity, the distribution of resources,\nand, ultimately, patient outcomes. Another field where GAI\ncan help is in addressing inequalities in triage judgements.\nImplicit biases can have an impact on triage decisions, among\nother things. GAI can reduce these discrepancies by offering\na data-driven, objective approach to triage. It is feasible to\nlessen prejudice and increase fairness in the triage process\nby training AI models on various representative datasets,\nensuring that patients receive the proper degree of treatment\nbased on their medical requirements rather than other criteria.\nHowever, ethical issues are raised by using GAI in medical\ntriage. To ensure that triage decisions can be understood,\nsupported, and accepted by healthcare professionals and pa-\ntients alike, transparency and explainability of AI models are\nessential. Furthermore, appropriate security measures must\nbe implemented to defend patient privacy and guarantee the\nsecure handling of sensitive medical data. It is imperative\nto understand that GAI should not take the role of human\nexpertise. Cooperation between artificial intelligence systems\nand healthcare personnel is essential to properly integrate\nGAI into the triage process and take advantage of both\nparties’ strengths Theriseo67:online. Recent developments\nin medical triage highlight the use of advanced AI, such as\nGAI tools, aiming to expedite decision-making by analyzing\na wide range of patient information swiftly and accurately\n[45]. Furthermore, Predictive analytics models incorporating\nAI and machine learning are integrated into triage systems\nto forecast patient outcomes, predict disease progression,\nand identify high-risk patients. Medical triage can be greatly\nenhanced by combining the strength of generative AI with\nhuman judgement, improving patient care and utilization of\nresources in challenging healthcare scenarios.\nL. TEXT GENERATION AND SUMMARIZATION\nMedical data are abundant in medical records, scientific\nliterature and patient feedback; extracting valuable informa-\ntion is time-consuming if done manually. GAI models like\nBART (Bidirectional and AutoRegressive Transformers) can\nprovide significant support. BART is a transformer architec-\nture that utilizes self-attention mechanisms to process and\nunderstand text data. By analyzing patient data and context,\nGAI models like BART can assist in generating clinical\ndocumentation such as discharge summaries, progress notes\nand medical reports that capture relevant information and\ngenerate personalized recommendations for diagnosis and\ntreatment plans. BART reduces the burden on healthcare\nproviders, allowing them to focus more on patient care; it\ncan help generate patient-friendly explanations of medical\nconditions, treatment options and surgical procedures, thus\nfilling any communication gap between patients and health-\ncare providers.\nBART can be leveraged for named entity recognition\n(NER). It can identify and classify specific entities within a\ntext, such as names of people, medical terms or medications\nand aid in organizing and extracting relevant information\nfrom large volumes of text, thus enabling efficient retrieval of\ndata points. Its ability to comprehend and generate coherent\ntext aids in performing information extraction tasks. By ana-\nlyzing patient data and using the information given, BART\ncan produce accurate and standardized automated reports\nsuch as radiology, pathology, and laboratory reports. By\nleveraging GAI models’ text generation and summarization\ncapabilities, healthcare organizations can enhance communi-\ncation, streamline documentation processes, and improve the\n10 VOLUME X, 2020\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\naccessibility and understanding of healthcare information.\nHowever, it is crucial to note that outputs generated by\nthese models need to be validated by healthcare professionals\nto maintain accuracy, reliability, and adherence to ethical\nguidelines.\nM. VIRTUAL REHABILITATION AND REHABILITATION\nROBOTICS\nFor a long time, two communities of academics with dif-\nfering pensiveness and interests have primarily researched\ngenerative artificial intelligence and virtual environments. In\nvirtual rehabilitation, where it may help create immersive and\nindividualized patient experiences, generative AI has much\npotential. In order to create interesting and interactive set-\ntings for therapeutic reasons, virtual rehabilitation combines\ngenerative models with virtual reality (VR) or augmented\nreality (AR) technologies. Because of this pairing, rehabil-\nitation patients can have customized immersive experiences.\nFor example, while movement replication models mimic and\nadjust patients’ movements in virtual environments, pro-\nviding real-time feedback and encouraging motor control,\ngesture recognition models employ generative AI to enable\npatients to engage with virtual environments using natural\nhand movements. The GAI can also be applied to further\ndevelop and enhance rehabilitation robotics by generating\nadaptive robotic movements. The GAI examines patient\nfeatures, mobility data, and treatment objectives to create\ncustomized rehabilitation plans. These programs allow robots\nto modify their movements, pressures, and levels of support\nin response to real-time feedback, maximizing patient par-\nticipation and the efficiency of therapy. Patients can actively\nengage in the therapeutic process by using gestures that cause\nvirtual responses and activities, opening up new paths for\ntherapeutic interaction. Such interactive components improve\nthe rehabilitation process and motivate patients, but they also\nhelp patients enhance their motor control abilities and give\nreal-time feedback on their progression. By using the GAI\nin this field, healthcare professionals can facilitate better\nrecovery outcomes for patients with medical impairments.\nN. ADVERSE DRUG REACTION (ADR) PREDICTION\nAdverse drug responses (ADRs) are unwanted and adverse\neffects of routine drug use. Enhancing drug safety and low-\nering costs can be achieved by anticipating and preventing\nADRs early in drug development. According to a survey,\nover 2 million major adverse drug reactions (ADRs) are\nestimated to happen among hospitalized patients in the US\neach year, resulting in over 100,000 fatalities as a useful tool\nfor anticipating adverse drug reactions (ADRs) during drug\ndiscovery and development.Generative methods can produce\nnovel compounds with expected ADR profiles by examining\nvast datasets of drug structures, chemical characteristics, and\nknown ADRs.\nIn addition to assisting in the early detection and preven-\ntion of adverse drug reactions (ADRs), GAI models provide\ninsightful analyses into the intricate processes that underlie\nADRs, fostering a deeper comprehension of drug safety.\nFor instance, the BMC Bioinformatics journal published a\nneural fingerprint technique in a concurrent deep learning\nframework for ADR prediction such that the label infor-\nmation (drug-ADR relationship) can be used in the feature\ncreation stage of the machine learning process. By creat-\ning molecular explanations and emphasizing the pertinent\nchemical interactions, generative models can also help us\nunderstand the underlying mechanisms of the ADRs. The\ninformation on pharmaceuticals, ADRs, and target proteins\nmay be better represented using knowledge graphs and GAI,\nwhich is extremely important for studying ADR prediction.\nUsing GAI to study ADRs can completely transform drug\ndiscovery and development processes. We can build targeted\ntherapies and interventions to reduce the impact of ADRs\non patient outcomes as the field develops, and the synergy\nof GAI, deep learning, and knowledge graphs improves our\nunderstanding of ADR mechanisms.\nO. SYNTHETIC NON-IMAGE DATA\nAUGMENTATION/GENERATION\nProducing artificial or simulated data that closely mimics ac-\ntual data is known as synthetic data production or augmenta-\ntion. This method has grown in significance in the healthcare\nsector, especially in addressing data privacy issues, extending\ndatasets, and improving the development and testing of algo-\nrithms. The GAI plays a crucial role in developing synthetic\ndata by utilizing its capability to learn patterns and produce\nnew data based on existing instances. Generative models\ncan create artificial data points that capture the statistical\ntraits and attributes of the original dataset by analyzing vast\namounts of real patient data. This synthetic data can be\nutilized alongside current datasets, giving researchers and\ndevelopers access to more varied data samples without sacri-\nficing patient privacy. Additionally, controlled and repeatable\ndatasets can be produced using synthetic data generation for\nbenchmarking and algorithm evaluation.\nThe GAI can also generate synthetic data for specific use\ncases or scenarios that may be difficult to obtain or rare in\nreal-world datasets. Techniques like GANs and V AEs have\nevolved to generate synthetic tabular data resembling original\ndatasets, aiding in financial analysis, medical research, and\nstructured data applications [46].This data can be used in ar-\neas with limited data availability and where generating more\ndata is difficult or not feasible. Enhancing synthetic data with\nthe GAI can also reduce the problems associated with imbal-\nanced datasets. It is common in the healthcare sector to have\nskewed distributions of particular illnesses or demographic\nfeatures due to several factors, such as data collection biases\nor the rarity of particular diseases. By creating synthetic data\nto balance these disparities, generic models can ensure that\nalgorithms are trained on representative and diverse samples,\nleading to more accurate and equitable healthcare solutions.\nBy effectively utilizing the power of the GAI for synthetic\ndata augmentation, we can overcome the problem of data\nlimitation and quicken the creation and implementation of\nVOLUME X, 2020 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAI-driven solutions for better patient outcomes and care.\nP. BIOMARKER IDENTIFICATION\nThe process of finding and verifying particular biological\nmarkers that can be utilized as indicators of a particular\ndisease, physiological condition, or response to treatment is\nknown as biomarker identification. Biomarkers are quantifi-\nable traits that can reveal details about a biological condi-\ntion. Examples include chemicals, genes, proteins, or imag-\ning properties [47]. Biomarkers significantly impact disease\ndiagnosis, prognosis, treatment choice, and monitoring in\nmedicine and healthcare. Medical experts can learn more\nabout a disease’s presence, progression, and severity and\nassess the efficacy of treatment measures by identifying and\nmeasuring biomarkers [48].\nThe GAI offers a revolutionary method for identifying\nbiomarkers in medicine and healthcare. Researchers may\nexamine enormous and complex biological datasets to find\nnew biomarkers by utilizing cutting-edge AI models like\nGenerative Adversarial Networks (GANs) and Variational\nAutoencoders (V AEs). The GAI augments conventional sta-\ntistical techniques, supplying a more thorough understanding\nof biological situations by revealing hidden patterns and link-\nages in genes, proteins, and imaging features. The GAI accel-\nerates the detection of biomarkers by creating hypothetical\nbiomarkers. The biomarker development process can be sped\nup, and expenses can be decreased by effectively testing and\nvalidating these AI-generated biomarkers in experimental\nsettings. Additionally, by producing synthetic data, GAI as-\nsists in solving data shortages and privacy issues. In order to\nimprove the robustness and reliability of biomarker detection,\nresearchers can expand their datasets by creating synthetic\nsamples that closely resemble real-world data.\nThe use of GAI in biomarker identification presents ex-\nciting possibilities for improving patient care and medical\nresearch. Artificial intelligence (AI) models speed up the\nidentification of critical illness indicators and therapeutic\nresponses by analyzing complex biological data and produc-\ning fictitious biomarkers. This game-changing technology\nimproves personalized therapy while deepening our under-\nstanding of various illnesses, opening the door to precision\nmedicine and better patient outcomes. In essence, while\nGAI’s current role in biomarker identification might be in\nits infancy, it is untapped potential captivates researchers,\npromising groundbreaking strides in understanding biomark-\ners’ role in health and disease. As scientists explore and inno-\nvate at the intersection of GAI and biomarker identification,\nthe prospects for revolutionizing diagnostics, prognostics,\nand personalized medicine are incredibly compelling.\nQ. DISEASE PROGRESSION MODELING\nIn disease progression modelling, a disease’s development,\nsymptoms, and potential effects are examined and predicted\nthrough time. The GAI has the potential to be a powerful\ntool in this field and tremendously benefit the healthcare\nindustry [49]. By analyzing huge databases of patient data,\nmedical records, and clinical outcomes, generative models\ncan discover patterns and linkages within the data and imi-\ntate disease development scenarios. These models can pro-\nduce hypothetical patient trajectories that accurately depict\nthe development of ailments while considering a variety of\nvariables, such as genetics, lifestyle, and therapeutic inter-\nventions. This makes it possible for medical practitioners\nand researchers to investigate multiple illness trajectories,\nassess the efficacy of different interventions, and appreciate\nthe potential adverse consequences of different treatment\nmodalities.\nLiu et al. [50] introduce a novel end-to-end network\ndesigned to address the complexities of modeling diffuse\ngliomas, malignant brain tumors that extensively infiltrate\nbrain tissue. The intricate interplay between neoplastic\ncells, normal tissue, and the changes induced by treatments\npresents challenges in accurately modeling glioma tumor\ngrowth. This approach is based on deep-segmentation neural\nnetworks and cutting-edge diffusion probabilistic models to\ngenerate future tumor masks and realistic MRIs depicting the\nanticipated tumor appearance at various future time points for\ndiverse treatment plans. Sequential multi-parametric mag-\nnetic resonance images (MRI) and treatment information\nare used as conditioning inputs to guide the generative dif-\nfusion process, enabling tumor growth estimations at any\nspecific time. By providing treatment-aware generated MRIs,\ntumour growth predictions, and uncertainty estimates, the\nmodel offers valuable insights for clinical decision-making,\naiding clinicians in assessing potential outcomes and guiding\ntreatment strategies.\nAlso, applying the GAI to progressing illnesses modeling\ncan enhance clinical judgment, personalized healthcare, and\nour comprehension of diseases. In addition to reproduc-\ning disease trajectories, the GAI affects disease progression\nmodelling. By employing vast amounts of healthcare data\nand clinical records, generative models can aid in predict-\ning illness outcomes, evaluating the severity of the disease,\nand identifying high-risk patients. To present patients with\ncustomized risk profiles, these models can investigate com-\nplex correlations between various data, including the popula-\ntion, biomarkers, multiple medical conditions, and lifestyle\nchoices. This enables medical professionals to make well-\ninformed decisions about treatment plans and treatments,\nleading to more concentrated and effective healthcare prac-\ntices. Along with contributing to its predictive abilities, GAI\ncan support the development of fresh perspectives and dis-\nease mechanism concepts. By examining the created disease\nprogressions and their related characteristics, researchers can\ngain a deeper understanding of the fundamental mechanisms\nand processes that underlie the course of disease. This infor-\nmation can aid in creating fresh therapeutic targets, make it\nsimpler to find prospective biomarkers and direct the investi-\ngation of novel therapeutic strategies. Moreover, the design\nand optimization of clinical trials may be improved using\nGAI. Researchers can examine alternative scenarios, evaluate\nthe effectiveness of various interventions, and calculate the\n12 VOLUME X, 2020\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\npotential impact of new therapeutics by modelling illness\ndevelopment and treatment responses in virtual patient popu-\nlations. This can speed up the discovery and approval of new\ntreatments, lower expenses associated with the clinical trial\nprocess, and streamline the clinical trial process, eventually\nbenefiting patients by giving them quicker access to cutting-\nedge therapies. The modeling of disease progression with\nthe GAI has enormous potential. Tailored healthcare, better\nclinical decision-making, and a greater understanding of dis-\nease mechanisms all benefit from its capacity to analyze big\ndatasets, simulate disease trajectories, and produce tailored\nrisk profiles. By utilising the GAI, researchers and healthcare\npractitioners can improve treatment plans, understand disease\nprogress, and boost healthcare delivery.\nV. HEALTHCARE-CUSTOMIZED LARGE LANGUAGE\nMODELS\nAdvanced AI systems called large language models (LLMs)\n(GAI models) have been trained to understand and produce\nlanguage in a way comparable to that of humans. These mod-\nels process and analyze text using deep learning techniques,\nenabling them to produce consistent and pertinent responses\nto the context. large language models offer the potential to\nimprove human-computer interactions and automate jobs in\nnumerous sectors where language understanding and gener-\nation are essential. In this section, we describe several GAI\nmodels that are customized for the healthcare domain.\nA. MED-PALM\nMed-PaLM [51] is a large language model (LLM) created\nto offer excellent responses to medical queries. It stands for\nMedical Pre-Trained Language Model.It is trained using ex-\ntensive medical literature, academic publications, electronic\nhealth records, and other healthcare information. Med-PaLM\nhas the capacity to comprehend medical jargon, decipher\nintricate medical ideas, and produce pertinent comments or\ninsights [52]. In addition, Med-PaLM [53] produces precise,\nbeneficial long-form responses to consumer health issues, as\ndetermined by panels of licensed doctors and users. Medical\ndocumentation, electronic health records, medical education\nand research, and information retrieval are only some of the\napplications that Med-PaLM can be used in the healthcare\nindustry. Med-PaLM has the potential to improve efficiency,\naccuracy, and knowledge availability in numerous areas of\nhealthcare delivery and research by making use of its ex-\ntensive medical knowledge and language-generating capabil-\nities. Recently, Google launched an upgraded model of Med-\nPaLM called Med-PaLM 2 [54], which has an 18% leap in\naccuracy compared to its predecessor. Med-PaLM 2 achieved\na staggering 86.5% accuracy rate on the United States Med-\nical Licensing Examination (USMLE) questions [52], which\nis on par with the \"expert\" test takers. Figure3 shows the\nperformance of Med-Palm over other medical models. Med-\nPalm 2 could surpass the 60% passing threshold required for\nthe examination.\nFIGURE 3: Performance of Med-PaLM over other healthcare\nLLM models\nMed-PaLM has several key capabilities that make it valu-\nable in the medical sector. For instance, Med-PaLM aids in\nthe representation of medical knowledge. Anatomy, illnesses,\nsymptoms, treatments, drugs, and medical procedures are\nall included in the Med-PaLM encoding system for medical\nknowledge. Because of this expertise, the model can inter-\npret and produce text unique to medical themes. By doing\nso, Med-PaLM plays a vital role in representing medical\nknowledge. The area of medical documentation is another\none where Med-PaLM is widely employed. Med-PaLM aids\nin producing thorough and accurate medical records. It can\nconstruct reports, automatically extract pertinent data from\npatient contacts, and help keep standardized terminology\nin electronic health records (EHRs). Figure 4 shows the\ndiagnostic report generated by Med-Palm 2 by analyzing the\nimage of a chest X-ray [55].\nAdditionally, Med-PaLM is useful in medical research\nand education. Medical students, researchers, and instructors\ncan use the model’s extensive medical knowledge base and\nlanguage-generating skills. To help in studying and com-\nprehending difficult medical ideas, Med-PaLM can offer\ndefinitions, justifications, and responses to medical ques-\ntions.Condensing research papers, highlighting pertinent in-\nformation, and extracting essential conclusions can help with\nliterature reviews. Med-PaLM may also create hypotheses,\nrecommend research topics, and promote evidence-based\npractice by giving users access to the most recent medical\nliterature. Information retrieval in the medical industry may\nbenefit from the use of Med-PaLM. It can efficiently search\nfor and retrieve pertinent research articles, guidelines, clini-\ncal trials, and other sources of medical knowledge because of\nits capacity to process and comprehend medical content; This\nenables medical practitioners to obtain the most recent, scien-\ntifically supported information, fostering informed decision-\nmaking and improving patient care.\nDespite the fact that Med-PaLM 2 achieved state-of-the-\nVOLUME X, 2020 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nart performance on several multiple-choice benchmarks for\nmedical question answering and that human evaluation shows\nanswers compare favourably to physician answers across\nseveral clinically important axes, more work needs to be\ndone to ensure it is used safely and effectively.The ethical\napplication of this technology will require careful thought, in-\ncluding thorough quality assessment when utilized in various\nclinical contexts with safeguards to reduce hazards in such\ncircumstances. For instance, utilizing an LLM to determine a\npatient’s diagnosis or course of treatment carries significantly\nmore risks than using an LLM to learn about a condition or\ndrug. More research is required to evaluate LLMs used in\nhealthcare for homogeneity and amplification of biases and\nsecurity vulnerabilities inherited from base models. Another\ndrawback is the potential for Med-PaLM to produce plausible\nyet inaccurate or deceptive information. Sometimes language\nmodels can produce responses that appear sensible but lack\nmedical precision or evidence-based backing. Healthcare\npractitioners should use prudence and cross-reference the\ndata supplied by Med-PaLM with reliable sources and their\nown experience. Additionally, Med-PaLM can have trou-\nble handling sensitive patient data and upholding privacy.\nWhen adopting Med-PaLM or any other language model,\nappropriate safeguards must be in place to secure sensitive\ninformation because patient privacy and data security are\ncrucial considerations in the healthcare industry.\nB. BIOGPT\nBioGPT [56] is a pre-trained transformer language model\nthat is domain-specifically generative and designed for pro-\nducing and mining biomedical texts. BioGPT is pre-trained\non 15M PubMed abstracts from scratch and adheres to the\ntransformer language model framework (GPT-2). BioGPT\nis capable of carrying out tasks like providing information,\nretrieving relevant information, and producing writing perti-\nnent to biomedical literature. The goal of BioGPT, which em-\nphasizes the biomedical field, is to help researchers, medical\npersonnel, and scientists with various tasks, such as literature\nreviews, drug discovery, protein modelling, and biomedical\ndata analysis.\nIn comparison to a single human annotation, BioGPT-\nLarge scored a record 81% accuracy on PubMedQA. The\naccuracy of most other NLP tools, including Google’s BERT\nfamily of language models, has not surpassed that of humans.\nBioGPT has several use cases in the field of bio-medicine\nand bio-informatics. Personalized medicine, drug discovery,\nprotein modelling, bio-informatics analysis, literature review,\nand educational help in the biomedical and bioinformatics\ndisciplines are some of the applications of BioGPT. It can\ncontribute to different facets of scientific research, medical\ncare, and educational endeavours in these fields because of its\ncapacity to comprehend and produce human-like language in\nthe setting of biology. By quickly finding appropriate infor-\nmation from voluminous biomedical literature, BioGPT can\nsupport literature reviews and research. Researchers can use\nthe approach to summarize study articles, extract essential\nfindings, and detect relationships between various studies;\nThis helps scholars stay current with the most recent develop-\nments in their fields while saving time. Additionally, BioGPT\ncan support bioinformatics studies and protein modelling. It\ncan help with domain identification, protein-protein interac-\ntions, and protein structure prediction. BioGPT may provide\nresearchers with thorough knowledge of genes, pathways,\nand biological networks by integrating diverse biological\ndatabases and knowledge sources, which can help with data\ninterpretation and analysis.\nBioGPT presents several of the same challenges as Chat-\nGPT despite being trained primarily in biomedical literature.\nInaccurate writing without any references produced by gener-\native language models is of growing concern because it may\nspread false information. Additionally, because BioGPT is\ntrained using previously published medical studies that may\ncontain biases, there is a chance that the GAI will reinforce\nthose prejudices. The model’s ability to produce consistent\nand human-like responses raises questions about possible\nabuse or the spread of false information. The thorough mon-\nitoring, validation, and implementation of suitable controls\nto avoid spreading false or biased information are necessary\nfor the responsible and ethical deployment of BioGPT. While\nBioGPT might be useful in decision-making and biological\nresearch, it should not replace the experience and wisdom of\nresearchers or healthcare professionals. The model’s outputs\nshould be used to support human judgment rather than as\na replacement for critical thinking, subject-matter expertise,\nand expert opinion.\nC. IBM WATSON FOR ONCOLOGY\nThe GAI-powered IBM Watson for Oncology system [57]\nwas created to aid oncologists in selecting the best course\nof treatment for cancer patients. It analyzes a large amount\nof medical literature, patient data, and therapy recommenda-\ntions using natural language processing, machine learning,\nand big data analytics methods [58]. Watson for Oncology\naids oncologists in quickly accessing pertinent medical infor-\nmation by processing and comprehending unstructured clin-\nical material. It also offers recommendations for treatments\nthat are supported by evidence.\nOncologists typically use Watson for Oncology as a deci-\nsion support tool in the healthcare sector. It can be connected\nwith electronic health records (EHRs) and other data to\nanalyze patient data, including medical history, test results,\npathology reports, and treatment recommendations, sources\n[58]. The system evaluates patient information against a\nsizable knowledge collection that includes academic papers,\nclinical studies, treatment regimens, and medical textbooks.\nThen, tailored to each patient’s particular characteristics,\noncologists can receive recommendations and insights on\nprospective therapy alternatives. The availability of expert\nknowledge is one of IBM Watson for Oncology’s main\nadvantages [57]. Oncologists can access a lot of clinical\ninformation and medical literature through the system, which\nmay be difficult to keep up with manually. It provides on-\n14 VOLUME X, 2020\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\ncologists with evidence-based treatment options by providing\nthem with access to the most up-to-date research and clinical\nrecommendations.\nA significant additional benefit is the tailored therapeutic\nrecommendations offered by Watson for Oncology. The al-\ngorithm considers patient-specific characteristics like health\nhistory, genetic data, and treatment response to create per-\nsonalized therapeutic options. As a result, oncologists can\ntailor treatment plans for specific individuals, considering\ntheir particular needs and circumstances. Additionally, Wat-\nson for Oncology has advantages for time management [57].\nOncologists can save a great deal of time by using technology\nto quickly assess huge volumes of patient data and medical\nliterature. Oncologists can review and make wise decisions\nmore quickly since it concisely delivers synthesized and\npertinent information.\nIt is important to remember that IBM Watson for Oncology\nhas some restrictions. The lack of adequate clinical validation\nis one drawback. Despite being educated on a vast body of\nmedical research, Watson for Oncology’s suggestions could\nnot always coincide with those of particular oncologists or\nestablished institutional policies. The system’s suggestions\nneed extensive clinical confirmation and be viewed as an\nadditional tool to aid clinical judgment rather than as a\ncomplete answer [58]. While Watson for Oncology excels\nat evaluating vast amounts of data, it might have trouble\ndeciphering difficult or uncommon instances when there is\na lack of data. It heavily relies on the availability of pertinent\nmedical literature and clinical evidence to create recommen-\ndations, which may be scarcer for particular cancer kinds or\npatient populations.\nIn a broader sense, IBM Watson for Oncology is an\nAI-driven system that supports oncologists in treatment\ndecision-making by offering recommendations supported by\npatient data and medical expertise.Personalized treatment\noptions and access to various information are provided, but\nits recommendations should be carefully weighed, and its\nincorporation into clinical processes should be rigorously\nassessed.\nD. HEALTHCARE LANGUAGE MODELS BY NVIDIA\nCLARA\nThe NVIDIA Clara platform is a collection of AI-powered\ntools and frameworks created primarily for healthcare ap-\nplications [59]. Healthcare language models trained to com-\nprehend and interpret content written in medical natural\nlanguage are part of the Clara ecosystem [60]. These models\nare essential for facilitating efficient clinical natural language\nprocessing (NLP) tasks and assisting healthcare professionals\nin making decisions. The substantial medical text data used to\ntrain the NVIDIA Clara healthcare language models includes\nelectronic health records (EHRs), medical literature, clinical\nguidelines, and other medical sources. The models can under-\nstand the complex language patterns, medical terminologies,\nand context unique to the healthcare sector by utilizing this\nenormous corpus of medical material [59]. The deep learning\nCan you write a report analyzing this chest X-ray?\n   Findings\nAttached devices: None found\nLungs : No substantial pleural effusion. No\npneumothorax. Lungs appear clear\nCardiomediastinal: Heart size normal. mediastinal\ncontours in normal limits\nOther: No acute skletal abnormality.\n   Impression:\nNo active disease seen.. Chest is normal.\nFIGURE 4: X-ray report analysis as done by Med-PaLM 2\narchitectures used to create these language models, including\ntransformer-based models, have proven incredibly effective\nat various tasks involving natural language processing. Clin-\nical entity recognition, medical concept normalization, rela-\ntion extraction, and clinical text categorization are just a few\nof the NLP tasks the models are taught to handle specifically\nfor the healthcare industry [60].\nNumerous healthcare applications can make use of\nNVIDIA Clara’s healthcare language models. They can be\nused, for instance, to automatically extract and categorize\nclinical entities from unstructured clinical literature, such as\nidentifying diagnoses, drugs, procedures, and test findings.\nThis skill is useful for clinical decision assistance, clinical\ndocumentation improvement, and automated medical coding.\nThe models could potentially improve efforts in tailored\nmedication. The models can identify significant traits and\ntrends supporting tailored treatment choices by reading, in-\nterpreting, and comprehending clinical narratives distinct to\nindividual patients; This can help with treatment choice, fore-\ncasting treatment results, and facilitating precision medicine\ntechniques. Overall, NVIDIA Clara’s healthcare language\nmodels give the healthcare industry access to the power\nof cutting-edge NLP [60]. These models provide insightful\ninformation, boost productivity, and help clinical decision-\nmaking for various healthcare applications by comprehend-\ning and processing medical text data.\nVOLUME X, 2020 15\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nE. DEEPHEALTH LLM\nMIT and Massachusetts General Hospital researchers cre-\nated the extensive language model known as DeepHealth\n[61]. It makes use of machine learning and natural language\nprocessing to meet the specific possibilities and challenges\nfaced by the healthcare sector. DeepHealth attempts to close\nthe knowledge gap between unstructured clinical data and\ninsights that healthcare professionals may use. DeepHealth\nis employed in the healthcare sector to improve many facets\nof healthcare delivery and decision-making [62]. Answering\nclinical questions is a crucial application. In order to de-\nliver accurate and contextually appropriate answers to certain\nclinical issues posed by healthcare professionals, the model\nmay assess medical material, such as research articles and\nclinical recommendations; This can speed up knowledge\nretrieval and aid in the use of evidence when making de-\ncisions. Medical image analysis is another field in which\nDeepHealth can be applied. The model has been trained to\ncomprehend radiology reports and medical images, allowing\nit to draw out important details and help with complex image\ninterpretation. It can help with disease diagnosis, abnormality\ndetection, and insight into treatment planning [63].\nBy examining patient-specific clinical narratives, Deep-\nHealth aids in the advancement of personalized medicine\ninitiatives. The program can process and comprehend each\nunique patient’s medical records to extract patterns, risk\nfactors, and treatment outcomes that help determine the best\ncourse of therapy; This can help with treatment outcome\nprediction and precision medicine strategy guidance.\nHowever, DeepHealth has several limitations, much like\nany noteworthy language model. The requirement for consid-\nerable training data and the potential for biases in the training\ndata are two major limitations. The calibre and variety of\nthe data used to train the model significantly impact its\nperformance. The predictions and suggestions made by the\nmodel could potentially be affected by biases in the data.\nFurthermore, it is difficult to interpret the model’s predic-\ntions. Even though DeepHealth can offer viewpoints and\nsuggestions, it can be difficult to comprehend the underlying\nlogic or describe the decision-making process. Concerns may\nbe raised by this lack of interpretability in complex medical\nsituations where explainability is essential.\nTo summarize, DeepHealth is a large language model\ncreated especially for healthcare applications. In order to\nimprove clinical question answering, medical picture analy-\nsis, and personalized treatment, it uses machine learning and\nnatural language processing techniques [62]. Benefits include\nfaster access to medical information, time savings, and en-\ncouragement of evidence-based decision-making. However,\ndifficulties with interpretability, domain adaptation, and the\nquality of the training data persist, which call for careful\nconsideration when using the model in various healthcare\ncontexts.\nF. BIOBERT\nDeveloped primarily for biomedical text mining and natural\nlanguage processing (NLP) activities in the healthcare sec-\ntor, BioBERT (Bidirectional Encoder Representations from\nTransformers for Biomedical Text Mining) is a large lan-\nguage model. It has been trained on a sizable corpus of\nbiological literature and is based on the BERT architecture\n(Bidirectional Encoder Representations from Transformers)\n[64]. BioBERT has shown to be a useful tool for sifting\nthrough biomedical literature and supporting various health-\ncare applications. BioBERT is frequently used in healthcare\nfor tasks like text classification, relation extraction, question\nanswering, and biomedical named entity recognition. It is\nextremely good at processing clinical and scientific materials\nbecause it can comprehend the language and vocabulary\nunique to the biomedical area. BioBERT can recognize and\nextract biomedical items from unstructured text data, includ-\ning diseases, genes, proteins, chemicals, and their interac-\ntions [64]. Figure 5 shows how the pre-training and fine-\ntuning of large language models like BioBert is done with\nthe help of the vast medical corpora.\nEnhancing the effectiveness and precision of biomedical\ninformation extraction is one of BioBERT’s key advantages.\nBioBERT can comprehend the subtleties and intricate link-\nages found in biomedical texts by using the contextual knowl-\nedge and semantic representation developed during training;\nThis helps with activities like automatic annotation, database\ncuration, and the extraction of insightful information from\nresearch publications [64]. Support for biomedical research\nand evidence-based medicine is another benefit of BioBERT.\nThe model can help with literature reviews, knowledge dis-\ncovery, and hypothesis development by looking at a lot\nof biomedical literature. It gives researchers and healthcare\npractitioners access to a vast knowledge base, facilitating\ndecision-making, study design, and the advancement of sci-\nentific knowledge. BioBERT can also fill the divide between\nstructured and unstructured clinical data. BioBERT enriches\nelectronic health records (EHRs) and facilitates a more thor-\nough study of patient data by removing pertinent biological\nelements and their relationships from clinical narratives [64].\nPatient stratification, clinical decision support systems, and\npopulation health analytics can all benefit from this.\nHowever, BioBERT also has certain drawbacks. The avail-\nability and calibre of training data constitute one restriction.\nDespite being trained in a vast body of biological literature,\nBioBERT may not comprehensively understand some fields\nor rare ailments; This may impact its performance on spe-\ncialized or understudied issues. Furthermore, biases in the\ntraining data may affect the model’s output and cause unex-\npected biases in applications used afterwards. Additionally,\nthe computing demands of BioBERT and resource-intensive\nnature can make it difficult to implement it on edge devices\nor in environments with limited resources. The model’s size\nand processing requirements might make it less accessible\nand useful in some healthcare applications.\nIn a nutshell, BioBERT is a potent language model created\n16 VOLUME X, 2020\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nPre-Training Fine-Tuning\nPre-training with bio-\nmedical corpora\nWeight Initializtion\n4.5B Words\n13.5B Words\nPre-Training Corpora LLM Model Pre-Training Task Specific Datasets LLM Model Fine-Tuning\nQuestion answering\nBIOASQ 5a, BIOASQ 5b,....\nNamed Entity Recognition\nBCG2M, NCBI disease,....\nRelation Extraction\nChemProt, EU-ADR,....\nWhat is Full form of mTOR\n --> Mammalian target of\nrapamycin\nThe adult renal failure cause..\n-->   O      O      B      I      O....   \nVariants' in @GENES region\ncontribute to @DISEASES\nsusceptibility \n--> True\nFIGURE 5: Pre-Training and Fine-Tuning of Large Language Models\nfor NLP and biomedical text-mining jobs in the healthcare\nsector. Improved information extraction efficiency, support\nfor research and medicine based on evidence, and enrichment\nof clinical data are some of its advantages [64]. However,\nwhile applying BioBERT in healthcare applications, it is\nimportant to take into account its limitations, which include\ndata accessibility and quality, interpretability, and computing\nneeds.\nG. MED7 LLM\nA large language model called Med7 was created especially\nfor medical NLU (natural language understanding) in the\nhealthcare sector. Med7 is a clinical text data extraction\ntool trained on a wide variety of clinical text data [65].\nUnstructured clinical text sources include electronic health\nrecords (EHRs), clinical notes, and medical literature. It\nfocuses on identifying medical entities and their correspond-\ning qualities to make clinical data analysis more effective\nand reliable. Med7 is employed in the healthcare sector\nto simplify several processes involving extracting clinical\ninformation. It can automatically identify and extract medical\nelements such as diseases, symptoms, medications, therapies,\nand test results from unstructured clinical data. Med7 helps\nto enhance clinical documentation, improve clinical coding,\nand promote clinical decision-making by converting free-text\nclinical narratives into structured data [65]. One of Med7’s\nkey advantages is its better accuracy and efficiency when\nprocessing and understanding clinical language. Applying\nthe knowledge and context it acquired throughout training\nhas allowed Med7 to identify medical organizations and\ntheir properties accurately; This improves healthcare infor-\nmation’s accuracy and thoroughness while requiring less\nmanual labour to separate organized data from unorganized\nclinical information.\nAdditionally, Med7 enhances the interoperability and data\nintegration of healthcare systems. Med7 enables data shar-\ning, communication, and analysis amongst various healthcare\nplatforms by transforming unstructured clinical language into\nstructured data [65]; This makes it possible for electronic\nhealth record (EHR) systems, decision support tools, and\nother healthcare systems to integrate seamlessly, thereby\nincreasing patient continuity and data interoperability. The\npotential for Med7 to help clinical research and commu-\nnity health investigations is an additional advantage. Med7\nsupports cohort identification, patient stratification, and data\nanalysis for research by effectively extracting medical enti-\nties and their features from clinical narratives. It makes large-\nscale data mining and analytics possible by enhancing both\nmedical research and evidence-based medicine [65].\nHowever, Med7 also has certain limitations that must be\ntaken into account. Its reliance on the calibre and variety of\nthe training data is one of its limitations. The availability and\nrepresentativeness of the clinical literature that the model is\ntrained on can have an impact on how well it performs. The\naccuracy and generalizability of the model’s predictions can\nbe affected by biases or gaps in the training data. Addition-\nally, Med7 acts as a \"black box,\" which can restrict inter-\npretability, like other big language models. Comprehending\nthe underlying theory or explanation underlying the model’s\npredictions can be difficult. In crucial healthcare situations\nwhere explainability is crucial, this lack of interpretability\nmay cause problems. For proper use of Med7 in healthcare\napplications, it is essential to be aware of these constraints.\nVI. REAL WORLD PERFORMANCE ASSESSMENT OF\nGAI IN HEALTHCARE\nUsing innovative solutions to overcome traditional methods\nis essential in healthcare. The use of GAI has been progres-\nsively increasing in various fields. This section will discuss\nfour real-world use cases where the GAI has played an\nessential role in healthcare.\nVOLUME X, 2020 17\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. VISUAL SNOW SYNDROME\nA positive visual disturbance known as visual snow syn-\ndrome (VSS) [66] has been characterized as the persistent\nflickering of countless tiny dots throughout the visual field.\nAlternatively, it is similar to viewing the world through the\nstatic noise of an improperly tuned television. The condition\nrequires innovative approaches for understanding and man-\naging, as its subjective nature and diverse symptomatology\nmake it challenging to diagnose traditionally. The current\ndiagnostic criteria for visual snow syndrome include the\npresence of minute dots throughout the entire visual field\nlasting longer than three months, as well as the presence of at\nleast two of the visual symptoms listed below: photophobia,\nnyctalopia, palinopsia, and entoptic phenomena.\nVisual snow is a reasonably uncommon syndrome expe-\nrienced worldwide, and as the researchers’ understanding\nevolves, exploring new innovative technologies becomes cru-\ncial for further understanding and advancement, analysis, and\nimproving patient care for people suffering from this per-\nplexing chronic disease. The diagnosis is primarily based on\npatients’ verbal descriptions of their symptoms; this causes\na barrier to understanding the true nature of the disease, as\nindividual patients may describe their experience differently,\nwhich may be difficult for parents and other healthcare mem-\nbers. For better understanding and seeing through the eyes of\nthe patients, Balas et al. [67] made use of GAI technologies\nto generate images of how a patient affected with visual snow\nsyndrome sees with the help of textual descriptions with the\nhelp of generative artificial intelligence models that could\ntranslate text to the image. Various models were used to get a\nclear image, such as DALL·E2 [2], midjourney [3] and stable\ndiffusion [4]. Figure 6 shows an image generated by Stable\ndiffusion with the text prompt “seeing a playground through\ngrainy static interface”. Although the current situation of\ngenerative text-to-image models shows promising results,\nmore research and training must be done to achieve precise\nresults that can be directly used in the medical industry.\nB. MOLECULAR OPTIMIZATION\nMolecular optimization is the process of improving the prop-\nerties and characteristics of chemical compounds which can\nalter the activity of the target molecule in a desired way. Find-\ning the perfect molecule for a set of specific requirements\norganically is very difficult, and designing a molecule from\nscratch with all the rightful properties is a challenging and\ncomplex task requiring a lot of time and resources as these\nmolecular structures have complex properties. Furthermore,\ntraditional molecule development methods are costly; 2005\npharmaceutical companies spent $2.6 billion to develop new\nUS Food and Drug Administration-approved drugs [68].\nWith the growth in computational power and the develop-\nment of new tools, it is essential to utilize advanced AI\ntools to optimize drug discovery. Deep generative models are\nbecoming popular and can automate the generation of new\nbioactive and synthesizable molecules.\nMolecular optimization can be helped with the help of\nFIGURE 6: Generation of an image using stable diffusion\nwith the text prompt \"seeing a playground through grainy\nstatic interface\"\ngenerative AI.Molecular Optimization with GAI(MOGA)\nevaluates many molecular structures using GAI models. Mol-\nCycleGAN [69], a CycleGAN-based model, is a generative\nmodel which improves the compound designing process.\nAt its core, it has two neural networks: a generator and a\ndiscriminator. The generator network learns to generate real-\nistic molecular structures, whereas the discriminator network\ntries to distinguish between generated and actual molecules.\nMaziarka et al [69]. Discuss how Mol-CycleGAN can gener-\nate a similarly structured molecule with optimized parameter\nvalues. The model was evaluated on optimization objectives\nbased on structural and physicochemical properties. Octanol-\nwater partition coefficient (logP) penalized was used to test\nmolecular optimization. The model was able to increase the\nactivity of a specific inactive drug.\nCADD is Computer Aided Drug Design, which uses in\nsilico methods to leverage existing chemical knowledge. De\nnovo design and virtual screening are two main approaches\nfor drug designing; de novo design uses generative AI models\nand has seen rapid progress [70]. Mol-CycleGAN is used to\ngenerate novel molecules which have multiparameter opti-\nmization. It leverages image-to-image translation to trans-\nform molecular structures between different representations\nor chemical spaces, optimizing desired properties. The model\ntrains from molecules from different chemical spaces named\nsource space and target space and can map the two spaces\nwithout requiring a direct correspondence between specific\nmolecules and generate molecules in the target space with\ndesirable properties. Once the model is trained, it can be used\ndirectly for molecular optimizationby providing the molec-\nular structure from the source space and defining details\nfrom the target space; the model generated has the desired\nstructure and properties. Figure 7 displays the generation of\nthe new molecule using GAI models Variational Autoencoder\nand MolCycleGAN; the generated molecules have a similar\nstructure to the original molecule but with optimized penal-\n18 VOLUME X, 2020\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nOriginal\nmolecule\nReconstructed\n moleculeLatent space\nEncoder Decoder\nMolCycleGAN      \nInput molecule Generated molecule\nExisting property\nPlogP: -7.663\nDesired property\nPlogP: 2.847\nFIGURE 7: GAI models for molecular optimization.\nized logP value [69]. As Mol-CycleGAN preserves struc-\ntural details, it can maintain cycle consistency; that is, the\ngenerated molecules can be converted back to their original\nspace while conserving their essential features. It allows the\ngenerated molecules to be refined and improved via iterative\noptimization loops.\nIn their study, Grisoni et al. [71] employed a combina-\ntion of generative deep learning models and a microfluidic\nplatform to achieve a combination of generative AI and on-\nchip chemical synthesis and successfully generated liver X\nreceptor (LXR) agonists. Their research focused on gener-\nating novel molecular compounds using a specifically tuned\ncomputational pipeline that explored the chemical space of\nLXR-alpha agonists. The pipeline was limited to products de-\nrived from 17 one-step reactions to ensure compatibility with\non-chip synthesis. The GAI model used in the study produced\n25 de novo designs, which underwent subsequent retesting,\nbatch resynthesis and purification. Out of the 14 retested\ndesigns, 12 were confirmed to be potent LXR agonists.\nThis design-make-test-analyze framework demonstrated its\nsuitability for drug design purposes. It showcased the po-\ntential of combining generative deep-learning models with\nmicrofluidic platforms for efficient and effective chemical\nsynthesis.\nBagal et al. [72] leverage deep learning techniques, par-\nticularly transformer-based models, for de novo molecule\ngeneration, known as inverse molecular design. This de-\nsign utilizes SMILES notation to represent character strings,\nenabling natural language processing models for molecular\ndesign. This approach utilizes generative pre-training (GPT)\nmodels to train a transformer-decoder using masked self-\nattention for predicting the next token, focusing on generat-\ning druglike molecules. The MolGPT model performs simi-\nlarly to modern machine learning frameworks in generating\nvalid, unique, and novel molecules.\nGAI models offer their unique ability to generate desired\ncompounds. This approach allows researchers to navigate\nthe vast chemical landscape and uncover new compounds\nwhich exhibit many properties, including improved efficacy,\nreduced toxicity, enhanced stability, or other desirable char-\nacteristics. Utilizing a generative model for molecule synthe-\nsis saves time and resources by automating the optimization\nprocess and facilitating the generation of diverse and high-\nquality molecular structures.\nC. MEDICAL EDUCATION\nGenerative language models (GLMs) and artificial intelli-\ngence (AI) can enhance medical education in various ways,\nsuch as through virtual patients, accurate simulations, cus-\ntomized feedback, evaluation techniques, and eliminating\nlinguistic obstacles. These innovative tools can enhance med-\nical students’ educational outcomes and facilitate immer-\nsive learning environments [73]. Interaction between educa-\ntors, researchers, and practitioners is essential to developing\nhealthy practices, regulations, and transparent AI models that\nsupport the moral and responsible use of GLMs and AI in\nmedical education. Developers can gain greater confidence\nand respect from the medical community by being open and\ntransparent about the data utilized for training, challenges\nfaced, and evaluation techniques.\nMedical education has the potential to be transformed by\ncontemporary GAI models with enhanced effectiveness, in-\nteractivity, and authenticity, such as OpenAI’s ChatGPT and\nGoogle’s BARD [74]. These models provide unprecedented\ncapabilities, including producing text that sounds like people,\nsimulating difficult patient scenarios, and delivering cus-\ntomized learning experiences [75]; This encourages the cre-\nation of a more interesting and relevant learning environment.\nThese GAI tools offer a more dynamic and realistic learning\nexperience than conventional computer-based simulations.\nVOLUME X, 2020 19\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nThey provide more complex practice settings for medical\nstudents, facilitating clinical judgment and patient care. By\nutilizing GLMs’ superior natural language understanding and\nproducing capabilities, platforms like PerSim [76], a novel\nmethod of delivering medical simulation, provide students\nwith contextually relevant patient situations that are more\ndynamic and adaptive than earlier computer-based models.\nDuring simulation exercises, these AI systems can offer real-\ntime, personalized feedback based on a learner’s performance\nand particular learning needs. Students who receive this eval-\nuation can discover their strengths and areas for development.\nWith the use of GAI in formative and summative assess-\nments, medical education can gain from more individualized,\nefficient, and targeted evaluation methods [77]. An example\nof the application of GAI in medical education assessments is\nthe generation of customized quizzes for students. A unique\nformative and summative examination can be created for\neach student by GAI once their strengths and limitations have\nbeen examined. By analyzing student performance and giv-\ning real-time feedback, these artificial intelligence-driven so-\nlutions can assist instructors in creating personalized learning\nprograms that cater to individual requirements and enhance\noverall results. GLMs can be used by a medical educator\nto create a variety of simulated patient scenarios. Students\ncan become familiar with a wide range of medical issues\nand patient interactions because of the variety and realism\nof these scenarios. For instance, a medical student could\nconverse with a dummy patient simulating a rare disease, ask\nquestions, and get answers like actual patients. This can allow\nthe learner to enhance their clinical reasoning abilities in a\nsecure setting.\nThe proficient ability of these models to produce text with\nvarious levels of complexity could improve the availability\nof medical information. AI tools can potentially improve the\naccessibility and comprehension of health information for a\nwide variety of people, from non-specialists to medical pro-\nfessionals, by changing the language and terminology used\naccording to the intended audience. This targeted communi-\ncation strategy can increase health literacy and enable people\nto make better decisions about their health. By producing\nmore explanations, examples, and visual aids, language mod-\nels like chatGPT can improve medical textbooks. Students’\ngeneral comprehension of the subject matter can be improved\nby making difficult medical ideas more understandable. Also,\nmedical students will find it smoother to readily understand a\nstudy’s main conclusions and consequences if language mod-\nels like chatGPT are trained to summarize medical research\npublications. This can assist students in staying current with\nthe most recent research in their subject while saving them\ntime.\nWhile there are numerous current and future potential\nbenefits of using GAI in the medical education sector, they\nare not without a few limitations. Potential problems with\nprecision, dependability, abuse of AI-generated content, and\nworries about academic integrity are real and warrant serious\nconsideration. The possibility of bias, privacy concerns, and\npotential dehumanization in the educational process also\nwarrant caution. The \"digital divide\" is a further crucial factor\nto consider. Unfair access to AI resources and technology\ncould worsen existing inequalities in the educational system,\nespecially in low-resource environments and among disad-\nvantaged student groups. In our age, cyberattacks and the\npossibility of spreading misinformation are two major wor-\nries that come with AI integration into the medical education\nsector.\nThe medical education area must be particularly watchful\nand aggressive in handling these possible issues, given the\nhigh stakes in health care and the potential for harm. So, for\nexample, the AI-generated material must be of the highest\ncalibre. It needs to be carefully evaluated to make sure it is\naccurate and pertinent. Comprehensive and detailed feedback\nloops and appropriate prompting are two strategies that can\nhelp improve the accuracy and dependability of AI-generated\ncontent in medical education. AI systems have been seen to\nengage in biased behaviour and further enlarge pre-existing\nstereotypes due to their training data and occasionally due\nto the skewed dataset. Exercise caution and overcome any\nbiases when implementing GLMs in medical education.\nNumerous earlier examples, such as racial biases in facial\nrecognition software and Chatbot Tay [78] from Microsoft\ntweeting offensive and sexist content, highlight the need for\ncaution. In addition, ethical and legal concerns are raised by\nthe use of generative AI in medical education, emphasizing\nthe necessity for students to get AI ethics training to ensure\nthe responsible and ethical implementation of these cutting-\nedge technologies.\nIn conclusion, both potential and challenges come with\nintegrating GLMs and AI into medical education. GAI mod-\nels can produce precise, individualized content for pupils,\nresulting in more productive learning occasions.It is imper-\native to properly heed potential biases and ethical problems\nto implement these cutting-edge technologies. These mod-\nels’ algorithms rely on enormous amounts of data, and if\nthat data reflects systemic disparities or is biased, it could\nperpetuate unequal learning opportunities or strengthen pre-\nexisting stereotypes. Certain controls and validation proce-\ndures must be implemented to ensure justice, equity, and\ninclusivity in the instructional content produced by GAI\nmodels. Educators, researchers, and practitioners must work\ntogether to develop standards, regulations, and best prac-\ntices that support the moral and efficient integration of GAI\nmodels in medical education. Building trust and credibility\nin AI-powered medical education is largely a function of\ntransparency. Those who developed and implemented these\ntechnologies must openly disclose the underlying algorithms,\ndata sources, and procedures used in creating educational\ncontent. This openness promotes a better awareness of the\nconstraints, prejudices, and potential uncertainties related to\nAI models, enabling educators and students to assess the\noffered instructional content critically.\n20 VOLUME X, 2020\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nD. DENTISTRY\nDental treatment could be enhanced by GAI, which has the\npotential to transform the medical field [79]. Dental research\ncan help to make sure AI is utilized to improve dental\ntreatment, make it more accessible, and benefit patients, prac-\ntitioners, and society at large. The introduction of AI in den-\ntistry is revolutionizing the industry by enabling higher preci-\nsion, fewer mistakes, and reduced human resources needs. By\nusing contemporary dental technologies like medical robots\nand specialized AI models, a new age known as dentistry\nmight greatly increase dentistry’s reliability, reproducibility,\nprecision, and efficiency. Additionally, Dentronics might im-\nprove risk-assessment techniques, diagnostics, and disease\nprediction in addition to improving treatment outcomes by\ndeepening our understanding of disease pathophysiology.\nBy examining patient information like CT scans or 3D\nmodels of the mouth and creating the most effective implant\nplacement plans, GAI models can also help with dental\nimplant planning.To create individualized treatment plans for\neach patient, these models may simulate various scenarios\nwhile considering elements like bone density, nearby teeth,\nand functional requirements. Similar to this example, these\nGAI models can provide digital photos or 3D models of\npotential smile alterations by evaluating facial characteris-\ntics, tooth shape, and proportions. Then, dentists can create\ncustomized and realistic smile designs for people seeking\ncosmetic dentistry procedures. They can interact with pa-\ntients and ensure their needs are properly handled. In order\nto identify and categorize different oral disorders, such as\ncavities, periodontal diseases, and oral malignancies, GAI\nmodels can be trained to analyze photographs or scans of\nthe mouth cavity. These models can help dentists make\nearly diagnoses and detections, improving patient outcomes.\nDental clinical applications and research have a great deal\nof potential to be improved by modern language models like\nChatGPT. They can revolutionize dentistry diagnostics and\ntreatment planning [80].\nWhile GAI models can help in the dental industry, they\nshould be used to assist dental practitioners rather than taking\nthe place of practitioners themselves. GAI models should be\nviewed as instruments to improve dentists’ talents and the\ncare they provide for patients because they play a crucial part\nin diagnosing and treating oral disorders. Using generative\nAI in the dentistry sector comes with limitations as well.\nThe training data’s precision and variety significantly impact\nthe correctness and dependability of these models. Therefore,\nhaving incomplete or biased data can result in less-than-\nideal outcomes and could also result in errors in diagno-\nsis or treatment planning. The entire clinical context and\npatient-specific aspects, such as medical history, lifestyle, or\nindividual variances, which are critical in dental care, are\nnot considered by generative AI models. Complex decision-\nmaking in dentistry requires human expertise and judgment,\nwhich AI models cannot fully mimic.\nAlso, GAI models generally look for patterns and gener-\nalizations in training data but could miss important patient-\nspecific elements. Important factors like medical history,\nlifestyle decisions, dental hygiene practices, and individual\ndifferences are not usually properly considered. Because of\nthis, the produced outputs might not exactly match a patient’s\nparticular situation, which could result in less-than-ideal\ntreatment plans or recommendations.Furthermore, GAI mod-\nels’ poor interpretability and transparency present a problem.\nIt becomes more challenging to comprehend the underlying\ndecision-making process, which limits dentists’ and patients’\ncapacity to fully trust and comprehend the created outputs.\nTo make informed decisions and guarantee the safety of their\npatients, dental professionals need clear explanations and\nclarity about how the AI arrived at its conclusions.\nVII. LIMITATIONS OF USING GAI IN HEALTHCARE\nThe GAI shows great promise in the healthcare industry; it\noffers an innovative approach towards traditional methods\nand has various advantages. Table 2 shows a list of types of\ncontent forms for which large language models (LLMs) are\navailable now and possible models which can be available\nin the future [18]. However, it is vital to note that GAI has\nvarious limitations, like data bias and ethical considerations.\nUnderstanding these limitations is crucial for advancing this\ntechnology in the healthcare sector. Figure 8 displays the lim-\nitations and possible future works that can be implemented\nwhile integrating generative AI in healthcare.\n1) Attribution Problem\nThe difficulty of comprehending and elucidating the moti-\nvations behind the judgements or outputs produced by GAI\nmodels in healthcare applications is known as the \"attribu-\ntion problem\" connected with its usage [81]. Deep learning\nmodels, one type of GAI, have demonstrated an extraordinary\nability to produce complicated and realistic data, such as pa-\ntient records, medical images, and therapy recommendations.\nThese models frequently cannot be understood or explained\neasily, preventing their responsible and efficient application\nin healthcare settings. Because GAI models function as\ncomplicated black boxes, it is challenging to identify and\ncredit the decision-making process. This poses an attribution\nproblem. It is more difficult to comprehend how and why\na GAI model arrived at a specific output than conventional\nrule-based systems or simpler machine learning models. In\nthe healthcare industry, this lack of transparency presents\nmany difficulties. GAI models may unintentionally inherit\nbiases or unequal representation of specific populations when\ntrained on huge datasets. Finding and correcting biases in the\ngenerated outputs is difficult without adequate attribution.\nThis can exacerbate already-existing inequities and result\nin differences in healthcare outcomes. Comprehending the\nattribution and potential biases in GAI models is crucial to\nensure just and equitable healthcare delivery [81].\nThe attribution issue also presents ethical and legal issues.\nIt is essential in the healthcare industry to be able to assign\nblame and accountability for choices made. It might be\nchallenging to pinpoint who or what is at fault if a GAI\nVOLUME X, 2020 21\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nType of content Potential applications Availability\nImage Analysis Detecting and Analyzing images Yes\nText / Conversations Engaging into human-like conversations\nand giving reliable answers to question prompts Yes\nSound Sound based interactions and voice-to-text applications No\nDocument/PDF Analysis Summarising documents and analyzing research papers No\nVideo Text to video outputs based on prompts by user Yes\nTABLE 2: Types of content forms available that LLMs could analyze now and possible versions in the future\nmodel generates inaccurate or harmful results. This lack of\naccountability on a legal and moral level brings concerns\nabout liability and patient safety. Healthcare organizations\nand providers can be reluctant to employ GAI solutions\nwithout clear attribution mechanisms. The attribution issue in\nGAI is currently being worked upon. Attention mechanisms,\nsaliency maps, and post-hoc analysis approaches are just a\nfew of the methodologies being researched for interpretabil-\nity and comprehensibility. These strategies hope to shed light\non the model’s decision-making process by tying the outputs\nto particular features or inputs. Healthcare practitioners can\nmore accurately evaluate generative AI models’ biases, limi-\ntations, and dependability by comprehending the attribution,\nensuring their responsible and efficient usage in healthcare.\n2) Contextualization Problem\nIn the GAI field, contextualization is absorbing and taking\ninto account pertinent contextual data when producing out-\nputs. It entails comprehending the precise context, restric-\ntions and needs related to the activity and using that informa-\ntion to deliver more precise, pertinent, and significant results.\nThis includes user preferences, subject-matter expertise,\ntask-specific requirements, input data, and activity-specific\nrequirements. Considering these contextual elements, the\nmodel can give results that are more closely aligned with the\nspecified criteria and appropriate for the specific application\nscenario.\nConsidering the issue’s wider context is another contextu-\nalization method. This may consider financial resources, time\nrestraints, ethical and moral issues, and institutional rules.\nGAI models can produce accurate, useful, practical, and\npracticable results within the particular application setting by\ntaking these contextual aspects into account. In health care,\nfor instance, contextualization would entail adding pertinent\nresearch findings, medical advice, patient-specific data, and\nclinical protocols into the generative AI framework. By doing\nthis, it would be feasible to ensure that the generated outputs,\nsuch as medical diagnoses, treatment recommendations, or\npatient records, are accurate, clinically relevant, and in line\nwith best practices.\nThe results might not be coherent, relevant, or comply\nwith particular constraints or rules without sufficient con-\ntextualization. GAI models are being advanced in terms of\ncontextualization. Researchers are experimenting with meth-\nods that include utilizing pre-trained models in particular\ndomains, adding external knowledge bases, focusing on task-\nspecific data, and creating attention mechanisms that enable\nthe model to concentrate on pertinent contextual information.\nThese methods seek to improve the model’s capacity to\nproduce more accurate, pertinent, and useful outputs across\nvarious applications.\n3) Data quality and bias\nOne of the significant limitations of GAI is the data quality\nand bias. The framework of GAI produces output based on\nthe training data provided by health care records and medical\nwritings; if the data supplied is biased or of poor quality, it\nwill lead to inaccurate results. These data biases can creep\ninto any development stage and come from several factors,\nincluding demographic disparities, variations in healthcare\npractices, and under-representation of specific populations.\nThe GAI model can amplify these biases and perpetuate\nhealthcare disparities and unequal treatment outcomes. Many\ngenerative AI applications in public deployment have al-\nready seen neglect in addressing bias. The data is unseeingly\nscraped from the internet without giving much attention\nto potential sources of misinformation or bias [82] or the\ntraining data is generated through unreliable sources [83].\nAlthough responses from chatbots powered by LLM may\nseem creative, they simply reflect the model’s extensive ana-\nlytical understanding of which words have been used before\nothers in the text that it has already viewed. They cannot\nunderstand any language they use, including their responses\nand the prompts they are given. Models trained on a large\nbody of internet data with little filtering (such as ChatGPT or\nsteady diffusion, for example) have absorbed facts and false\ninformation, biased and fair stuff, and hazardous and innocu-\nous things. LLMs run the risk of duplicating, amplifying, and\nspreading bad content and false information without a way to\nevaluate any of these characteristics prior to responding to a\nprompt, as shown by many examples.\n4) Generalization of unseen data\nThe generalization of unseen data is essential for GAI models\nto be reliable in real-world healthcare systems. However, this\ngeneralization can be affected due to various factors. If the\ngenerative AI models fail to update data over time or adapt\nto new medical trends, they will struggle to generalize to the\ncurrent data distribution. Furthermore, the generative models\nare sensitive to variations and noise in the input data. This\n22 VOLUME X, 2020\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nnoise and variations can occur due to measurement errors\nor differences in imaging techniques. If the model cannot\nhandle these issues, it will face performance issues when\ndealing with new data, which is different from training data.\nFurthermore, in healthcare, choosing the correct training data\nwithout infringement of copyrights or other ethical consid-\nerations while opting for optimum model performance is\nchallenging.\n5) Patient data privacy\nPatient data privacy is one of the major concerns in health-\ncare. In healthcare treatment, ambient sensors will collect\npatient data, including name, age, area of residence, medical\nhistory and other relevant information. The sensors may also\ncapture information like the patient’s voice, face, or heart\nrate, depending on the hardware. Information like this, if\nleaked, could lead to the exposure of patients’ health status\nand their private information. [84]\nFurthermore, many patients receive in-house treatment or\nhealthcare facilities considered free from sensors.For exam-\nple, a patient might want to restrict monitoring a particular\nbody part during a particular duration while using the bath-\nroom. GAI models must adhere to this decisional privacy [85]\nand give the right to decide on their privacy and the amount\nof information they want to share.\nThe United States has data protection regulation- HIPAA\nto safeguard patient privacy. Generative AI models can be\nvulnerable to adversarial attacks. Malicious hackers can try\nto manipulate the models’ outputs or get unauthorized access\nto sensitive information [86]. Countersecurity measures such\nas intrusion detection systems, encryption, and authentication\nmust be implemented to avoid such attacks.\n6) False information generation\nAn LLM might occasionally reveal the truth or create infor-\nmation that is pertinent, acceptable, occasionally surprising,\ninnovative, and appealing. Other times, it might generate or\nsupport the most flagrant and harmful falsehood. Second,\nthe model cannot determine which one it is at any given\nmoment, let alone alert the user. It is unsure whether the\nstuff it creates tells the truth or contains fabrications, misrep-\nresentations, or objectionable material. Moreover, because\nLLMs are probabilistic algorithms, they may return different\nresponses when given the same task or question more than\nonce. These responses may be updated versions of previously\nincorrect or complicated answers, updated versions of incor-\nrect answers that were previously correct, or combinations\nof these. This behaviour creates an issue with repeatability\nand reliability that necessitates ongoing human supervision\nof model operation.\n7) Integration with current healthcare technologies\nImplementing generative AI models into the current health-\ncare systems can be challenging. The generative models must\nbe compatible with electronic health record (EHR) systems,\nclinical decision support tools, and other healthcare IT infras-\ntructure. This integration can be complex and may require\nfurther study.To access real-time scenarios, generative AI\nmodels should be capable of processing data and generating\noutputs in real-time to ensure that healthcare professionals\ncan make timely decisions. This real-time processing re-\nquires efficient computational infrastructure and optimized\nalgorithms, which must be programmed appropriately before\nimplementation.\n8) Computational Cost\nOne significant factor to consider while integrating GAI into\nhealthcare is cost. While implementing GAI is revolutionary,\nit may take a toll on the pocket. Training and building gener-\native AI models require significant resources, expertise, and\ncomputational infrastructure investment. It requires hiring\nAI technicians, investing in powerful hardware and software\nand acquiring large datasets for models to train on. While\nsome resources require only a bigger initial payment, others\nsupply recurring bills. These include upgrading the models,\nmaintaining the hardware and addressing potential security\nvulnerabilities.\nFurthermore, Healthcare professionals must be trained to\nintegrate this technology into their daily dealings. Providing\ncomprehensive training programs will incur costs associated\nwith building a new curriculum, training sessions, and con-\ntinuous learning initiatives.\n9) Lack of professional expertise\nIt is important to note that generative AI models cannot\nreplace professional human involvement. While the models\ncan assist in decision-making, the responsibility of patient\ncare must be taken by healthcare professionals, and recom-\nmendations by these models must be considered after proper\nvalidation by human clinicians. Guidelines and regulatory\nframeworks to address these ethical considerations must be\nestablished to govern the deployment and use of GAI models\nin healthcare.\n10) Ethics\nEthics plays a pivotal role in ensuring the responsibility\nand deployment of generative AI in healthcare, and the\nintegration of AI into healthcare has introduced complex\nethical considerations which must be taken into care to\nuphold patient privacy, fairness, accountability and overall\nwell-being of the individual. Legal and ethical frameworks\nare mandatory to establish to prevent privacy concerns. Strict\nlaws and governing bodies may protect a patient’s overall\ntreatment process. A recent high-profile lawsuit on Stable\ndiffusion reveals the use of \"derivative works\" by OpenAI’s\nmodels [87]. It suggests that LLM models use their work as\na reference to generate images without the artist’s consent,\nthus invoking copyright issues. These issues, if not dealt with,\ncan affect patient treatment and harm the patient in the long\nrun as these practices will not be limited to copyrights but\nmay also extend to the patient’s consent. One major factor in\nVOLUME X, 2020 23\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nRegulatory challenges Description\nIntellectual Property Intellectual Property issues can be generated if GAI models create proprietary\nmedical literature or research.\nMedical Malpractice Liability\nIf recommendations given by GAI models bring harm to the patient when\nimplemented, who should be held accountable? Healthcare professionals who\nused it, model engineers, or the institution that granted permission for usage.\nQuality Control & Standardization Consistency and reliability of recommendations made by GAI models\nneed to be regulated as well as the data which is used to train the model.\nData Ownership It is hard to justify who owns the data from which the LLMs learn.\nConcerns are raised when it comes to patients’ data.\nContinuous Monitoring & Validation Ensuring continuous accuracy, performance and validity of GAI models\nover different categories at all times is a challenging task.\nInformed Consent\nIt can be difficult for patients to understand the implications of GAI usage\nin their treatment; in every case, GAI is used, the patient must be informed\nof its pros and cons.\nInterpretability & Transparency\nTransparency must be maintained as to how and why a GAI model suggested\na particular treatment. It can be difficult to explain every step taken by GAI\nmodel to patients.\nOver-reliance on GAI Models Over-reliance on GAI models can limit the need to consult professionals every\nnow and then. It can lead to serious implications if the model malfunctions.\nTABLE 3: Regulatory Challenges in Implementation of Generative AI in Healthcare\nethical considerations is transparency. In the current scenario,\na user cannot tell if the output provided by GAI models on\nuser prompts is true without any external assistance. It has\nbeen noted that some GAI models, such as Galactica AI,\ndisplayed made-up citations and papers [88]. If such models\nare used without proper checking, it will lead to misdiagnosis\nand fatal treatment. In addition to these, Table 3 [18] lists\nvarious regulatory challenges that must be considered when\nimplementing generative AI in healthcare [82].\nApart from the above challenges, there is also a chance\nthat excessive dependence on LLMs will result in the un-\ndervaluation of clinical judgment and human competence.\nWhile LLMs can support decision-making, they should not\nreplace a healthcare professional’s knowledge, experience,\nor capacity for critical thought. It is crucial to balance\nthe capabilities of LLMs and the engagement of human\nprofessionals to maintain patient safety and the best possi-\nble healthcare outcomes. GAI researchers, medical experts,\nethicists, and regulatory agencies must continue to work\ntogether on research, development, and collaborative projects\nto address these limitations. To maximize the advantages\nof LLMs while minimizing their drawbacks and potential\nrisks in future healthcare applications, transparent standards,\nstrong validation processes, and strict ethical frameworks\nare essential. These should be seen as inadequate machines\nthat have the potential to greatly increase process efficiency\nbut necessitate tight human oversight and intervention at all\noperational interfaces, including input and output.\nVIII. FUTURE RESEARCH DIRECTIONS\nGAI applications that create new content in response to\ntextual instructions, including text, images, audio, code, and\nvideos, heavily rely on large language models. These GAI\napplications can become techniques with significant potential\nfor spreading false information or damaging and erroneous\ncontent at an unprecedented scale without human oversight,\nguidance, and responsible design and operation. They could,\nhowever, develop into extremely effective, reliable aids for\ninformation management provided they are positioned and\ndeveloped responsibly as companions in offering support to\npeople, enhancing but not replacing their role in decision-\nmaking, knowledge retrieval, and other cognitive processes.\nEnhancing these models’ clinical decision-support skills is\nbecoming increasingly a priority. In this section, we present\nfuture directions for research in GAI for healthcare.\nA. CUSTOMIZED/PERSONALIZED SUGGESTIONS AND\nA PLATFORM FOR INFORMATION EXCHANGE\nLLMs can be progressively improved to provide more pre-\ncise and individualized suggestions for diagnosis, therapy\nplanning, and patient outcome monitoring. These models\ncan deliver timely and context-aware insights to healthcare\npractitioners, improving clinical decision-making. They in-\ncorporate real-time patient data, such as electronic health\nrecords and information from wearable devices. LLMs may\nalso help professionals in the healthcare industry collaborate\nacross disciplines and share knowledge. These models can\nwork as a shared platform for information exchange, enabling\npractitioners from different disciplines to interact and gain\nfrom each other’s experience. They are accessible to many\npractitioners, including doctors, nurses, and allied healthcare\nprofessionals. This partnership may result in more thorough\nand all-encompassing patient care and a culture of ongoing\nlearning and development among healthcare professionals.\n24 VOLUME X, 2020\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nLimitations and Future Works for\nUsing GAI in Healthcare\nData Quality and\nBias\nPatient Data\nPrivacy\nIntegration with\nHealth System\n Ethics\nPersonalized\nPlatform\nBetter Patient\nWorker Interaction\nStreamlining\nOperations\nBridging\nKnowledge Gap\nAttribution\nProblem\nContextualization\nProblem\nGeneralization of\nunseen data\nLack of\nprofessional\nexpertise\nFIGURE 8: Limitations and Future Work\nB. ENHANCED PATIENT AND WORKER INTERACTIONS\nEnhancing LLMs’ natural language comprehension abilities\nto understand better medical jargon, contextual nuances, and\npatient-specific information is another topic of future efforts.\nThis would allow patients and virtual assistants powered by\nthese models to interact more successfully. Patients could\nprovide precise information about their medical conditions,\nreceive explanations and recommendations specific to their\nneeds, and even have dialogues that resemble human inter-\nactions. This could enhance patient education, involvement,\nand informational access to healthcare. LLMs could also\nhelp with cross-language communication and multilingual\nhealthcare encounters. These models’ ability to translate\nbetween languages can assist healthcare workers and pa-\ntients who speak different languages to communicate more\neffectively. Ensuring accurate and effective communication\nbetween patients and healthcare practitioners might consid-\nerably increase access to healthcare services, especially in\nmulticultural and diverse areas.\nC. STREAMLINING ADMINISTRATIVE OPERATIONS\nLLMs can also help the healthcare sector’s administrative\noperations run more smoothly. They can streamline and\nautomate tasks like invoicing, coding, and medical docu-\nmentation, giving healthcare workers more time to provide\ndirect patient care. These models can help create precise\nand uniform clinical notes, summarise patient encounters,\nand extract pertinent data from medical records, improving\nefficiency and easing administrative stress.LLMs may one\nday be essential to advancing medical science. They can help\nwith information synthesis, literature reviews, and finding\npatterns or relationships in the massive body of biological\nliterature. These models could help with drug discovery\nand repurposing initiatives, advancing personalized medical\ntechniques, and accelerating scientific advancements.\nD. ENHANCING DECISION MAKING AND BRIDGING\nTHE KNOWLEDGE GAP\nLLMs can also close the time gap between the limited time\navailable to healthcare professionals and the continually de-\nveloping body of medical knowledge. Keeping up with the\nmost recent research can be difficult for busy practitioners\ndue to the constant influx of new research papers, clinical\ntrials, and treatment guidelines. On the other hand, LLMs\nmay continuously study and analyze the most recent data,\nguaranteeing that healthcare professionals have access to the\nmost up-to-date and pertinent insights. This modern infor-\nmation integration raises the standard of care by enabling\npractitioners to make educated judgments based on the most\nrecent data.\nIX. CONCLUSION\nGAI models, recently catching attention, have promising\npotential in healthcare. This paper has discussed various\napplications in which different GAI models enhanced health-\ncare operations. The diverse applications, including medi-\ncal imaging, drug discovery, personalized patient treatment,\nmedical simulation and training, clinical trial optimization,\nmental health support, healthcare operations and resource\nmanagement, chatbots, human movement simulation and\nanalysis and text generation and summarization indicate how\nflexible and reliable this technology is and how it can be\nVOLUME X, 2020 25\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nimplemented in healthcare. The different use cases discussed\nshow how GAI has been used to aid patients suffering from\nvisual snow syndrome and enhance the molecular optimisa-\ntion process. Further, how GAI is used in Medical Education\nand Dentistry has also been showcased. However, despite\nthe gains, there continue to be challenges associated with\napplying GAI in healthcare. Ethics, including patient privacy\nand data security, must be prioritized. Stringent laws and\nsafety measures must be in place to ensure the appropriate\nand secure use of patient information. More study and anal-\nysis are required to utilise GAI’s power in regular health-\ncare practices properly. Collaboration between AI scientists,\nhealthcare practitioners, and legislators is essential to solve\ntechnological constraints, validate models and smoothly in-\ntegrate GAI into current healthcare systems.\nREFERENCES\n[1] L. Floridi and M. Chiriatti, “Gpt-3: Its nature, scope, limits, and conse-\nquences,” Minds and Machines, vol. 30, pp. 681–694, 2020.\n[2] G. Marcus, E. Davis, and S. Aaronson, “A very preliminary analysis of\ndall-e 2,” arXiv preprint arXiv:2204.13807, 2022.\n[3] “Midjourney.” [Online]. Available: https://www.midjourney.com/home/\n?callbackUrl=%2Fapp%2F\n[4] “Stable diffusion.” [Online]. Available: https://stablediffusionweb.com/\n[5] J. Zhang, Y . Peng, and M. Yuan, “Unsupervised generative adversarial\ncross-modal hashing,” in Proceedings of the AAAI conference on artificial\nintelligence, vol. 32, no. 1, 2018.\n[6] D. Baidoo-Anu and L. O. Ansah, “Education in the era of generative\nartificial intelligence (ai): Understanding the potential benefits of chatgpt\nin promoting teaching and learning,” Journal of AI, vol. 7, no. 1, pp. 52–62,\n2023.\n[7] O. Temsah, S. A. Khan, Y . Chaiah, A. Senjab, K. Alhasan, A. Jamal,\nF. Aljamaan, K. H. Malki, R. Halwani, J. A. Al-Tawfiq et al., “Overview\nof early chatgpt’s presence in medical literature: insights from a hybrid\nliterature review by chatgpt and human experts,” Cureus, vol. 15, no. 4,\n2023.\n[8] “Diagnamed launches a generative artificial intelligence medical chatbot,\ndr. genai,” https://aithority.com/machine-learning/diagnamed-launches-\na-generative-artificial-intelligence-medical-chatbot-dr-genai/, (Accessed\non 06/19/2023).\n[9] M. Sabry Abdel-Messih and M. N. Kamel Boulos, “Chatgpt in clinical\ntoxicology,” JMIR Medical Education, vol. 9, p. e46876, 2023.\n[10] M. Sallam, “Chatgpt utility in healthcare education, research, and practice:\nsystematic review on the promising perspectives and valid concerns,” in\nHealthcare, vol. 11, no. 6. MDPI, 2023, p. 887.\n[11] T. N. Y . Times, “Generative a.i. start-up cohere valued at about $2 billion\nin funding round,” 2023. [Online]. Available: https://www.nytimes.com/\n2023/05/02/technology/generative-ai-start-up-coherefunding.html\n[12] R. Mao, G. Chen, X. Zhang, F. Guerin, and E. Cambria, “Gpteval: A survey\non assessments of chatgpt and gpt-4,” arXiv preprint arXiv:2308.12488,\n2023.\n[13] T. H. Kung, M. Cheatham, A. Medenilla, C. Sillos, L. De Leon,\nC. Elepaño, M. Madriaga, R. Aggabao, G. Diaz-Candido, J. Maningo\net al., “Performance of chatgpt on usmle: Potential for ai-assisted medical\neducation using large language models,” PLoS digital health, vol. 2, no. 2,\np. e0000198, 2023.\n[14] O. AI, “Gpt-4 is openai’s most advanced system, producing safer and more\nuseful responses,” 2023, https://openai.com/gpt-4.\n[15] S. Teebagy, L. Colwell, E. Wood, A. Yaghy, and M. Faustina, “Improved\nperformance of chatgpt-4 on the okap exam: A comparative study with\nchatgpt-3.5,” medRxiv, pp. 2023–04, 2023.\n[16] G. A. Johnson, J. N. Bloom, L. Szczotka-Flynn, D. Zauner, and R. L.\nTomsak, “A comparative study of resident performance on standardized\ntraining examinations and the american board of ophthalmology written\nexamination,” Ophthalmology, vol. 117, no. 12, pp. 2435–2439, 2010.\n[17] R. Bhayana, S. Krishna, and R. R. Bleakney, “Performance of chatgpt on\na radiology board-style examination: Insights into current strengths and\nlimitations,” Radiology, p. 230582, 2023.\n[18] B. Meskó and E. J. Topol, “The imperative for regulatory oversight of large\nlanguage models (or generative ai) in healthcare,” npj Digital Medicine,\nvol. 6, no. 1, p. 120, 2023.\n[19] R. Rothbaum and J. S. McGee, “Aquagenic urticaria: diagnostic and\nmanagement challenges,” Journal of asthma and allergy, pp. 209–213,\n2016.\n[20] J. T. Ludlow, R. G. Wilkerson, and T. M. Nappe, “Methemoglobinemia,”\n2019.\n[21] Y . Yang, H. Fu, A. I. Aviles-Rivero, C.-B. Schönlieb, and L. Zhu, “Diffmic:\nDual-guidance diffusion network for medical image classification,” in\nInternational Conference on Medical Image Computing and Computer-\nAssisted Intervention. Springer, 2023, pp. 95–105.\n[22] X. Bing, W. Zhang, L. Zheng, and Y . Zhang, “Medical image super\nresolution using improved generative adversarial networks,” IEEE Access,\nvol. 7, pp. 145 030–145 038, 2019.\n[23] J. S. Yoon, C. Zhang, H.-I. Suk, J. Guo, and X. Li, “Sadm: Sequence-\naware diffusion model for longitudinal medical image generation,” in\nInternational Conference on Information Processing in Medical Imaging.\nSpringer, 2023, pp. 388–400.\n[24] T. Susnjak, “Beyond predictive learning analytics modelling and onto\nexplainable artificial intelligence with prescriptive analytics and chatgpt,”\nInternational Journal of Artificial Intelligence in Education, pp. 1–31,\n2023.\n[25] C. Molnar, G. Casalicchio, and B. Bischl, “Interpretable machine learning–\na brief history, state-of-the-art and challenges,” in Joint European con-\nference on machine learning and knowledge discovery in databases.\nSpringer, 2020, pp. 417–431.\n[26] M. T. Ribeiro, S. Singh, and C. Guestrin, “Anchors: High-precision model-\nagnostic explanations,” in Proceedings of the AAAI conference on artifi-\ncial intelligence, vol. 32, no. 1, 2018.\n[27] Y . Li, J. Pei, and L. Lai, “Structure-based de novo drug design using 3d\ndeep generative models,” Chemical science, vol. 12, no. 41, pp. 13 664–\n13 675, 2021.\n[28] T. Salimans, I. Goodfellow, W. Zaremba, V . Cheung, A. Radford, and\nX. Chen, “Improved techniques for training gans,” Advances in neural\ninformation processing systems, vol. 29, 2016.\n[29] H. Abdine, M. Chatzianastasis, C. Bouyioukos, and M. Vazirgiannis,\n“Prot2text: Multimodal protein’s function generation with gnns and trans-\nformers,” arXiv preprint arXiv:2307.14367, 2023.\n[30] C. Buche, F. Lasson, and S. Kerdelo, “Conditional autoencoder pre-\ntraining and optimization algorithms for personalized care of hemophiliac\npatients,” Frontiers in Artificial Intelligence, vol. 6, 2023.\n[31] H. Luo, K. Nagano, H.-W. Kung, Q. Xu, Z. Wang, L. Wei, L. Hu, and H. Li,\n“Normalized avatar synthesis using stylegan and perceptual refinement,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2021, pp. 11 662–11 672.\n[32] J. Mueller, R. B. Parikh, and A. Noble, “Evaluating clinical trial inclu-\nsion/exclusion criteria from claims using generative artificial intelligence.”\n2023.\n[33] L. Gootjes-Dreesbach, M. Sood, A. Sahay, M. Hofmann-Apitius, and\nH. Fröhlich, “Variational autoencoder modular bayesian networks for\nsimulation of heterogeneous clinical study data,” Frontiers in big Data,\nvol. 3, p. 16, 2020.\n[34] L. Yang, “Multi-modal depression detection and estimation,” in 2019 8th\nInternational Conference on Affective Computing and Intelligent Interac-\ntion Workshops and Demos (ACIIW). IEEE, 2019, pp. 26–30.\n[35] “Revolutionizing mental health: Generative ai in therapy,”\nhttps://www.productiveedge.com/blog/revolutionizing-mental-health-\ngenerative-ai-and-therapy, (Accessed on 12/11/2023).\n[36] “Generative ai for mental health is upping the ante by going\nmulti-modal, embracing e-wearables, and a whole lot more,”\nhttps://www.forbes.com/sites/lanceeliot/2023/11/02/generative-ai-for-\nmental-health-is-upping-the-ante-by-going-multi-modal-embracing-e-\nwearables-and-a-whole-lot-more/, (Accessed on 12/11/2023).\n[37] M. Barat, P. Soyer, and A. Dohan, “Appropriateness of recommendations\nprovided by chatgpt to interventional radiologists,” Canadian Association\nof Radiologists Journal, p. 08465371231170133, 2023.\n[38] T. Dave, S. A. Athaluri, and S. Singh, “Chatgpt in medicine: an overview\nof its applications, advantages, limitations, future prospects, and ethical\nconsiderations,” Frontiers in Artificial Intelligence, vol. 6, 2023.\n[39] H. Lee, “The rise of chatgpt: Exploring its potential in medical education,”\nAnatomical Sciences Education, 2023.\n26 VOLUME X, 2020\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n[40] “What is instructgpt and how to access instruct gpt?” https://www.\ntheinsaneapp.com/2023/05/everything-about-instructgpt.html, (Accessed\non 06/29/2023).\n[41] “Ai ushers in next-gen prior authorization in healthcare | mckinsey |\nmckinsey,” https://www.mckinsey.com/industries/healthcare/our-insights/\nai-ushers-in-next-gen-prior-authorization-in-healthcare, (Accessed on\n07/17/2023).\n[42] “Triage - an overview | sciencedirect topics,” https://www.sciencedirect.\ncom/topics/medicine-and-dentistry/triage, (Accessed on 07/17/2023).\n[43] J. Huang, L. Neill, M. Wittbrodt, D. Melnick, M. Klug, M. Thompson,\nJ. Bailitz, T. Loftus, S. Malik, A. Phull et al., “Generative artificial intel-\nligence for chest radiograph interpretation in the emergency department,”\nJAMA network open, vol. 6, no. 10, pp. e2 336 100–e2 336 100, 2023.\n[44] “Safely harness the power of generative ai in healthcare with\nclinically validated virtual triage,” https://www.prnewswire.com/news-\nreleases/safely-harness-the-power-of-generative-ai-in-healthcare-\nwith-clinically-validated-virtual-triage-301811636.html, (Accessed on\n07/17/2023).\n[45] D. M. Levine, R. Tuwani, B. Kompa, A. Varma, S. G. Finlayson,\nA. Mehrotra, and A. Beam, “The diagnostic and triage accuracy of the\ngpt-3 artificial intelligence model,” medRxiv, pp. 2023–01, 2023.\n[46] A. Jadon and S. Kumar, “Leveraging generative ai models for synthetic\ndata generation in healthcare: Balancing research and privacy,” arXiv\npreprint arXiv:2305.05247, 2023.\n[47] H. Mischak, G. Allmaier, R. Apweiler, T. Attwood, M. Baumann, A. Be-\nnigni, S. E. Bennett, R. Bischoff, E. Bongcam-Rudloff, G. Capasso et al.,\n“Recommendations for biomarker identification and qualification in clini-\ncal proteomics,” Science translational medicine, vol. 2, no. 46, pp. 46ps42–\n46ps42, 2010.\n[48] R. Nair, D. D. Mohan, S. Frank, S. Setlur, V . Govindaraju, and\nM. Ramanathan, “Generative adversarial networks for modelling clinical\nbiomarker profiles with race/ethnicity,” British Journal of Clinical Phar-\nmacology, vol. 89, no. 5, pp. 1588–1600, 2023.\n[49] O. Zaballa, A. Pérez, E. G. Inhiesto, T. A. Ayesta, and J. A. Lozano,\n“Learning the progression patterns of treatments using a probabilistic\ngenerative model,” Journal of Biomedical Informatics, vol. 137, p. 104271,\n2023.\n[50] Q. Liu, E. Fuster-Garcia, I. T. Hovden, D. Sederevicius, K. Skogen,\nB. J. MacIntosh, E. Grødem, T. Schellhorn, P. Brandal, A. Bjørnerud\net al., “Treatment-aware diffusion probabilistic model for longitudinal\nmri generation and diffuse glioma growth prediction,” arXiv preprint\narXiv:2309.05406, 2023.\n[51] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales,\nA. Tanwani, H. Cole-Lewis, S. Pfohl et al., “Large language models\nencode clinical knowledge,” arXiv preprint arXiv:2212.13138, 2022.\n[52] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, K. Clark,\nS. Pfohl, H. Cole-Lewis, D. Neal et al., “Towards expert-level med-\nical question answering with large language models,” arXiv preprint\narXiv:2305.09617, 2023.\n[53] H. Bair and J. Norden, “Large language models and their implications on\nmedical education,” Academic Medicine, pp. 10–1097, 2023.\n[54] “Health ai research llm updates,” https://blog.google/technology/health/ai-\nllm-medpalm-research-thecheckup/, (Accessed on 06/28/2023).\n[55] “Med-palm,” https://sites.research.google/med-palm/, (Accessed on\n07/14/2023).\n[56] R. Luo, L. Sun, Y . Xia, T. Qin, S. Zhang, H. Poon, and T.-Y . Liu,\n“Biogpt: generative pre-trained transformer for biomedical text generation\nand mining,” Briefings in Bioinformatics, vol. 23, no. 6, 2022.\n[57] Z. Jie, Z. Zhiying, and L. Li, “A meta-analysis of watson for oncology in\nclinical application,” Scientific reports, vol. 11, no. 1, p. 5792, 2021.\n[58] Z. Dlamini, F. Z. Francies, R. Hull, and R. Marima, “Artificial intelligence\n(ai) and big data in cancer and precision oncology,” Computational and\nstructural biotechnology journal, vol. 18, pp. 2300–2311, 2020.\n[59] “Intelligent computing for healthcare | nvidia clara,” https://www.nvidia.\ncom/en-in/clara/, (Accessed on 07/17/2023).\n[60] “Healthcare developer resources | nvidia developer,” https://developer.\nnvidia.com/industries/healthcare, (Accessed on 07/17/2023).\n[61] “Deep health | radiology ai machine learning solutions,” https://deephealth.\ncom/, (Accessed on 07/17/2023).\n[62] “Deephealth: A deep learning solution for the covid-19 crisis | by pablo\ncastañeda | saturdays.ai | medium,” https://medium.com/saturdays-\nai/deephealth-a-deep-learning-solution-for-the-covid-19-crisis-\n785238119c1a, (Accessed on 07/17/2023).\n[63] A. S. Panayides, A. Amini, N. D. Filipovic, A. Sharma, S. A. Tsaftaris,\nA. Young, D. Foran, N. Do, S. Golemati, T. Kurc et al., “Ai in medical\nimaging informatics: current challenges and future directions,” IEEE jour-\nnal of biomedical and health informatics, vol. 24, no. 7, pp. 1837–1857,\n2020.\n[64] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang, “Biobert:\na pre-trained biomedical language representation model for biomedical\ntext mining,” Bioinformatics, vol. 36, no. 4, pp. 1234–1240, 2020.\n[65] A. Kormilitzin, N. Vaci, Q. Liu, and A. Nevado-Holgado, “Med7: A\ntransferable clinical natural language processing model for electronic\nhealth records,” Artificial Intelligence in Medicine, vol. 118, p. 102086,\n2021.\n[66] F. Puledda, C. Schankin, K. Digre, and P. J. Goadsby, “Visual snow\nsyndrome: what we know so far,” Current opinion in neurology, vol. 31,\nno. 1, pp. 52–58, 2018.\n[67] M. Balas and J. A. Micieli, “Visual snow syndrome: Use of text-to-image\nartificial intelligence models to improve the patient perspective,” Canadian\nJournal of Neurological Sciences, pp. 1–2, 2022.\n[68] J. Avorn, “The $2.6 billion pill—methodologic and policy considerations,”\nNew England Journal of Medicine, vol. 372, no. 20, pp. 1877–1879, 2015.\n[69] Ł. Maziarka, A. Pocha, J. Kaczmarczyk, K. Rataj, T. Danel, and M. War-\nchoł, “Mol-cyclegan: a generative model for molecular optimization,”\nJournal of Cheminformatics, vol. 12, no. 1, pp. 1–18, 2020.\n[70] M. H. Segler, T. Kogej, C. Tyrchan, and M. P. Waller, “Generating focused\nmolecule libraries for drug discovery with recurrent neural networks,”\nACS central science, vol. 4, no. 1, pp. 120–131, 2018.\n[71] F. Grisoni, B. J. Huisman, A. L. Button, M. Moret, K. Atz, D. Merk, and\nG. Schneider, “Combining generative artificial intelligence and on-chip\nsynthesis for de novo drug design,” Science Advances, vol. 7, no. 24, p.\neabg3338, 2021.\n[72] V . Bagal, R. Aggarwal, P. Vinod, and U. D. Priyakumar, “Molgpt: molec-\nular generation using a transformer-decoder model,” Journal of Chemical\nInformation and Modeling, vol. 62, no. 9, pp. 2064–2076, 2021.\n[73] J. G. Ruiz, M. J. Mintzer, and R. M. Leipzig, “The impact of e-learning\nin medical education,” Academic medicine, vol. 81, no. 3, pp. 207–212,\n2006.\n[74] M. Karabacak, B. B. Ozkara, K. Margetis, M. Wintermark, and S. Bisdas,\n“The advent of generative language models in medical education,” JMIR\nMedical Education, vol. 9, p. e48163, 2023.\n[75] Y . Okuda, E. O. Bryson, S. DeMaria Jr, L. Jacobson, J. Quinones,\nB. Shen, and A. I. Levine, “The utility of simulation in medical education:\nwhat is the evidence?” Mount Sinai Journal of Medicine: A Journal of\nTranslational and Personalized Medicine: A Journal of Translational and\nPersonalized Medicine, vol. 76, no. 4, pp. 330–343, 2009.\n[76] “What is persim®? - medcognition,” https://medcognition.com/what-is-\npersim/, (Accessed on 06/29/2023).\n[77] “20230624-22994-cporbe.pdf,” https://assets.cureus.com/uploads/\nreview_article/pdf/158756/20230624-22994-cporbe.pdf, (Accessed\non 07/14/2023).\n[78] “In 2016, microsoft’s racist chatbot revealed the dangers of online\nconversation - ieee spectrum,” https://spectrum.ieee.org/in-2016-\nmicrosofts-racist-chatbot-revealed-the-dangers-of-online-conversation,\n(Accessed on 06/29/2023).\n[79] H. M. Alhaidry, B. Fatani, J. O. Alrayes, A. M. Almana, N. K. Alfhaed,\nH. Alhaidry, J. Alrayes, A. Almana, and N. K. Alfhaed Sr, “Chatgpt in\ndentistry: A comprehensive review,” Cureus, vol. 15, no. 4, 2023.\n[80] T. T. Nguyen, N. Larrivée, A. Lee, O. Bilaniuk, and R. Durand, “Use\nof artificial intelligence in dentistry: current clinical trends and research\nadvances,” J Can Dent Assoc, vol. 87, no. l7, pp. 1488–2159, 2021.\n[81] “The attribution problem with generative ai - hacking semantics,” https:\n//hackingsemantics.xyz/2022/attribution/, (Accessed on 07/17/2023).\n[82] S. Harrer, “Attention is not all you need: the complicated case of ethically\nusing large language models in healthcare and medicine,” EBioMedicine,\nvol. 90, 2023.\n[83] B. Perrigo, “Openai used kenyan workers on less than 2 dollars per hour\nto make chatgpt less toxic. time,” 2023.\n[84] N. Martinez-Martin, Z. Luo, A. Kaushal, E. Adeli, A. Haque, S. S. Kelly,\nS. Wieten, M. K. Cho, D. Magnus, L. Fei-Fei et al., “Ethical issues in using\nambient intelligence in health-care settings,” The lancet digital health,\nvol. 3, no. 2, pp. e115–e123, 2021.\n[85] A. L. Allen, “Coercing privacy,” Wm. & Mary L. Rev., vol. 40, p. 723,\n1998.\nVOLUME X, 2020 27\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n[86] J. Deng and Y . Lin, “The benefits and challenges of chatgpt: An overview,”\nFrontiers in Computing and Intelligent Systems, vol. 2, no. 2, pp. 81–83,\n2022.\n[87] S. Goldman, “Stable diffusion ai art lawsuit, plus caution\nfrom openai, deepmind | the ai beat,” 2023. [Online]. Avail-\nable: https://venturebeat.com/ai/stable-diffusion-lawsuit-plus-words-of-\ncaution-from-openai-deepmind-the-ai-beat/\n[88] P. J. B. Aaron J Snoswell, “A galaxy of deep science fakes: The problems\nwith galactica ai,” 2022. [Online]. Available: https://www.siliconrepublic.\ncom/machines/galactica-ai-meta-fake-science-misinformation\nSIVA SAIis a research scholar in the EEE depart-\nment at Birla Institute of Technology and Science\n(Pilani). Previously, he has completed his B.E in\nComputer Science and M.Sc (Hons) in Economics\nwith BITS Pilani. His research interests include\napplications of blockchain and machine learning\nfor healthcare, natural language processing, com-\nputer vision, and connected vehicles. He worked\non multiple research problems, including WiFi\nCSI activity recognition, multimodal hate speech\ndetection, multilingual offensive/fake speech identification, NLP technolo-\ngies for lower-resource languages, and deep learning for time series analysis.\nHis past publications were accepted by prestigeous journals and conferences\nlike EACL, AAAI, FIRE, EMNLP, IEEE IoT journal, IEEE TITS, and\nNeural Networks(Elsevier).\nAANCHAL GAUR is a final-year student in the\nECE department at Maharaja Agrasen Institute of\nTechnology (New Delhi). She is a research intern\nin medsupervision at the Birla Institute of Tech-\nnology and Science (Pilani). Her research interests\ninclude machine learning and artificial intelligence\napplications in healthcare, agriculture, and com-\nputer vision. Throughout her academic journey,\nshe has actively pursued challenging projects in\nmachine learning. She is dedicated to exploring\nthe applications of cutting-edge technology in healthcare.\nREVANT SAI is currently a pre-final year stu-\ndent pursuing a Bachelor of Engineering degree\nin Computer Science at Birla Institute of Technol-\nogy and Science, Pilani. With a keen interest in\nthe ever-evolving fields of Artificial Intelligence\nand Machine Learning, he has actively pursued\ncoursework and practical projects to advance their\nknowledge and skills. Throughout his academic\njourney, Revant Sai has undertaken challenging\nprojects in areas including Database Systems,\nObject-Oriented Programming (OOP), and Data Structures. These experi-\nences have not only equipped him with a strong foundation in computer\nscience fundamentals but have also cultivated a problem-solving mindset\ncrucial in the world of AI and ML. Revant’s research interests revolve around\nthe convergence of data-driven decision-making and machine intelligence.\nHe is dedicated to exploring innovative solutions to intricate problems\nand harnessing the capabilities of AI and ML to better comprehend the\ncomplexities of our world. With a firm belief in the transformative potential\nof technology, Revant Sai is committed to contributing to the advancement\nof these fields for the benefit of society.\n28 VOLUME X, 2020\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nVINAY CHAMOLA (Fellow, IET) received the\nB.E. degree in electrical and electronics engineer-\ning and master’s degree in communication engi-\nneering from the Birla Institute of Technology and\nScience, Pilani, India, in 2010 and 2013, respec-\ntively. He received his Ph.D. degree in electrical\nand computer engineering from the National Uni-\nversity of Singapore, Singapore, in 2016. In 2015,\nhe was a Visiting Researcher with the Autonomous\nNetworks Research Group (ANRG), University of\nSouthern California, Los Angeles, CA, USA. He also worked as a post-\ndoctoral research fellow at the National University of Singapore, Singapore.\nHe is currently an Associate Professor with the Department of Electrical and\nElectronics Engineering, BITS-Pilani, Pilani, where he heads the Internet of\nThings Research Group / Lab. His research interests include IoT Security,\nBlockchain, UA Vs, V ANETs, 5G, and Healthcare. He serves as an Area\nEditor for the Ad Hoc Networks Journal, Elsevier and the IEEE Internet of\nThings Magazine. He also serves as an Associate Editor in the IEEE Transac-\ntions on Intelligent Transportation Systems, IEEE Networking Letters, IEEE\nConsumer electronics magazine, IET Quantum Communications, IET Net-\nworks, and several other journals. He serves as co-chair of various reputed\nworkshops like IEEE Globecom Workshop 2021, IEEE INFOCOM 2022\nworkshop, IEEE ANTS 2021, and IEEE ICIAfS 2021, to name a few. He is\nlisted in the World’s Top 2% Scientists identified by Stanford University. He\nis co-founder and President of a healthcare startup Medsupervision Pvt. Ltd.\nHe is a senior member of the IEEE.\nMOHSEN GUIZANI(S’85–M’89–SM’99–F’09)\nreceived the B.S. (with distinction), M.S. and\nPh.D. degrees in Electrical and Computer engi-\nneering from Syracuse University, Syracuse, NY ,\nUSA. He is currently a Professor at the Ma-\nchine Learning Department, Mohamed Bin Zayed\nUniversity of Artificial Intelligence (MBZUAI),\nAbu Dhabi. Previously, he worked in different\ninstitutions: Qatar University, University of Idaho,\nWestern Michigan University, University of West\nFlorida, University of Missouri-Kansas City, University of Colorado-\nBoulder, and Syracuse University. His research interests include wireless\ncommunications and mobile computing, applied machine learning, cloud\ncomputing, security and its application to healthcare systems. He was\nelevated to the IEEE Fellow in 2009. He was listed as a Clarivate Analytics\nHighly Cited Researcher in Computer Science in 2019 and 2020. Dr. Guizani\nhas won several research awards including the “2015 IEEE Communications\nSociety Best Survey Paper Award” as well 4 Best Paper Awards from ICC\nand Globecom Conferences. He is the author of nine books and more than\n800 publications. He is also the recipient of the 2017 IEEE Communications\nSociety Wireless Technical Committee (WTC) Recognition Award, the\n2018 AdHoc Technical Committee Recognition Award, and the 2019 IEEE\nCommunications and Information Security Technical Recognition (CISTC)\nAward. He served as the Editor-in-Chief of IEEE Network and is currently\nserves on the Editorial Boards of many IEEE journals/Transactions. He\nwas the Chair of the IEEE Communications Society Wireless Technical\nCommittee and the Chair of the TAOS Technical Committee. He served as\nthe IEEE Computer Society Distinguished Speaker and is currently the IEEE\nComSoc Distinguished Lecturer.\nJOEL J. P . C. RODRIGUES( (Fellow, IEEE) is\na Leader of the Center for Intelligence at Fecomér-\ncio/CE, Brazil, and Full Professor at COPELABS,\nLusófona University, Lisbon, Portugal. He is an\nHighly Cited Researcher (Clarivate), N. 1 of the\ntop scientists in computer science in Brazil (Re-\nsearch.com), the Leader of the Next Generation\nNetworks and Applications (NetGNA) research\ngroup (CNPq), Member Representative of the\nIEEE Communications Society on the IEEE Bio-\nmetrics Council, and the President of the scientific council at ParkUrbis\n– Covilhã Science and Technology Park. He has authored or coauthored\nabout 1150 papers in refereed international journals and conferences, three\nbooks, two patents, and one ITU-T Recommendation. He was the Director\nfor Conference Development - IEEE ComSoc Board of Governors, an IEEE\nDistinguished Lecturer, Technical Activities Committee Chair of the IEEE\nComSoc Latin America Region Board, a Past-Chair of the IEEE ComSoc\nTechnical Committee (TC) on eHealth and the TC on Communications\nSoftware, a Steering Committee member of the IEEE Life Sciences Tech-\nnical Community and Publications co-Chair. He is a member of the Internet\nSociety, a senior member ACM, and Fellow of AAIA. He has been general\nchair and TPC Chair of many international conferences, including IEEE\nICC, IEEE GLOBECOM, IEEE HEALTHCOM, and IEEE LatinCom. He\nhad been awarded several Outstanding Leadership and Outstanding Service\nAwards by IEEE Communications Society and several best papers awards.\nHe is the Editor-in-Chief of the International Journal of E-Health and\nMedical Communications and a editorial board member of several high\nreputed journals (mainly, from IEEE).)\nVOLUME X, 2020 29\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3367715\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nTitle:Generative AI and its Transformative Value for Digital Platforms. Abstract:ABSTRACT The emergence of generative artificial intelligence (GenAI) represents a watershed moment in the evolution of digital platforms. The capabilities of this AI technology go beyond traditional AI systems, enabling the autonomous generation of novel outcomes with significant implications for platform value creation, architecture, governance, and stakeholder interactions. We develop an integrative conceptual framework that identifies four key mechanisms through which GenAI transforms digital platforms: intelligent automation, democratization, hyper-personalization, and collaborative innovation. Through intelligent automation, GenAI transforms boundary resources from passive interfaces into active, intelligent mediators of value creation. Democratization systematically lowers barriers to platform participation. Hyper-personalization enables dynamic, individual-level adaptation of platform content. Collaborative innovation transforms platform innovation by making GenAI an active participant in human-AI value co-creation. We use this framework to situate the papers in the special issue and develop a research agenda that explores the transformative impact of GenAI on platform stakeholder relationships.\nTitle:Use of Generative Artificial Intelligence, Including Large Language Models Such as ChatGPT, in Scientific Publications: Policies of KJR and Prominent Authorities. Abstract:Generative artificial intelligence (AI) refers to algorithms that can be used to create new content, such as text, code, images, videos, and audio. Particularly, with the introduction of generative adversarial networks (GAN) in medical imaging [1,2], generative AI has gained significant attention in the scientific community, leading to numerous publications in the past few years. The Korean Journal of Radiology (KJR) has published several articles on this topic [3-5]. However, the landscape of generative AI in scientific research and publication has dramatically shifted with the emergence of generative large language models (LLMs), such as ChatGPT, which are capable of generating text that closely resembles human writing and easily accessible to the public. The use of LLMs is rapidly expanding in scientific publications [6], creating ethical and legal concerns and challenges related to research integrity, plagiarism, copyright infringement, and authorship, not only for authors, but also for peer reviewers and editors [7-9]. Moreover, these concerns and challenges Use of Generative Artificial Intelligence, Including Large Language Models Such as ChatGPT, in Scientific Publications: Policies of KJR and Prominent Authorities\nTitle:Generative AI for Autonomous Driving: Frontiers and Opportunities. Abstract:Generative Artificial Intelligence (GenAI) constitutes a transformative technological wave that reconfigures industries through its unparalleled capabilities for content creation, reasoning, planning, and multimodal understanding. This revolutionary force offers the most promising path yet toward solving one of engineering's grandest challenges: achieving reliable, fully autonomous driving, particularly the pursuit of Level 5 autonomy. This survey delivers a comprehensive and critical synthesis of the emerging role of GenAI across the autonomous driving stack. We begin by distilling the principles and trade-offs of modern generative modeling, encompassing VAEs, GANs, Diffusion Models, and Large Language Models (LLMs). We then map their frontier applications in image, LiDAR, trajectory, occupancy, video generation as well as LLM-guided reasoning and decision making. We categorize practical applications, such as synthetic data workflows, end-to-end driving strategies, high-fidelity digital twin systems, smart transportation networks, and cross-domain transfer to embodied AI. We identify key obstacles and possibilities such as comprehensive generalization across rare cases, evaluation and safety checks, budget-limited implementation, regulatory compliance, ethical concerns, and environmental effects, while proposing research plans across theoretical assurances, trust metrics, transport integration, and socio-technical influence. By unifying these threads, the survey provides a forward-looking reference for researchers, engineers, and policymakers navigating the convergence of generative AI and advanced autonomous mobility. An actively maintained repository of cited works is available at https://github.com/taco-group/GenAI4AD."
  }
}